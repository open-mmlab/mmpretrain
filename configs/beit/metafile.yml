Collections:
  - Name: BEiT
    Metadata:
      Architecture:
        - Attention Dropout
        - Convolution
        - Dense Connections
        - Dropout
        - GELU
        - Layer Normalization
        - Multi-Head Attention
        - Scaled Dot-Product Attention
        - Tanh Activation
    Paper:
      URL: https://arxiv.org/abs/2106.08254
      Title: 'BEiT: BERT Pre-Training of Image Transformers'
    README: configs/beit/README.md
    Code:
      URL: https://github.com/open-mmlab/mmclassification/blob/dev-1.x/mmcls/models/backbones/beit.py
      Version: v1.0.0rc4

Models:
  - Name: beit-base-p16_8xb64_in1k
    In Collection: BEiT
    Metadata:
      FLOPs: 33030000000
      Parameters: 86860000
      Training Data:
        - ImageNet-21k
        - ImageNet-1k
    Results:
    - Dataset: ImageNet-1k
      Task: Image Classification
      Metrics:
        Top 1 Accuracy: 85.28
        Top 5 Accuracy: 97.59
    Weights: 
    Converted From:
      Weights: https://console.cloud.google.com/storage/browser/_details/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz
      Code: https://github.com/microsoft/unilm/tree/master/beit
    Config: configs/vision_transformer/beit-base-p16_8xb64_in1k.py
