# CLIP

[model page](../../configs/clip/README.md)

## Usage

### Installation

CLIP requires to install open_clip.

```
pip install open_clip_torch
```

### Convert weigts

You can check all pretrained models in [official repo](https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/pretrained.py).

```
python3 tools/model_converters/openclip_to_mmcls.py ${MODEL_NAME} ${PRETRAINED} ${OUT}

# Example
python3 tools/model_converters/openclip_to_mmcls.py ViT-L-14-336 openai ViT-L-14-336px.pth
```

## Results and models

### Pre-trained Models

The pre-trained models are used to fine-tune, and therefore don't have evaluation results.

|                    Model                     | Pretrain | Params(M) | Flops(G) |          Config           |  Download   |
| :------------------------------------------: | :------: | :-------: | :------: | :-----------------------: | :---------: |
| clip-l-336-laion2b-in12k-pre_3rdparty_in1k\* |  OpenAI  |     -     |    -     | [config](./clip-l-336.py) | [model](<>) |

\*Models with * are converted from the [official repo](https://github.com/mlfoundations/open_clip).

## Citation

```bibtex
@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}
```

```bibtex
@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}
```

```bibtex
@inproceedings{schuhmann2022laionb,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}
```
