# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, OpenMMLab
# This file is distributed under the same license as the MMClassification
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
msgid ""
msgstr ""
"Project-Id-Version: MMClassification\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-22 08:42+0800\n"
"PO-Revision-Date: 2022-11-22 15:18+0800\n"
"Last-Translator: Ma Zerun <mzr1996@163.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Language-Team: \n"
"Language: zh_CN\n"
"X-Generator: Poedit 2.3\n"

#: ../../api/apis.rst:7 ../../api/apis.rst:14
msgid "mmcls.apis"
msgstr ""

#: ../../api/apis.rst:9
msgid "These are some high-level APIs for classification tasks."
msgstr "该包提供了一些用于分类任务的高阶 API"

#: ../../api/apis.rst:17
msgid "Inference"
msgstr "推理"

#: ../../api/apis.rst:24:<autosummary>:1
msgid ":py:obj:`init_model <mmcls.apis.init_model>`"
msgstr ""

#: ../../api/apis.rst:24:<autosummary>:1 mmcls.apis.inference.init_model:1 of
msgid "Initialize a classifier from config file."
msgstr "从配置文件初始化一个分类器"

#: ../../api/apis.rst:24:<autosummary>:1
msgid ":py:obj:`inference_model <mmcls.apis.inference_model>`"
msgstr ""

#: ../../api/apis.rst:24:<autosummary>:1 mmcls.apis.inference.inference_model:1 of
msgid "Inference image(s) with the classifier."
msgstr "使用分类器推理图像"

#: ../../api/data_process.rst:5
msgid "Data Process"
msgstr "数据处理"

#: ../../api/data_process.rst:7
msgid ""
"In MMClassification, the data process and the dataset is decomposed. The datasets only define how to get "
"samples' basic information from the file system. These basic information includes the ground-truth label "
"and raw images data / the paths of images.The data process includes data transforms, data preprocessors and "
"batch augmentations."
msgstr ""
"在 MMClassification 中，数据处理和数据集是解耦的。数据集只定义了如何从文件系统中获取样本的基本信息。这些基本"
"信息包括分类标签和原始图像数据/图像的路径。完整的数据处理流程包括了数据变换（data transform）、数据预处理器"
"（data preprocessor）及批量数据增强（batch augmentation）。"

#: ../../api/data_process.rst:13
msgid ""
":mod:`Data Transforms <mmcls.datasets.transforms>`: Transforms includes loading, preprocessing, formatting "
"and etc."
msgstr ""
":mod:`数据变换 <mmcls.datasets.transforms>`：数据变换包括了数据的加载、部分预处理/增强、数据格式化等操作"

#: ../../api/data_process.rst:14
msgid ""
":mod:`Data Preprocessors <mmcls.models.utils.data_preprocessor>`: Processes includes collate, "
"normalization, stacking, channel fliping and etc."
msgstr ""
":mod:`数据预处理器 <mmcls.models.utils.data_preprocessor>`：主要负责批量数据的收集、归一化、堆叠、通道翻转等"
"操作。"

#: ../../api/data_process.rst:16
msgid ""
":mod:`Batch Augmentations <mmcls.models.utils.batch_augments>`: Batch augmentation involves multiple "
"samples, such as Mixup and CutMix."
msgstr ""
":mod:`批量数据增强 <mmcls.models.utils.batch_augments>`：批量数据增强是数据预处理器的功能之一，负责处理涉及"
"多个样本的数据增强操作，例如 Mixup 和 CutMix。"

#: ../../api/data_process.rst:21
msgid "Data Transforms"
msgstr "数据变换"

#: ../../api/data_process.rst:23
msgid ""
"To prepare the inputs data, we need to do some transforms on these basic information. These transforms "
"includes loading, preprocessing and formatting. And a series of data transforms makes up a data pipeline. "
"Therefore, you can find the a ``pipeline`` argument in the configs of dataset, for example:"
msgstr ""
"为了准备输入数据，我们需要对数据集中保存的基本信息做一些变换。这些变换包括数据加载、部分预处理和增强、格式"
"化。一系列的数据变换组成了数据流水线（data pipeline）。因此，在数据集的配置参数中通常存在一个 ``pipeline`` "
"参数，例如："

#: ../../api/data_process.rst:46
msgid ""
"Every item of a pipeline list is one of the following data transforms class. And if you want to add a "
"custom data transformation class, the tutorial :doc:`Custom Data Pipelines </advanced_guides/pipeline>` "
"will help you."
msgstr ""
"``pipeline`` 列表中的每一项都是以下数据变换类之一。如果您想添加自定义数据变换类，可以参考 :doc:`自定义数据流"
"水线教程 </advanced_guides/pipeline>`。"

#: ../../api/data_process.rst:54
msgid "Processing and Augmentation"
msgstr "组合式增强"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`Albumentations <mmcls.datasets.transforms.Albumentations>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.Albumentations:1 of
msgid "Wrapper to use augmentation from albumentations library."
msgstr "使用 Albumentations 库进行数据变换的封装类"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`ColorJitter <mmcls.datasets.transforms.ColorJitter>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.ColorJitter:1 of
msgid "Randomly change the brightness, contrast and saturation of an image."
msgstr "随机改变图像的亮度、对比度和饱和度"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`EfficientNetCenterCrop <mmcls.datasets.transforms.EfficientNetCenterCrop>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.EfficientNetCenterCrop:1
#: of
msgid "EfficientNet style center crop."
msgstr "EfficientNet 风格的中心裁剪"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`EfficientNetRandomCrop <mmcls.datasets.transforms.EfficientNetRandomCrop>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.EfficientNetRandomCrop:1
#: of
msgid "EfficientNet style RandomResizedCrop."
msgstr "EfficientNet 风格的随机缩放裁剪"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`Lighting <mmcls.datasets.transforms.Lighting>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.Lighting:1 of
msgid "Adjust images lighting using AlexNet-style PCA jitter."
msgstr "使用 AlexNet 风格的 PCA 抖动随机调整图像照明"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`RandomCrop <mmcls.datasets.transforms.RandomCrop>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.RandomCrop:1 of
msgid "Crop the given Image at a random location."
msgstr "在随机位置裁剪给定图像"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`RandomErasing <mmcls.datasets.transforms.RandomErasing>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.RandomErasing:1 of
msgid "Randomly selects a rectangle region in an image and erase pixels."
msgstr "在图像中随机选择一个矩形区域并擦除像素"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`RandomResizedCrop <mmcls.datasets.transforms.RandomResizedCrop>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.RandomResizedCrop:1 of
msgid "Crop the given image to random scale and aspect ratio."
msgstr "将给定图像按照随机尺寸和纵横比进行裁剪"

#: ../../api/data_process.rst:70:<autosummary>:1
msgid ":py:obj:`ResizeEdge <mmcls.datasets.transforms.ResizeEdge>`"
msgstr ""

#: ../../api/data_process.rst:70:<autosummary>:1 mmcls.datasets.transforms.processing.ResizeEdge:1 of
msgid "Resize images along the specified edge."
msgstr "按照指定边长调整图像尺寸"

#: ../../api/data_process.rst:72
msgid "Composed Augmentation"
msgstr "组合式增强"

#: ../../api/data_process.rst:73
msgid ""
"Composed augmentation is a kind of methods which compose a series of data augmentation transforms, such as "
"``AutoAugment`` and ``RandAugment``."
msgstr ""
"组合式增强将一系列数据增强方法组合在一起，实现对样本的整体增强，例如 ``AutoAugment`` 和 ``RandAugment``"

#: ../../api/data_process.rst:83:<autosummary>:1
msgid ":py:obj:`AutoAugment <mmcls.datasets.transforms.AutoAugment>`"
msgstr ""

#: ../../api/data_process.rst:83:<autosummary>:1 mmcls.datasets.transforms.auto_augment.AutoAugment:1 of
msgid "Auto augmentation."
msgstr ""

#: ../../api/data_process.rst:83:<autosummary>:1
msgid ":py:obj:`RandAugment <mmcls.datasets.transforms.RandAugment>`"
msgstr ""

#: ../../api/data_process.rst:83:<autosummary>:1 mmcls.datasets.transforms.auto_augment.RandAugment:1 of
msgid "Random augmentation."
msgstr ""

#: ../../api/data_process.rst:84
msgid ""
"To specify the augmentation combination (The ``policies`` argument), you can use string to specify from "
"some preset policies."
msgstr "为了指定增强组合的策略（即上述变换中的 ``policies`` 参数），你可以使用字符串从一系列预设策略中指定。"

#: ../../api/data_process.rst:91
msgid "Preset policy"
msgstr "预设策略"

#: ../../api/data_process.rst:92
msgid "Use for"
msgstr "用于"

#: ../../api/data_process.rst:93
msgid "Description"
msgstr "说明"

#: ../../api/data_process.rst:94
msgid "\"imagenet\""
msgstr ""

#: ../../api/data_process.rst:95
msgid ":class:`AutoAugment`"
msgstr ""

#: ../../api/data_process.rst:96
msgid "Policy for ImageNet, come from `DeepVoltaire/AutoAugment`_"
msgstr "用于 ImageNet 数据集的增强组合，来自 `DeepVoltaire/AutoAugment`_ 仓库"

#: ../../api/data_process.rst:97
msgid "\"timm_increasing\""
msgstr ""

#: ../../api/data_process.rst:98
msgid ":class:`RandAugment`"
msgstr ""

#: ../../api/data_process.rst:99
msgid "The ``_RAND_INCREASING_TRANSFORMS`` policy from `timm`_"
msgstr "`timm`_ 仓库中的 ``_RAND_INCREASING_TRANSFORMS`` 增强组合"

#: ../../api/data_process.rst:104
msgid "And you can also configure a group of policies manually by selecting from the below table."
msgstr "你还可以通过根据下表手动配置一组策略。"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`AutoContrast <mmcls.datasets.transforms.AutoContrast>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.AutoContrast:1 of
msgid "Auto adjust image contrast."
msgstr "自动调整图像对比度"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Brightness <mmcls.datasets.transforms.Brightness>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Brightness:1 of
msgid "Adjust images brightness."
msgstr "自动调整图像亮度"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`ColorTransform <mmcls.datasets.transforms.ColorTransform>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.ColorTransform:1 of
msgid "Adjust images color balance."
msgstr "自动调整图像平衡"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Contrast <mmcls.datasets.transforms.Contrast>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Contrast:1 of
msgid "Adjust images contrast."
msgstr "改变图像对比度"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Cutout <mmcls.datasets.transforms.Cutout>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Cutout:1 of
msgid "Cutout images."
msgstr "擦除部分图像区域"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Equalize <mmcls.datasets.transforms.Equalize>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Equalize:1 of
msgid "Equalize the image histogram."
msgstr "均衡化图像直方图"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Invert <mmcls.datasets.transforms.Invert>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Invert:1 of
msgid "Invert images."
msgstr "反转图像色阶"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Posterize <mmcls.datasets.transforms.Posterize>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Posterize:1 of
msgid "Posterize images (reduce the number of bits for each color channel)."
msgstr "图像像素化（降低各色彩通道的比特数）"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Rotate <mmcls.datasets.transforms.Rotate>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Rotate:1 of
msgid "Rotate images."
msgstr "旋转图像"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Sharpness <mmcls.datasets.transforms.Sharpness>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Sharpness:1 of
msgid "Adjust images sharpness."
msgstr "改变图像锐度"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Shear <mmcls.datasets.transforms.Shear>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Shear:1 of
msgid "Shear images."
msgstr "图像切变"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Solarize <mmcls.datasets.transforms.Solarize>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Solarize:1 of
msgid "Solarize images (invert all pixel values above a threshold)."
msgstr "图像日光化（反转高于某一阈值的所有图像色阶）"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`SolarizeAdd <mmcls.datasets.transforms.SolarizeAdd>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.SolarizeAdd:1 of
msgid "SolarizeAdd images (add a certain value to pixels below a threshold)."
msgstr "图像过曝（为低于某一阈值的所有色阶增加一个固定值）"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`Translate <mmcls.datasets.transforms.Translate>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.Translate:1 of
msgid "Translate images."
msgstr "平移图像"

#: ../../api/data_process.rst:126:<autosummary>:1
msgid ":py:obj:`BaseAugTransform <mmcls.datasets.transforms.BaseAugTransform>`"
msgstr ""

#: ../../api/data_process.rst:126:<autosummary>:1 mmcls.datasets.transforms.auto_augment.BaseAugTransform:1 of
msgid "The base class of augmentation transform for RandAugment."
msgstr "用于组合式增强的数据变换基类"

#: ../../api/data_process.rst:128
msgid "Formatting"
msgstr "格式化"

#: ../../api/data_process.rst:141:<autosummary>:1
msgid ":py:obj:`Collect <mmcls.datasets.transforms.Collect>`"
msgstr ""

#: ../../api/data_process.rst:141:<autosummary>:1 mmcls.datasets.transforms.formatting.Collect:1 of
msgid "Collect and only reserve the specified fields."
msgstr "收集并仅保留指定字段的数据"

#: ../../api/data_process.rst:141:<autosummary>:1
msgid ":py:obj:`PackClsInputs <mmcls.datasets.transforms.PackClsInputs>`"
msgstr ""

#: ../../api/data_process.rst:141:<autosummary>:1 mmcls.datasets.transforms.formatting.PackClsInputs:1 of
msgid "Pack the inputs data for the classification."
msgstr "将输入数据整理成为用于分类任务的数据格式。"

#: ../../api/data_process.rst:141:<autosummary>:1
msgid ":py:obj:`ToNumpy <mmcls.datasets.transforms.ToNumpy>`"
msgstr ""

#: ../../api/data_process.rst:141:<autosummary>:1 mmcls.datasets.transforms.formatting.ToNumpy:1 of
msgid "Convert object to :obj:`numpy.ndarray`."
msgstr "将对象转变为 :obj:`numpy.ndarray`"

#: ../../api/data_process.rst:141:<autosummary>:1
msgid ":py:obj:`ToPIL <mmcls.datasets.transforms.ToPIL>`"
msgstr ""

#: ../../api/data_process.rst:141:<autosummary>:1 mmcls.datasets.transforms.formatting.ToPIL:1 of
msgid "Convert the image from OpenCV format to :obj:`PIL.Image.Image`."
msgstr "将图片从 OpenCV 格式转为为 :obj:`PIL.Image.Image` 格式"

#: ../../api/data_process.rst:141:<autosummary>:1
msgid ":py:obj:`Transpose <mmcls.datasets.transforms.Transpose>`"
msgstr ""

#: ../../api/data_process.rst:141:<autosummary>:1 mmcls.datasets.transforms.formatting.Transpose:1 of
msgid "Transpose numpy array."
msgstr "转置 NumPy 数组"

#: ../../api/data_process.rst:143
msgid "MMCV transforms"
msgstr "MMCV 中的数据变换"

#: ../../api/data_process.rst:145
msgid ""
"We also provides many transforms in MMCV. You can use them directly in the config files. Here are some "
"frequently used transforms, and the whole transforms list can be found in :external+mmcv:doc:`api/"
"transforms`."
msgstr ""
"我们还在 MMCV 中提供了很多数据转换类。你可以在配置文件中直接使用它们。这里我们列举了一些常用的数据变换类，完"
"整的数据变换类列表可以在 :external+mmcv:doc:`api/transforms` 中找到。"

#: ../../api/data_process.rst:150
msgid ":external:class:`~mmcv.transforms.LoadImageFromFile`"
msgstr ""

#: ../../api/data_process.rst:151
msgid "Load an image from file."
msgstr "从图片路径加载图片"

#: ../../api/data_process.rst:152
msgid ":external:class:`~mmcv.transforms.Resize`"
msgstr ""

#: ../../api/data_process.rst:153
msgid "Resize images & bbox & seg & keypoints."
msgstr "缩放图像、bbox、分割图、关键点等"

#: ../../api/data_process.rst:154
msgid ":external:class:`~mmcv.transforms.RandomResize`"
msgstr ""

#: ../../api/data_process.rst:155
msgid "Random resize images & bbox & keypoints."
msgstr "随机缩放图像、bbox、关键点等"

#: ../../api/data_process.rst:156
msgid ":external:class:`~mmcv.transforms.RandomFlip`"
msgstr ""

#: ../../api/data_process.rst:157
msgid "Flip the image & bbox & keypoints & segmentation map."
msgstr "随机翻转图像、bbox、关键点等"

#: ../../api/data_process.rst:158
msgid ":external:class:`~mmcv.transforms.RandomGrayscale`"
msgstr ""

#: ../../api/data_process.rst:159
msgid "Randomly convert image to grayscale with a probability."
msgstr "随机灰度化图像"

#: ../../api/data_process.rst:160
msgid ":external:class:`~mmcv.transforms.CenterCrop`"
msgstr ""

#: ../../api/data_process.rst:161
msgid ""
"Crop the center of the image, segmentation masks, bounding boxes and key points. If the crop area exceeds "
"the original image and ``auto_pad`` is True, the original image will be padded before cropping."
msgstr ""
"裁剪一张图像的中心区域（同时处理分割图、bbox、关键点等）。如果裁剪尺寸超出原图区域，并且指定了 "
"``auto_pad=True``，则会在裁剪之前扩充原图至合适大小"

#: ../../api/data_process.rst:162
msgid ":external:class:`~mmcv.transforms.Normalize`"
msgstr ""

#: ../../api/data_process.rst:163
msgid "Normalize the image."
msgstr "归一化图像"

#: ../../api/data_process.rst:164
msgid ":external:class:`~mmcv.transforms.Compose`"
msgstr ""

#: ../../api/data_process.rst:165
msgid "Compose multiple transforms sequentially."
msgstr "顺序组合一系列数据变换"

#: ../../api/data_process.rst:170
msgid "Data Preprocessors"
msgstr "数据预处理器"

#: ../../api/data_process.rst:172
msgid ""
"The data preprocessor is also a component to process the data before feeding data to the neural network. "
"Comparing with the data transforms, the data preprocessor is a module of the classifier, and it takes a "
"batch of data to process, which means it can use GPU and batch to accelebrate the processing."
msgstr ""
"数据预处理器也是在数据进入神经网络之前，对数据进行处理的组件。与数据变换相比，数据预处理器是模型的一个的模"
"块，并且可以获得一个批次的数据进行处理，这意味着它可以使用模型所在的设备（如 GPU），并利用批量处理，实现加"
"速。"

#: ../../api/data_process.rst:176
msgid "The default data preprocessor in MMClassification could do the pre-processing like following:"
msgstr "MMClassification 中使用的默认的数据预处理器可以进行以下操作："

#: ../../api/data_process.rst:178
msgid "Move data to the target device."
msgstr "将数据移动到模型所在的设备"

#: ../../api/data_process.rst:179
msgid "Pad inputs to the maximum size of current batch."
msgstr "将不同尺寸的输入填充至统一的尺寸"

#: ../../api/data_process.rst:180
msgid "Stack inputs to a batch."
msgstr "将一系列输入的 tensor 组成 batch"

#: ../../api/data_process.rst:181 mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:16 of
msgid "Convert inputs from bgr to rgb if the shape of input is (3, H, W)."
msgstr "如果输入的 tensor 形状为 (3, H, W)，则可以执行 BGR 到 RGB 的通道转换"

#: ../../api/data_process.rst:182 mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:17 of
msgid "Normalize image with defined std and mean."
msgstr "根据给定的均值和方差对图像进行归一化"

#: ../../api/data_process.rst:183
msgid "Do batch augmentations like Mixup and CutMix during training."
msgstr "在训练时进行批量数据增强，如 Mixup 和 CutMix"

#: ../../api/data_process.rst:185
msgid ""
"You can configure the data preprocessor by the ``data_preprocessor`` field or ``model.data_preprocessor`` "
"field in the config file. Typical usages are as below:"
msgstr ""
"你可以在配置文件的 ``data_preprocessor`` 字段，或是 ``model.data_preprocessor`` 字段对数据预处理器进行配置。"
"一个典型的用法如下："

#: ../../api/data_process.rst:196
msgid "Or define in ``model.data_preprocessor`` as following:"
msgstr "或者在 ``model.data_preprocessor`` 字段配置如下："

#: ../../api/data_process.rst:211
msgid "Note that the ``model.data_preprocessor`` has higher priority than ``data_preprocessor``."
msgstr "请注意如果在两处均进行了配置，``model.data_preprocessor`` 拥有更高的优先级。"

#: ../../api/data_process.rst:219:<autosummary>:1
msgid ":py:obj:`ClsDataPreprocessor <mmcls.models.utils.data_preprocessor.ClsDataPreprocessor>`"
msgstr ""

#: ../../api/data_process.rst:219:<autosummary>:1 mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:1
#: of
msgid "Image pre-processor for classification tasks."
msgstr "用于分类任务的图像预处理器"

#: ../../api/data_process.rst:223
msgid "Batch Augmentations"
msgstr "批量数据增强"

#: ../../api/data_process.rst:225
msgid ""
"The batch augmentation is a component of data preprocessors. It involves multiple samples and mix them in "
"some way, such as Mixup and CutMix."
msgstr ""
"批量数据增强是数据预处理器的一个功能。它可以利用一个批次的多个样本，以某种方式进行混合增强，如 Mixup 和 "
"CutMix。"

#: ../../api/data_process.rst:227
msgid ""
"These augmentations are usually only used during training, therefore, we use the ``model.train_cfg`` field "
"to configure them in config files."
msgstr "这些数据增强只会在训练过程中生效，因此，我们使用 ``model.train_cfg`` 字段来配置这些功能。"

#: ../../api/data_process.rst:241
msgid "You can also specify the probabilities of every batch augmentation by the ``probs`` field."
msgstr "你也可以通过 ``probs`` 字段指定每一个批量数据增强的概率。"

#: ../../api/data_process.rst:255
msgid "Here is a list of batch augmentations can be used in MMClassification."
msgstr "这里是 MMClassification 中支持的所有批量数据增强列表。"

#: ../../api/data_process.rst:264:<autosummary>:1
msgid ":py:obj:`Mixup <mmcls.models.utils.batch_augments.Mixup>`"
msgstr ""

#: ../../api/data_process.rst:264:<autosummary>:1 mmcls.models.utils.batch_augments.mixup.Mixup:1 of
msgid "Mixup batch augmentation."
msgstr ""

#: ../../api/data_process.rst:264:<autosummary>:1
msgid ":py:obj:`CutMix <mmcls.models.utils.batch_augments.CutMix>`"
msgstr ""

#: ../../api/data_process.rst:264:<autosummary>:1 mmcls.models.utils.batch_augments.cutmix.CutMix:1 of
msgid "CutMix batch agumentation."
msgstr ""

#: ../../api/data_process.rst:264:<autosummary>:1
msgid ":py:obj:`ResizeMix <mmcls.models.utils.batch_augments.ResizeMix>`"
msgstr ""

#: ../../api/data_process.rst:264:<autosummary>:1 mmcls.models.utils.batch_augments.resizemix.ResizeMix:1 of
msgid "ResizeMix Random Paste layer for a batch of data."
msgstr ""

#: ../../api/datasets.rst:7 ../../api/datasets.rst:14
msgid "mmcls.datasets"
msgstr ""

#: ../../api/datasets.rst:9
msgid ""
"The ``datasets`` package contains several usual datasets for image classification tasks and some dataset "
"wrappers."
msgstr "``dataset`` 包中包含了分类任务中常用的数据集，以及一些数据集封装。"

#: ../../api/datasets.rst:17
msgid "Custom Dataset"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:1 of
msgid "Custom dataset for classification."
msgstr ""

#: mmcls.datasets.custom.CustomDataset:3 of
msgid "The dataset supports two kinds of annotation format."
msgstr ""

#: mmcls.datasets.custom.CustomDataset:5 of
msgid "An annotation file is provided, and each line indicates a sample:"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:7 of
msgid "The sample files: ::"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:19 of
msgid ""
"The annotation file (the first column is the image path and the second column is the index of category): ::"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:28 of
msgid "Please specify the name of categories by the argument ``classes`` or ``metainfo``."
msgstr ""

#: mmcls.datasets.custom.CustomDataset:31 of
msgid "The samples are arranged in the specific way: ::"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:45 of
msgid ""
"If the ``ann_file`` is specified, the dataset will be generated by the first way, otherwise, try the second "
"way."
msgstr ""

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.datasets.base_dataset.BaseDataset mmcls.datasets.cifar.CIFAR10 mmcls.datasets.cifar.CIFAR100
#: mmcls.datasets.cub.CUB mmcls.datasets.custom.CustomDataset mmcls.datasets.dataset_wrappers.KFoldDataset
#: mmcls.datasets.imagenet.ImageNet mmcls.datasets.imagenet.ImageNet21k mmcls.datasets.mnist.FashionMNIST
#: mmcls.datasets.mnist.MNIST mmcls.datasets.multi_label.MultiLabelDataset
#: mmcls.datasets.transforms.auto_augment.AutoAugment mmcls.datasets.transforms.auto_augment.AutoContrast
#: mmcls.datasets.transforms.auto_augment.BaseAugTransform mmcls.datasets.transforms.auto_augment.Brightness
#: mmcls.datasets.transforms.auto_augment.ColorTransform mmcls.datasets.transforms.auto_augment.Contrast
#: mmcls.datasets.transforms.auto_augment.Cutout mmcls.datasets.transforms.auto_augment.Equalize
#: mmcls.datasets.transforms.auto_augment.Invert mmcls.datasets.transforms.auto_augment.Posterize
#: mmcls.datasets.transforms.auto_augment.RandAugment mmcls.datasets.transforms.auto_augment.Rotate
#: mmcls.datasets.transforms.auto_augment.Sharpness mmcls.datasets.transforms.auto_augment.Shear
#: mmcls.datasets.transforms.auto_augment.Solarize mmcls.datasets.transforms.auto_augment.SolarizeAdd
#: mmcls.datasets.transforms.auto_augment.Translate mmcls.datasets.transforms.formatting.Collect
#: mmcls.datasets.transforms.formatting.PackClsInputs mmcls.datasets.transforms.formatting.ToNumpy
#: mmcls.datasets.transforms.formatting.Transpose mmcls.datasets.transforms.processing.Albumentations
#: mmcls.datasets.transforms.processing.Albumentations.transform
#: mmcls.datasets.transforms.processing.ColorJitter mmcls.datasets.transforms.processing.ColorJitter.transform
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop mmcls.datasets.transforms.processing.Lighting
#: mmcls.datasets.transforms.processing.Lighting.transform mmcls.datasets.transforms.processing.RandomCrop
#: mmcls.datasets.transforms.processing.RandomCrop.transform
#: mmcls.datasets.transforms.processing.RandomErasing
#: mmcls.datasets.transforms.processing.RandomErasing.transform
#: mmcls.datasets.transforms.processing.RandomResizedCrop
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform
#: mmcls.datasets.transforms.processing.ResizeEdge mmcls.datasets.transforms.processing.ResizeEdge.transform
#: mmcls.datasets.voc.VOC mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_test
#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_train
#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_val
#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook
#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook.before_train
#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook
#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_epoch
#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_iter
#: mmcls.engine.hooks.visualization_hook.VisualizationHook
#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter
#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter mmcls.engine.optimizers.lamb.Lamb
#: mmcls.engine.optimizers.lamb.Lamb.step mmcls.evaluation.metrics.multi_label.AveragePrecision
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric mmcls.evaluation.metrics.single_label.Accuracy
#: mmcls.evaluation.metrics.single_label.Accuracy.calculate
#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics
#: mmcls.evaluation.metrics.single_label.Accuracy.process
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.process
#: mmcls.evaluation.metrics.voc_multi_label.VOCAveragePrecision
#: mmcls.evaluation.metrics.voc_multi_label.VOCMultiLabelMetric mmcls.models.backbones.alexnet.AlexNet
#: mmcls.models.backbones.conformer.Conformer mmcls.models.backbones.convmixer.ConvMixer
#: mmcls.models.backbones.convnext.ConvNeXt mmcls.models.backbones.cspnet.CSPDarkNet
#: mmcls.models.backbones.cspnet.CSPNet mmcls.models.backbones.cspnet.CSPResNeXt
#: mmcls.models.backbones.cspnet.CSPResNet mmcls.models.backbones.davit.DaViT
#: mmcls.models.backbones.deit.DistilledVisionTransformer mmcls.models.backbones.deit3.DeiT3
#: mmcls.models.backbones.densenet.DenseNet mmcls.models.backbones.edgenext.EdgeNeXt
#: mmcls.models.backbones.efficientformer.EfficientFormer mmcls.models.backbones.efficientnet.EfficientNet
#: mmcls.models.backbones.hornet.HorNet mmcls.models.backbones.hrnet.HRNet
#: mmcls.models.backbones.inception_v3.InceptionV3 mmcls.models.backbones.lenet.LeNet5
#: mmcls.models.backbones.mlp_mixer.MlpMixer mmcls.models.backbones.mobilenet_v2.MobileNetV2
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer mmcls.models.backbones.mobilenet_v3.MobileNetV3
#: mmcls.models.backbones.mobileone.MobileOne mmcls.models.backbones.mobilevit.MobileViT
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer mmcls.models.backbones.mvit.MViT
#: mmcls.models.backbones.poolformer.PoolFormer mmcls.models.backbones.regnet.RegNet
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float mmcls.models.backbones.replknet.RepLKNet
#: mmcls.models.backbones.repmlp.RepMLPNet mmcls.models.backbones.repvgg.RepVGG
#: mmcls.models.backbones.res2net.Res2Net mmcls.models.backbones.resnest.ResNeSt
#: mmcls.models.backbones.resnet.ResNet mmcls.models.backbones.resnet_cifar.ResNet_CIFAR
#: mmcls.models.backbones.resnext.ResNeXt mmcls.models.backbones.seresnet.SEResNet
#: mmcls.models.backbones.seresnext.SEResNeXt mmcls.models.backbones.shufflenet_v1.ShuffleNetV1
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2 mmcls.models.backbones.swin_transformer.SwinTransformer
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2 mmcls.models.backbones.t2t_vit.T2T_ViT
#: mmcls.models.backbones.timm_backbone.TIMMBackbone mmcls.models.backbones.tnt.TNT
#: mmcls.models.backbones.twins.PCPVT mmcls.models.backbones.twins.SVT mmcls.models.backbones.van.VAN
#: mmcls.models.backbones.vgg.VGG mmcls.models.backbones.vision_transformer.VisionTransformer
#: mmcls.models.classifiers.base.BaseClassifier mmcls.models.classifiers.base.BaseClassifier.extract_feat
#: mmcls.models.classifiers.base.BaseClassifier.extract_feats
#: mmcls.models.classifiers.base.BaseClassifier.forward
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict
#: mmcls.models.classifiers.image.ImageClassifier mmcls.models.classifiers.image.ImageClassifier.extract_feat
#: mmcls.models.classifiers.image.ImageClassifier.forward mmcls.models.classifiers.image.ImageClassifier.loss
#: mmcls.models.classifiers.image.ImageClassifier.predict mmcls.models.classifiers.timm.TimmClassifier
#: mmcls.models.classifiers.timm.TimmClassifier.loss mmcls.models.classifiers.timm.TimmClassifier.predict
#: mmcls.models.heads.cls_head.ClsHead mmcls.models.heads.cls_head.ClsHead.loss
#: mmcls.models.heads.cls_head.ClsHead.predict mmcls.models.heads.conformer_head.ConformerHead
#: mmcls.models.heads.conformer_head.ConformerHead.predict mmcls.models.heads.deit_head.DeiTClsHead
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss
#: mmcls.models.heads.linear_head.LinearClsHead mmcls.models.heads.margin_head.ArcFaceClsHead
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss
#: mmcls.models.heads.margin_head.ArcFaceClsHead.set_margins
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict
#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead
#: mmcls.models.heads.stacked_head.StackedLinearClsHead
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward
#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss mmcls.models.losses.focal_loss.FocalLoss
#: mmcls.models.losses.focal_loss.FocalLoss.forward mmcls.models.losses.label_smooth_loss.LabelSmoothLoss
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward mmcls.models.losses.seesaw_loss.SeesawLoss
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward mmcls.models.necks.gap.GlobalAveragePooling
#: mmcls.models.necks.gem.GeneralizedMeanPooling mmcls.models.necks.hr_fuse.HRFuseScales
#: mmcls.models.utils.attention.MultiheadAttention mmcls.models.utils.attention.ShiftWindowMSA
#: mmcls.models.utils.attention.WindowMSA mmcls.models.utils.attention.WindowMSA.forward
#: mmcls.models.utils.attention.WindowMSAV2 mmcls.models.utils.attention.WindowMSAV2.forward
#: mmcls.models.utils.batch_augments.cutmix.CutMix
#: mmcls.models.utils.batch_augments.cutmix.CutMix.cutmix_bbox_and_lam
#: mmcls.models.utils.batch_augments.cutmix.CutMix.mix
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox_minmax
#: mmcls.models.utils.batch_augments.mixup.Mixup mmcls.models.utils.batch_augments.mixup.Mixup.mix
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix
#: mmcls.models.utils.channel_shuffle.channel_shuffle mmcls.models.utils.data_preprocessor.ClsDataPreprocessor
#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward mmcls.models.utils.embed.HybridEmbed
#: mmcls.models.utils.embed.PatchEmbed mmcls.models.utils.embed.PatchMerging
#: mmcls.models.utils.embed.PatchMerging.forward mmcls.models.utils.embed.resize_pos_embed
#: mmcls.models.utils.embed.resize_relative_position_bias_table mmcls.models.utils.helpers._ntuple
#: mmcls.models.utils.inverted_residual.InvertedResidual
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward mmcls.models.utils.layer_scale.LayerScale
#: mmcls.models.utils.make_divisible.make_divisible
#: mmcls.models.utils.position_encoding.ConditionalPositionEncoding mmcls.models.utils.se_layer.SELayer
#: mmcls.utils.setup_env.register_all_modules mmcls.visualization.cls_visualizer.ClsVisualizer of
msgid "参数"
msgstr ""

#: mmcls.datasets.custom.CustomDataset:48 mmcls.datasets.imagenet.ImageNet:6
#: mmcls.datasets.imagenet.ImageNet21k:7 of
msgid "Annotation file path. Defaults to ''."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:14 mmcls.datasets.custom.CustomDataset:50
#: mmcls.datasets.imagenet.ImageNet:8 mmcls.datasets.imagenet.ImageNet21k:9
#: mmcls.datasets.multi_label.MultiLabelDataset:35 of
msgid "Meta information for dataset, such as class information. Defaults to None."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:17 mmcls.datasets.custom.CustomDataset:53
#: mmcls.datasets.imagenet.ImageNet:11 mmcls.datasets.imagenet.ImageNet21k:12
#: mmcls.datasets.multi_label.MultiLabelDataset:38 of
msgid "The root directory for ``data_prefix`` and ``ann_file``. Defaults to ''."
msgstr ""

#: mmcls.datasets.custom.CustomDataset:56 of
msgid "Prefix for the data. Defaults to ''."
msgstr ""

#: mmcls.datasets.custom.CustomDataset:58 of
msgid ""
"A sequence of allowed extensions. Defaults to ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif')."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:37 mmcls.datasets.custom.CustomDataset:61
#: mmcls.datasets.multi_label.MultiLabelDataset:59 of
msgid ""
"Whether to load annotation during instantiation. In some cases, such as visualization, only the meta "
"information of the dataset is needed, which is not necessary to load annotation file. ``Basedataset`` can "
"skip load annotations to save time by set ``lazy_init=False``. Defaults to False."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:20 mmcls.datasets.cifar.CIFAR100:17 mmcls.datasets.custom.CustomDataset:67
#: mmcls.datasets.mnist.FashionMNIST:18 mmcls.datasets.mnist.MNIST:20 mmcls.datasets.voc.VOC:40 of
msgid "Other keyword arguments in :class:`BaseDataset`."
msgstr ""

#: ../../api/datasets.rst:22
msgid "ImageNet"
msgstr ""

#: mmcls.datasets.imagenet.ImageNet:1 of
msgid "`ImageNet <http://www.image-net.org>`_ Dataset."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet:3 of
msgid ""
"The dataset supports two kinds of annotation format. More details can be found in :class:`CustomDataset`."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:20 mmcls.datasets.imagenet.ImageNet:14
#: mmcls.datasets.imagenet.ImageNet21k:15 mmcls.datasets.multi_label.MultiLabelDataset:41 of
msgid "Prefix for training data. Defaults to ''."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet:16 mmcls.datasets.imagenet.ImageNet21k:20 of
msgid "Other keyword arguments in :class:`CustomDataset` and :class:`BaseDataset`."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet21k:1 of
msgid "ImageNet21k Dataset."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet21k:3 of
msgid ""
"Since the dataset ImageNet21k is extremely big, cantains 21k+ classes and 1.4B files. We won't provide the "
"default categories list. Please specify it from the ``classes`` argument."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet21k:17 of
msgid "Not implement by now. Use multi label or not. Defaults to False."
msgstr ""

#: ../../api/datasets.rst:29
msgid "CIFAR"
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:1 of
msgid "`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:3 of
msgid ""
"This implementation is modified from https://github.com/pytorch/vision/blob/master/torchvision/datasets/"
"cifar.py"
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:6 mmcls.datasets.cifar.CIFAR100:3 mmcls.datasets.mnist.FashionMNIST:4
#: mmcls.datasets.mnist.MNIST:6 of
msgid "Prefix for data."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:8 mmcls.datasets.cifar.CIFAR100:5 mmcls.datasets.cub.CUB:28
#: mmcls.datasets.mnist.FashionMNIST:6 mmcls.datasets.mnist.MNIST:8 mmcls.datasets.voc.VOC:34 of
msgid "``test_mode=True`` means in test phase. It determines to use the training set or test set."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:11 mmcls.datasets.cifar.CIFAR100:8 mmcls.datasets.mnist.FashionMNIST:9
#: mmcls.datasets.mnist.MNIST:11 mmcls.datasets.voc.VOC:37 of
msgid "Meta information for dataset, such as categories information. Defaults to None."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:14 mmcls.datasets.cifar.CIFAR100:11 mmcls.datasets.mnist.FashionMNIST:12
#: mmcls.datasets.mnist.MNIST:14 of
msgid "The root directory for ``data_prefix``. Defaults to ''."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:17 mmcls.datasets.cifar.CIFAR100:14 mmcls.datasets.mnist.FashionMNIST:15
#: mmcls.datasets.mnist.MNIST:17 of
msgid "Whether to download the dataset if not exists. Defaults to True."
msgstr ""

#: mmcls.datasets.cifar.CIFAR100:1 of
msgid "`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset."
msgstr ""

#: ../../api/datasets.rst:36
msgid "MNIST"
msgstr ""

#: mmcls.datasets.mnist.MNIST:1 of
msgid "`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset."
msgstr ""

#: mmcls.datasets.mnist.MNIST:3 of
msgid ""
"This implementation is modified from https://github.com/pytorch/vision/blob/master/torchvision/datasets/"
"mnist.py"
msgstr ""

#: mmcls.datasets.mnist.FashionMNIST:1 of
msgid "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset."
msgstr ""

#: ../../api/datasets.rst:43
msgid "VOC"
msgstr ""

#: mmcls.datasets.voc.VOC:1 of
msgid "`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Dataset."
msgstr ""

#: mmcls.datasets.voc.VOC:3 of
msgid "After decompression, the dataset directory structure is as follows:"
msgstr ""

#: mmcls.datasets.voc.VOC:5 of
msgid "VOC dataset directory: ::"
msgstr ""

#: mmcls.datasets.voc.VOC:18 of
msgid ""
"Extra difficult label is in VOC annotations, we will use `gt_label_difficult` to record the difficult "
"labels in each sample and corresponding evaluation should take care of this field to calculate metrics. "
"Usually, difficult labels are reckoned as negative in defaults."
msgstr ""

#: mmcls.datasets.voc.VOC:24 of
msgid "The root directory for VOC dataset."
msgstr ""

#: mmcls.datasets.voc.VOC:26 of
msgid ""
"The path of image set, The file which lists image ids of the sub dataset, and this path is relative to "
"``data_root``."
msgstr ""

#: mmcls.datasets.voc.VOC:30 of
msgid ""
"Prefix for data and annotation, keyword 'img_path' and 'ann_path' can be set. Defaults to be "
"``dict(img_path='JPEGImages', ann_path='Annotations')``."
msgstr ""

#: ../../api/datasets.rst:48
msgid "CUB"
msgstr ""

#: mmcls.datasets.cub.CUB:1 of
msgid "The CUB-200-2011 Dataset."
msgstr ""

#: mmcls.datasets.cub.CUB:3 of
msgid ""
"Support the `CUB-200-2011 <http://www.vision.caltech.edu/visipedia/CUB-200-2011.html>`_ Dataset. Comparing "
"with the `CUB-200 <http://www.vision.caltech.edu/visipedia/CUB-200.html>`_ Dataset, there are much more "
"pictures in `CUB-200-2011`. After downloading and decompression, the dataset directory structure is as "
"follows."
msgstr ""

#: mmcls.datasets.cub.CUB:8 of
msgid "CUB dataset directory: ::"
msgstr ""

#: mmcls.datasets.cub.CUB:26 of
msgid "The root directory for CUB-200-2011 dataset."
msgstr ""

#: mmcls.datasets.cub.CUB:31 of
msgid "Annotation file path, path relative to ``data_root``. Defaults to 'images.txt'."
msgstr ""

#: mmcls.datasets.cub.CUB:34 of
msgid "Prefix for iamges, path relative to ``data_root``. Defaults to 'images'."
msgstr ""

#: mmcls.datasets.cub.CUB:37 of
msgid "The label file, path relative to ``data_root``. Defaults to 'image_class_labels.txt'."
msgstr ""

#: mmcls.datasets.cub.CUB:40 of
msgid ""
"The split file  to split train and test dataset, path relative to ``data_root``. Defaults to "
"'train_test_split_file.txt'."
msgstr ""

#: mmcls.datasets.cub.CUB:46 mmcls.datasets.transforms.auto_augment.RandAugment:44
#: mmcls.evaluation.metrics.multi_label.AveragePrecision:39
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:69 mmcls.evaluation.metrics.single_label.Accuracy:32
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:68 mmcls.models.backbones.mvit.MViT:80
#: mmcls.models.backbones.swin_transformer.SwinTransformer:75
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:78 mmcls.models.backbones.twins.PCPVT:46
#: mmcls.models.backbones.twins.SVT:47 mmcls.models.backbones.van.VAN:50
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:49
#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:25
#: mmcls.models.classifiers.timm.TimmClassifier:40 mmcls.structures.cls_data_sample.ClsDataSample:21
#: mmcls.visualization.cls_visualizer.ClsVisualizer:22 of
msgid "实际案例"
msgstr "使用示例"

#: ../../api/datasets.rst:53
msgid "Base classes"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:1 of
msgid "Base dataset for image classification task."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:3 mmcls.datasets.multi_label.MultiLabelDataset:3 of
msgid "This dataset support annotation file in `OpenMMLab 2.0 style annotation format`."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:9 of
msgid "Comparing with the :class:`mmengine.BaseDataset`, this class implemented several useful methods."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:12 mmcls.datasets.multi_label.MultiLabelDataset:33 of
msgid "Annotation file path."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:22 mmcls.datasets.multi_label.MultiLabelDataset:43 of
msgid "Config for filter data. Defaults to None."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:24 of
msgid ""
"Support using first few data in annotation file to facilitate training/testing on a smaller dataset. "
"Defaults to None, which means using all ``data_infos``."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:28 mmcls.datasets.multi_label.MultiLabelDataset:49 of
msgid ""
"Whether to hold memory using serialized objects, when enabled, data loader workers can use shared RAM from "
"master process instead of making a copy. Defaults to True."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:32 of
msgid "Processing pipeline. Defaults to an empty tuple."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:34 mmcls.datasets.multi_label.MultiLabelDataset:56 of
msgid "``test_mode=True`` means in test phase. Defaults to False."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:43 mmcls.datasets.multi_label.MultiLabelDataset:65 of
msgid ""
"If ``Basedataset.prepare_data`` get a None img. The maximum extra number of cycles to get a valid image. "
"Defaults to 1000."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:47 mmcls.datasets.multi_label.MultiLabelDataset:69 of
msgid ""
"Specify names of classes.  - If is string, it should be a file path, and the every line of   the file is a "
"name of a class. - If is a sequence of string, every item is a name of class. - If is None, use categories "
"information in ``metainfo`` argument,   annotation file or the class attribute ``METAINFO``.  Defaults to "
"None."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:47 mmcls.datasets.multi_label.MultiLabelDataset:69 of
msgid "Specify names of classes."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:49 mmcls.datasets.multi_label.MultiLabelDataset:71 of
msgid "If is string, it should be a file path, and the every line of the file is a name of a class."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:51 mmcls.datasets.multi_label.MultiLabelDataset:73 of
msgid "If is a sequence of string, every item is a name of class."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:52 mmcls.datasets.multi_label.MultiLabelDataset:74 of
msgid ""
"If is None, use categories information in ``metainfo`` argument, annotation file or the class attribute "
"``METAINFO``."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:55 mmcls.datasets.multi_label.MultiLabelDataset:77
#: mmcls.models.backbones.hrnet.HRNet:23 mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:32
#: mmcls.models.classifiers.image.ImageClassifier:23 mmcls.models.classifiers.timm.TimmClassifier:23 of
msgid "Defaults to None."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset:1 of
msgid "Multi-label Dataset."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset:9 of
msgid "The annotation format is shown as follows."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset:45 of
msgid ""
"Support using first few data in annotation file to facilitate training/testing on a smaller dataset. "
"Defaults to None which means using all ``data_infos``."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset:54 of
msgid "Processing pipeline. Defaults to []."
msgstr ""

#: ../../api/datasets.rst:60
msgid "Dataset Wrappers"
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:1 of
msgid "A wrapper of dataset for K-Fold cross-validation."
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:3 of
msgid ""
"K-Fold cross-validation divides all the samples in groups of samples, called folds, of almost equal sizes. "
"And we use k-1 of folds to do training and use the fold left to do validation."
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:7 of
msgid "The dataset to be divided"
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:10 of
msgid "The fold used to do validation. Defaults to 0."
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:12 of
msgid "The number of all folds. Defaults to 5."
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:14 of
msgid "Use the training dataset or validation dataset. Defaults to False."
msgstr ""

#: mmcls.datasets.dataset_wrappers.KFoldDataset:17 of
msgid "The seed to shuffle the dataset before splitting. If None, not shuffle the dataset. Defaults to None."
msgstr ""

#: ../../api/datasets.rst:64
msgid "The dataset wrappers in the MMEngine can be directly used in MMClassification."
msgstr ""

#: ../../api/datasets.rst:68
msgid ":class:`~mmengine.dataset.ConcatDataset`"
msgstr ""

#: ../../api/datasets.rst:69
msgid "A wrapper of concatenated dataset."
msgstr ""

#: ../../api/datasets.rst:70
msgid ":class:`~mmengine.dataset.RepeatDataset`"
msgstr ""

#: ../../api/datasets.rst:71
msgid "A wrapper of repeated dataset."
msgstr ""

#: ../../api/datasets.rst:72
msgid ":class:`~mmengine.dataset.ClassBalancedDataset`"
msgstr ""

#: ../../api/datasets.rst:73
msgid "A wrapper of class balanced dataset."
msgstr ""

#: ../../api/engine.rst:7 ../../api/engine.rst:19
msgid "mmcls.engine"
msgstr ""

#: ../../api/engine.rst:9
msgid ""
"This package includes some runtime components, including hooks, runners, optimizers and loops. These "
"components are useful in classification tasks but not supported by MMEngine yet."
msgstr ""
"该包中包含了一些运行时组件，如钩子（hook）、执行器（runner）、优化器（optimizer）和循环执行器（loop）。这些"
"组件在分类任务中需要用到，而还未被 MMEngine 支持。"

#: ../../api/engine.rst:14
msgid "Some components may be moved to MMEngine in the future."
msgstr "部分组件未来可能会被移动到 MMEngine 中。"

#: ../../api/engine.rst:24
msgid "Hooks"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid ":py:obj:`ClassNumCheckHook <mmcls.engine.hooks.ClassNumCheckHook>`"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1 mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook:1 of
msgid "Class Number Check HOOK."
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid ":py:obj:`PreciseBNHook <mmcls.engine.hooks.PreciseBNHook>`"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1 mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:1 of
msgid "Precise BN hook."
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid ":py:obj:`VisualizationHook <mmcls.engine.hooks.VisualizationHook>`"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid "Classification Visualization Hook."
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid ":py:obj:`PrepareProtoBeforeValLoopHook <mmcls.engine.hooks.PrepareProtoBeforeValLoopHook>`"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1 mmcls.engine.hooks.retriever_hooks.PrepareProtoBeforeValLoopHook:1
#: of
msgid "The hook to prepare the prototype in retrievers."
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1
msgid ":py:obj:`SetAdaptiveMarginsHook <mmcls.engine.hooks.SetAdaptiveMarginsHook>`"
msgstr ""

#: ../../api/engine.rst:36:<autosummary>:1 mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook:1 of
msgid "Set adaptive-margins in ArcFaceClsHead based on the power of category-wise count."
msgstr ""

#: ../../api/engine.rst:40
msgid "Optimizers"
msgstr ""

#: ../../api/engine.rst:47:<autosummary>:1
msgid ":py:obj:`Lamb <mmcls.engine.optimizers.Lamb>`"
msgstr ""

#: ../../api/engine.rst:47:<autosummary>:1 mmcls.engine.optimizers.lamb.Lamb:1 of
msgid "A pure pytorch variant of FuseLAMB (NvLamb variant) optimizer."
msgstr ""

#: ../../api/evaluation.rst:7 ../../api/evaluation.rst:14
msgid "mmcls.evaluation"
msgstr ""

#: ../../api/evaluation.rst:9
msgid "This package includes metrics and evaluators for classification tasks."
msgstr "该包中包含了用于分类任务的一系列评测指标及评测器。"

#: ../../api/evaluation.rst:17
msgid "Single Label Metric"
msgstr ""

#: ../../api/evaluation.rst:26:<autosummary>:1
msgid ":py:obj:`Accuracy <mmcls.evaluation.Accuracy>`"
msgstr ""

#: ../../api/evaluation.rst:26:<autosummary>:1 mmcls.evaluation.metrics.single_label.Accuracy:1 of
msgid "Accuracy evaluation metric."
msgstr ""

#: ../../api/evaluation.rst:26:<autosummary>:1
msgid ":py:obj:`SingleLabelMetric <mmcls.evaluation.SingleLabelMetric>`"
msgstr ""

#: ../../api/evaluation.rst:26:<autosummary>:1 mmcls.evaluation.metrics.single_label.SingleLabelMetric:1 of
msgid "A collection of precision, recall, f1-score and support for single-label tasks."
msgstr ""

#: ../../api/evaluation.rst:28
msgid "Multi Label Metric"
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1
msgid ":py:obj:`AveragePrecision <mmcls.evaluation.AveragePrecision>`"
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1 mmcls.evaluation.metrics.multi_label.AveragePrecision:1 of
msgid "Calculate the average precision with respect of classes."
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1
msgid ":py:obj:`MultiLabelMetric <mmcls.evaluation.MultiLabelMetric>`"
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1 mmcls.evaluation.metrics.multi_label.MultiLabelMetric:1 of
msgid "A collection of precision, recall, f1-score and support for multi-label tasks."
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1
msgid ":py:obj:`VOCAveragePrecision <mmcls.evaluation.VOCAveragePrecision>`"
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1 mmcls.evaluation.metrics.voc_multi_label.VOCAveragePrecision:1
#: of
msgid "Calculate the average precision with respect of classes for VOC dataset."
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1
msgid ":py:obj:`VOCMultiLabelMetric <mmcls.evaluation.VOCMultiLabelMetric>`"
msgstr ""

#: ../../api/evaluation.rst:36:<autosummary>:1 mmcls.evaluation.metrics.voc_multi_label.VOCMultiLabelMetric:1
#: of
msgid ""
"A collection of metrics for multi-label multi-class classification task based on confusion matrix for VOC "
"dataset."
msgstr ""

#: ../../api/generated/mmcls.apis.inference_model.rst:2
msgid "mmcls.apis.inference\\_model"
msgstr ""

#: mmcls.apis.inference.inference_model:3 of
msgid "The loaded classifier."
msgstr ""

#: mmcls.apis.inference.inference_model:5 of
msgid "The image filename or loaded image."
msgstr ""

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.datasets.transforms.processing.Albumentations.albu_builder
#: mmcls.datasets.transforms.processing.Albumentations.mapper
#: mmcls.datasets.transforms.processing.Albumentations.transform
#: mmcls.datasets.transforms.processing.ColorJitter.transform
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform
#: mmcls.datasets.transforms.processing.Lighting.transform
#: mmcls.datasets.transforms.processing.RandomCrop.transform
#: mmcls.datasets.transforms.processing.RandomErasing.transform
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform
#: mmcls.datasets.transforms.processing.ResizeEdge.transform
#: mmcls.evaluation.metrics.single_label.Accuracy.calculate
#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float
#: mmcls.models.classifiers.base.BaseClassifier.extract_feats
#: mmcls.models.classifiers.base.BaseClassifier.forward
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict
#: mmcls.models.classifiers.image.ImageClassifier.extract_feat
#: mmcls.models.classifiers.image.ImageClassifier.forward mmcls.models.classifiers.image.ImageClassifier.loss
#: mmcls.models.classifiers.timm.TimmClassifier.loss mmcls.models.classifiers.timm.TimmClassifier.predict
#: mmcls.models.heads.cls_head.ClsHead.loss mmcls.models.heads.cls_head.ClsHead.predict
#: mmcls.models.heads.conformer_head.ConformerHead.predict
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward mmcls.models.losses.focal_loss.FocalLoss.forward
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward mmcls.models.utils.batch_augments.cutmix.CutMix.mix
#: mmcls.models.utils.batch_augments.mixup.Mixup.mix mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix
#: mmcls.models.utils.channel_shuffle.channel_shuffle
#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward
#: mmcls.models.utils.embed.PatchMerging.forward mmcls.models.utils.embed.resize_pos_embed
#: mmcls.models.utils.embed.resize_relative_position_bias_table
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward
#: mmcls.models.utils.make_divisible.make_divisible of
msgid "返回"
msgstr ""

#: mmcls.apis.inference.inference_model:8 of
msgid "The classification results that contains     `class_name`, `pred_label` and `pred_score`."
msgstr ""

#: mmcls.apis.inference.inference_model:10 of
msgid "The classification results that contains"
msgstr ""

#: mmcls.apis.inference.inference_model:11 of
msgid "`class_name`, `pred_label` and `pred_score`."
msgstr ""

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.datasets.transforms.processing.Albumentations.albu_builder
#: mmcls.datasets.transforms.processing.Albumentations.mapper
#: mmcls.datasets.transforms.processing.Albumentations.transform
#: mmcls.datasets.transforms.processing.ColorJitter.transform
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform
#: mmcls.datasets.transforms.processing.Lighting.transform
#: mmcls.datasets.transforms.processing.RandomCrop.transform
#: mmcls.datasets.transforms.processing.RandomErasing.transform
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform
#: mmcls.datasets.transforms.processing.ResizeEdge.transform
#: mmcls.evaluation.metrics.single_label.Accuracy.calculate
#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float
#: mmcls.models.classifiers.base.BaseClassifier.extract_feats
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict
#: mmcls.models.classifiers.image.ImageClassifier.extract_feat
#: mmcls.models.classifiers.image.ImageClassifier.loss mmcls.models.classifiers.timm.TimmClassifier.loss
#: mmcls.models.classifiers.timm.TimmClassifier.predict mmcls.models.heads.cls_head.ClsHead.loss
#: mmcls.models.heads.cls_head.ClsHead.predict mmcls.models.heads.conformer_head.ConformerHead.predict
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward mmcls.models.losses.focal_loss.FocalLoss.forward
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward mmcls.models.utils.batch_augments.cutmix.CutMix.mix
#: mmcls.models.utils.batch_augments.mixup.Mixup.mix mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix
#: mmcls.models.utils.channel_shuffle.channel_shuffle
#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward
#: mmcls.models.utils.embed.PatchMerging.forward mmcls.models.utils.embed.resize_pos_embed
#: mmcls.models.utils.embed.resize_relative_position_bias_table
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward
#: mmcls.models.utils.make_divisible.make_divisible of
msgid "返回类型"
msgstr ""

#: ../../api/generated/mmcls.apis.init_model.rst:2
msgid "mmcls.apis.init\\_model"
msgstr ""

#: mmcls.apis.inference.init_model:3 of
msgid "Config file path or the config object."
msgstr ""

#: mmcls.apis.inference.init_model:6 of
msgid "Checkpoint path. If left as None, the model will not load any weights."
msgstr ""

#: mmcls.apis.inference.init_model:9 of
msgid "Options to override some settings in the used config."
msgstr ""

#: mmcls.apis.inference.init_model:12 of
msgid "The constructed classifier."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Albumentations.rst:7
msgid "Albumentations"
msgstr ""

#: mmcls.datasets.transforms.formatting.Collect:3 mmcls.datasets.transforms.formatting.PackClsInputs:3
#: mmcls.datasets.transforms.formatting.ToNumpy:3 mmcls.datasets.transforms.formatting.ToPIL:3
#: mmcls.datasets.transforms.formatting.Transpose:3 mmcls.datasets.transforms.processing.Albumentations:3
#: mmcls.datasets.transforms.processing.ColorJitter:7
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:3
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:3
#: mmcls.datasets.transforms.processing.Lighting:3 mmcls.datasets.transforms.processing.RandomCrop:3
#: mmcls.datasets.transforms.processing.RandomErasing:3
#: mmcls.datasets.transforms.processing.RandomResizedCrop:7 mmcls.datasets.transforms.processing.ResizeEdge:3
#: of
msgid "**Required Keys:**"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:5 mmcls.datasets.transforms.formatting.ToPIL:5
#: mmcls.datasets.transforms.formatting.ToPIL:9 mmcls.datasets.transforms.processing.Albumentations:5
#: mmcls.datasets.transforms.processing.Albumentations:9 mmcls.datasets.transforms.processing.ColorJitter:9
#: mmcls.datasets.transforms.processing.ColorJitter:13
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:5
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:9
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:5
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:9
#: mmcls.datasets.transforms.processing.Lighting:5 mmcls.datasets.transforms.processing.Lighting:9
#: mmcls.datasets.transforms.processing.RandomCrop:5 mmcls.datasets.transforms.processing.RandomCrop:9
#: mmcls.datasets.transforms.processing.RandomErasing:5 mmcls.datasets.transforms.processing.RandomErasing:9
#: mmcls.datasets.transforms.processing.RandomResizedCrop:9
#: mmcls.datasets.transforms.processing.RandomResizedCrop:13 mmcls.datasets.transforms.processing.ResizeEdge:5
#: mmcls.datasets.transforms.processing.ResizeEdge:9 of
msgid "img"
msgstr ""

#: mmcls.datasets.transforms.formatting.ToNumpy:7 mmcls.datasets.transforms.formatting.ToPIL:7
#: mmcls.datasets.transforms.formatting.Transpose:7 mmcls.datasets.transforms.processing.Albumentations:7
#: mmcls.datasets.transforms.processing.ColorJitter:11
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:7
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:7
#: mmcls.datasets.transforms.processing.Lighting:7 mmcls.datasets.transforms.processing.RandomCrop:7
#: mmcls.datasets.transforms.processing.RandomErasing:7
#: mmcls.datasets.transforms.processing.RandomResizedCrop:11 mmcls.datasets.transforms.processing.ResizeEdge:7
#: of
msgid "**Modified Keys:**"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations:10
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:10
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:10
#: mmcls.datasets.transforms.processing.RandomCrop:10
#: mmcls.datasets.transforms.processing.RandomResizedCrop:14
#: mmcls.datasets.transforms.processing.ResizeEdge:10 of
msgid "img_shape"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations:12 of
msgid ""
"Adds custom transformations from albumentations library. More details can be found in `Albumentations "
"<https://albumentations.readthedocs.io>`_. An example of ``transforms`` is as followed:"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations:42 of
msgid "List of albumentations transform configs."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations:44 of
msgid ""
"Mapping of mmcls to albumentations fields, in format {'input key':'albumentation-style key'}. Defaults to "
"None."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations:50 mmcls.models.backbones.cspnet.CSPDarkNet:30
#: mmcls.models.backbones.cspnet.CSPNet:63 mmcls.models.backbones.cspnet.CSPResNeXt:28
#: mmcls.models.backbones.cspnet.CSPResNet:28 mmcls.models.backbones.efficientformer.EfficientFormer:53
#: mmcls.models.backbones.hrnet.HRNet:52 mmcls.models.backbones.inception_v3.InceptionV3:23
#: mmcls.models.backbones.mobileone.MobileOne:48 mmcls.models.backbones.regnet.RegNet:45
#: mmcls.models.backbones.res2net.Res2Net:56 mmcls.models.backbones.resnet.ResNet:54
#: mmcls.models.backbones.seresnet.SEResNet:56 mmcls.models.heads.margin_head.ArcFaceClsHead:9 of
msgid "示例"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.albu_builder:1 of
msgid "Import a module from albumentations."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.albu_builder:3 of
msgid ""
"It inherits some of :func:`build_from_cfg` logic. :param cfg: Config dict. It should at least contain the "
"key \"type\". :type cfg: dict"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.albu_builder:7 of
msgid "The constructed object."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.mapper:1 of
msgid "Dictionary mapper."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.mapper:3 of
msgid ""
"Renames keys according to keymap provided. :param d: old dict :type d: dict :param keymap: "
"{'old_key':'new_key'} :type keymap: dict"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.mapper:9 of
msgid "new dict."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.transform:1 of
msgid "Transform function to perform albumentations transforms."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.transform:3
#: mmcls.datasets.transforms.processing.ColorJitter.transform:3
#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform:3
#: mmcls.datasets.transforms.processing.Lighting.transform:3
#: mmcls.datasets.transforms.processing.RandomCrop.transform:3
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform:3
#: mmcls.datasets.transforms.processing.ResizeEdge.transform:3 of
msgid "Result dict from loading pipeline."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.transform:6 of
msgid "Transformed results, 'img' and 'img_shape' keys are     updated in result dict."
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.transform:8 of
msgid "Transformed results, 'img' and 'img_shape' keys are"
msgstr ""

#: mmcls.datasets.transforms.processing.Albumentations.transform:9 of
msgid "updated in result dict."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.AutoAugment.rst:7
msgid "AutoAugment"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoAugment:3 of
msgid ""
"This data augmentation is proposed in `AutoAugment: Learning Augmentation Policies from Data <https://arxiv."
"org/abs/1805.09501>`_."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoAugment:6 of
msgid ""
"The policies of auto augmentation. If string, use preset policies collection like \"imagenet\". If list, "
"Each item is a sub policies, composed by several augmentation policy dicts. When AutoAugment is called, a "
"random sub policies in ``policies`` will be selected to augment images."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoAugment:12 mmcls.datasets.transforms.auto_augment.RandAugment:38
#: of
msgid ""
"Configs of hyperparameters. Hyperparameters will be used in policies that require these arguments if these "
"arguments are not set in policy dicts. Defaults to ``dict(pad_val=128)``."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.AutoContrast.rst:7
msgid "AutoContrast"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoContrast:3 of
msgid "The probability for performing auto contrast therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoContrast:6 mmcls.datasets.transforms.auto_augment.Brightness:15
#: mmcls.datasets.transforms.auto_augment.ColorTransform:15 mmcls.datasets.transforms.auto_augment.Cutout:15
#: mmcls.datasets.transforms.auto_augment.Equalize:6 mmcls.datasets.transforms.auto_augment.Invert:6
#: mmcls.datasets.transforms.auto_augment.Posterize:11 mmcls.datasets.transforms.auto_augment.Rotate:27
#: mmcls.datasets.transforms.auto_augment.Sharpness:15 mmcls.datasets.transforms.auto_augment.Shear:23
#: mmcls.datasets.transforms.auto_augment.Solarize:10 mmcls.datasets.transforms.auto_augment.SolarizeAdd:13
#: mmcls.datasets.transforms.auto_augment.Translate:25 of
msgid "Other keyword arguments of :class:`BaseAugTransform`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.AutoContrast.transform:1
#: mmcls.datasets.transforms.auto_augment.Brightness.transform:1
#: mmcls.datasets.transforms.auto_augment.ColorTransform.transform:1
#: mmcls.datasets.transforms.auto_augment.Contrast.transform:1
#: mmcls.datasets.transforms.auto_augment.Cutout.transform:1
#: mmcls.datasets.transforms.auto_augment.Equalize.transform:1
#: mmcls.datasets.transforms.auto_augment.Invert.transform:1
#: mmcls.datasets.transforms.auto_augment.Posterize.transform:1
#: mmcls.datasets.transforms.auto_augment.Rotate.transform:1
#: mmcls.datasets.transforms.auto_augment.Sharpness.transform:1
#: mmcls.datasets.transforms.auto_augment.Shear.transform:1
#: mmcls.datasets.transforms.auto_augment.Solarize.transform:1
#: mmcls.datasets.transforms.auto_augment.SolarizeAdd.transform:1
#: mmcls.datasets.transforms.auto_augment.Translate.transform:1 of
msgid "Apply transform to results."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.BaseAugTransform.rst:7
msgid "BaseAugTransform"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:3 of
msgid ""
"This class provides several common attributions and methods to support the magnitude level mapping and "
"magnitude level randomness in :class:`RandAugment`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:7 of
msgid "Magnitude level."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:9 of
msgid ""
"For augmentation have magnitude argument, maybe \"magnitude\", \"angle\" or other, you can specify the "
"magnitude level mapping range to generate the magnitude argument. For example, assume ``total_level`` is "
"10, ``magnitude_level=3`` specify magnitude is 3 if ``magnitude_range=(0, 10)`` while specify magnitude is "
"7 if ``magnitude_range=(10, 0)``. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:17 of
msgid ""
"Deviation of magnitude noise applied.  - If positive number, the magnitude obeys normal distribution   :"
"math:`\\mathcal{N}(magnitude, magnitude_std)`. - If 0 or negative number, magnitude remains unchanged. - If "
"str \"inf\", the magnitude obeys uniform distribution   :math:`Uniform(min, magnitude)`.  Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:17
#: mmcls.datasets.transforms.auto_augment.RandAugment:27 of
msgid "Deviation of magnitude noise applied."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:19 of
msgid ""
"If positive number, the magnitude obeys normal distribution :math:`\\mathcal{N}(magnitude, magnitude_std)`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:21
#: mmcls.datasets.transforms.auto_augment.RandAugment:31 of
msgid "If 0 or negative number, magnitude remains unchanged."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:22
#: mmcls.datasets.transforms.auto_augment.RandAugment:32 of
msgid "If str \"inf\", the magnitude obeys uniform distribution :math:`Uniform(min, magnitude)`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:25 of
msgid "Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:27
#: mmcls.datasets.transforms.auto_augment.RandAugment:35 of
msgid "Total level for the magnitude. Defaults to 10."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:30 of
msgid "The probability for performing transformation therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform:33 of
msgid "The probability that turns the magnitude negative, which should be in range [0,1]. Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.BaseAugTransform.extra_repr:1 of
msgid "Extra repr string when auto-generating magnitude is enabled."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Brightness.rst:7
msgid "Brightness"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Brightness:3 of
msgid ""
"The magnitude used for adjusting brightness. A positive magnitude would enhance the brightness and a "
"negative magnitude would make the image darker. A magnitude=0 gives the origin img. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Brightness:9 of
msgid ""
"The probability for performing brightness adjusting therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Brightness:12
#: mmcls.datasets.transforms.auto_augment.ColorTransform:12 mmcls.datasets.transforms.auto_augment.Contrast:12
#: mmcls.datasets.transforms.auto_augment.Sharpness:12 mmcls.datasets.transforms.auto_augment.Shear:17
#: mmcls.datasets.transforms.auto_augment.Translate:19 of
msgid "The probability that turns the magnitude negative, which should be in range [0,1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Collect.rst:7
msgid "Collect"
msgstr ""

#: mmcls.datasets.transforms.formatting.Collect:5 mmcls.datasets.transforms.formatting.Transpose:5
#: mmcls.datasets.transforms.formatting.Transpose:9 of
msgid "``*keys``"
msgstr ""

#: mmcls.datasets.transforms.formatting.Collect:7 mmcls.datasets.transforms.formatting.PackClsInputs:9 of
msgid "**Deleted Keys:**"
msgstr ""

#: mmcls.datasets.transforms.formatting.Collect:9 of
msgid "All keys except those in the argument ``*keys``."
msgstr ""

#: mmcls.datasets.transforms.formatting.Collect:11 of
msgid "The keys of the fields to be collected."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.ColorJitter.rst:7
msgid "ColorJitter"
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter:3 of
msgid ""
"Modified from https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms.py Licensed "
"under the BSD 3-Clause License."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter:15 of
msgid ""
"How much to jitter brightness. brightness_factor is chosen uniformly from ``[max(0, 1 - brightness), 1 + "
"brightness]`` or the given ``[min, max]``. Should be non negative numbers. Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter:20 of
msgid ""
"How much to jitter contrast. contrast_factor is chosen uniformly from ``[max(0, 1 - contrast), 1 + "
"contrast]`` or the given ``[min, max]``. Should be non negative numbers. Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter:25 of
msgid ""
"How much to jitter saturation. saturation_factor is chosen uniformly from ``[max(0, 1 - saturation), 1 + "
"saturation]`` or the given ``[min, max]``. Should be non negative numbers. Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter:30 of
msgid ""
"How much to jitter hue. hue_factor is chosen uniformly from ``[-hue, hue]`` (0 <= hue <= 0.5) or the given "
"``[min, max]`` (-0.5 <= min <= max <= 0.5). Defaults to 0."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter.transform:1
#: mmcls.datasets.transforms.processing.Lighting.transform:1
#: mmcls.datasets.transforms.processing.ResizeEdge.transform:1 of
msgid "Transform function to resize images."
msgstr ""

#: mmcls.datasets.transforms.processing.ColorJitter.transform:6 of
msgid "ColorJitter results, 'img' key is updated in result dict."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.ColorTransform.rst:7
msgid "ColorTransform"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.ColorTransform:3 of
msgid ""
"The magnitude used for color transform. A positive magnitude would enhance the color and a negative "
"magnitude would make the image grayer. A magnitude=0 gives the origin img. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.ColorTransform:9 of
msgid "The probability for performing ColorTransform therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Contrast.rst:7
msgid "Contrast"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Contrast:3 of
msgid ""
"The magnitude used for adjusting contrast. A positive magnitude would enhance the contrast and a negative "
"magnitude would make the image grayer. A magnitude=0 gives the origin img. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Contrast:9 of
msgid ""
"The probability for performing contrast adjusting therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Cutout.rst:7
msgid "Cutout"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Cutout:3 of
msgid ""
"Expected cutout shape (h, w). If given as a single value, the value will be used for both h and w. If None, "
"generate from ``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Cutout:8 of
msgid ""
"Pixel pad_val value for constant fill. If it is a sequence, it must have the same length with the image "
"channels. Defaults to 128."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Cutout:12 of
msgid "The probability for performing cutout therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.EfficientNetCenterCrop.rst:7
msgid "EfficientNetCenterCrop"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:12 of
msgid "Expected size after cropping with the format of (h, w)."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:15
#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:18 of
msgid "The crop padding parameter in efficientnet style center crop. Defaults to 32."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:18 of
msgid ""
"Interpolation method, accepted values are 'nearest', 'bilinear', 'bicubic', 'area', 'lanczos'. Only valid "
"if ``efficientnet_style`` is True. Defaults to 'bicubic'."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:22 of
msgid ""
"The image resize backend type, accepted values are `cv2` and `pillow`. Only valid if efficientnet style is "
"True. Defaults to `cv2`."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:28
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:17
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:17
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:30 of
msgid "提示"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:29 of
msgid "If the image is smaller than the crop size, return the original image."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:31 of
msgid "The pipeline will be to first to perform the center crop with the ``crop_size_`` as:"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:34 of
msgid ""
"\\text{crop_size_} = \\frac{\\text{crop_size}}{\\text{crop_size} +\n"
"\\text{crop_padding}} \\times \\text{short_edge}"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop:39 of
msgid "And then the pipeline resizes the img to the input crop size."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform:1
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform:1 of
msgid "Transform function to randomly resized crop images."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform:6 of
msgid ""
"EfficientNet style center cropped results, 'img_shape'     key in result dict is updated according to crop "
"size."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform:8 of
msgid "EfficientNet style center cropped results, 'img_shape'"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetCenterCrop.transform:9
#: mmcls.datasets.transforms.processing.RandomCrop.transform:9
#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform:9 of
msgid "key in result dict is updated according to crop size."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.EfficientNetRandomCrop.rst:7
msgid "EfficientNetRandomCrop"
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:12 of
msgid "Desired output scale of the crop. Only int size is accepted, a square crop (size, size) is made."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:15 of
msgid "Minimum ratio of the cropped area to the original area. Defaults to 0.1."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:21
#: mmcls.datasets.transforms.processing.RandomResizedCrop:20 of
msgid "Range of the random size of the cropped image compared to the original image. Defaults to (0.08, 1.0)."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:24
#: mmcls.datasets.transforms.processing.RandomResizedCrop:23 of
msgid ""
"Range of the random aspect ratio of the cropped image compared to the original image. Defaults to (3. / 4., "
"4. / 3.)."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:28
#: mmcls.datasets.transforms.processing.RandomResizedCrop:27 of
msgid "Maximum number of attempts before falling back to Central Crop. Defaults to 10."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:31 of
msgid ""
"Interpolation method, accepted values are 'nearest', 'bilinear', 'bicubic', 'area', 'lanczos'. Defaults to "
"'bicubic'."
msgstr ""

#: mmcls.datasets.transforms.processing.EfficientNetRandomCrop:35
#: mmcls.datasets.transforms.processing.RandomResizedCrop:34 of
msgid "The image resize backend type, accepted values are 'cv2' and 'pillow'. Defaults to 'cv2'."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Equalize.rst:7
msgid "Equalize"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Equalize:3 of
msgid "The probability for performing equalize therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Invert.rst:7
msgid "Invert"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Invert:3 of
msgid "The probability for performing invert therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Lighting.rst:7
msgid "Lighting"
msgstr ""

#: mmcls.datasets.transforms.processing.Lighting:11 of
msgid "the eigenvalue of the convariance matrix of pixel values, respectively."
msgstr ""

#: mmcls.datasets.transforms.processing.Lighting:14 of
msgid "the eigenvector of the convariance matrix of pixel values, respectively."
msgstr ""

#: mmcls.datasets.transforms.processing.Lighting:17 of
msgid "The standard deviation for distribution of alpha. Defaults to 0.1."
msgstr ""

#: mmcls.datasets.transforms.processing.Lighting:20 of
msgid "Whether to convert img to rgb. Defaults to False."
msgstr ""

#: mmcls.datasets.transforms.processing.Lighting.transform:6 of
msgid "Lightinged results, 'img' key is updated in result dict."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.PackClsInputs.rst:7
msgid "PackClsInputs"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:6 of
msgid "gt_label (optional)"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:7 of
msgid "``*meta_keys`` (optional)"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:11 of
msgid "All keys in the dict."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:13 mmcls.datasets.transforms.processing.ResizeEdge:12 of
msgid "**Added Keys:**"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:15 of
msgid "inputs (:obj:`torch.Tensor`): The forward data of models."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:16 of
msgid "data_samples (:obj:`~mmcls.structures.ClsDataSample`): The annotation info of the sample."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:19 of
msgid ""
"The meta keys to be saved in the ``metainfo`` of the packed ``data_samples``. Defaults to a tuple includes "
"keys:  - ``sample_idx``: The id of the image sample. - ``img_path``: The path to the image file. - "
"``ori_shape``: The original shape of the image as a tuple (H, W). - ``img_shape``: The shape of the image "
"after the pipeline as a   tuple (H, W). - ``scale_factor``: The scale factor between the resized image "
"and   the original image. - ``flip``: A boolean indicating if image flip transform was used. - "
"``flip_direction``: The flipping direction."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:19 of
msgid ""
"The meta keys to be saved in the ``metainfo`` of the packed ``data_samples``. Defaults to a tuple includes "
"keys:"
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:23 of
msgid "``sample_idx``: The id of the image sample."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:24 of
msgid "``img_path``: The path to the image file."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:25 of
msgid "``ori_shape``: The original shape of the image as a tuple (H, W)."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:26 of
msgid "``img_shape``: The shape of the image after the pipeline as a tuple (H, W)."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:28 of
msgid "``scale_factor``: The scale factor between the resized image and the original image."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:30 of
msgid "``flip``: A boolean indicating if image flip transform was used."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs:31 of
msgid "``flip_direction``: The flipping direction."
msgstr ""

#: mmcls.datasets.transforms.formatting.PackClsInputs.transform:1 of
msgid "Method to pack the input data."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Posterize.rst:7
msgid "Posterize"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Posterize:3 of
msgid ""
"Number of bits for each pixel in the output img, which should be less or equal to 8. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Posterize:8 of
msgid "The probability for posterizing therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.RandAugment.rst:7
msgid "RandAugment"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:3 of
msgid ""
"This data augmentation is proposed in `RandAugment: Practical automated data augmentation with a reduced "
"search space <https://arxiv.org/abs/1909.13719>`_."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:7 of
msgid ""
"The policies of random augmentation. If string, use preset policies collection like \"timm_increasing\". If "
"list, each item is one specific augmentation policy dict. The policy dict shall should have these keys:  - "
"``type`` (str), The type of augmentation. - ``magnitude_range`` (Sequence[number], optional): For those   "
"augmentation have magnitude, you need to specify the magnitude   level mapping range. For example, assume "
"``total_level`` is 10,   ``magnitude_level=3`` specify magnitude is 3 if   ``magnitude_range=(0, 10)`` "
"while specify magnitude is 7 if   ``magnitude_range=(10, 0)``. - other keyword arguments of the "
"augmentation."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:7 of
msgid ""
"The policies of random augmentation. If string, use preset policies collection like \"timm_increasing\". If "
"list, each item is one specific augmentation policy dict. The policy dict shall should have these keys:"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:12 of
msgid "``type`` (str), The type of augmentation."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:13 of
msgid ""
"``magnitude_range`` (Sequence[number], optional): For those augmentation have magnitude, you need to "
"specify the magnitude level mapping range. For example, assume ``total_level`` is 10, ``magnitude_level=3`` "
"specify magnitude is 3 if ``magnitude_range=(0, 10)`` while specify magnitude is 7 if "
"``magnitude_range=(10, 0)``."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:19 of
msgid "other keyword arguments of the augmentation."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:21 of
msgid "Number of policies to select from policies each time."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:24 of
msgid "Magnitude level for all the augmentation selected."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:27 of
msgid ""
"Deviation of magnitude noise applied.  - If positive number, the magnitude obeys normal distribution   :"
"math:`\\mathcal{N}(magnitude_level, magnitude_std)`. - If 0 or negative number, magnitude remains "
"unchanged. - If str \"inf\", the magnitude obeys uniform distribution   :math:`Uniform(min, magnitude)`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:29 of
msgid ""
"If positive number, the magnitude obeys normal distribution :math:`\\mathcal{N}(magnitude_level, "
"magnitude_std)`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:45 of
msgid ""
"To use \"timm-increasing\" policies collection, select two policies every time, and magnitude_level of "
"every policy is 6 (total is 10 by default)"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:60 of
msgid ""
"If you want the ``magnitude_level`` randomly changes every time, you can use ``magnitude_std`` to specify "
"the random distribution. For example, a normal distribution :math:`\\mathcal{N}(6, 0.5)`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:71 of
msgid "You can also use your own policies:"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:86 of
msgid ""
"``magnitude_std`` will introduce some randomness to policy, modified by https://github.com/rwightman/"
"pytorch-image-models."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:89 of
msgid "When magnitude_std=0, we calculate the magnitude as follows:"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment:91 of
msgid ""
"\\text{magnitude} = \\frac{\\text{magnitude_level}}\n"
"{\\text{totallevel}} \\times (\\text{val2} - \\text{val1})\n"
"+ \\text{val1}\n"
"\n"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.RandAugment.transform:1 of
msgid "Randomly choose a sub-policy to apply."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.RandomCrop.rst:7
msgid "RandomCrop"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:12 of
msgid ""
"Desired output size of the crop. If crop_size is an int instead of sequence like (h, w), a square crop "
"(crop_size, crop_size) is made."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:16 of
msgid ""
"Optional padding on each border of the image. If a sequence of length 4 is provided, it is used to pad "
"left, top, right, bottom borders respectively.  If a sequence of length 2 is provided, it is used to pad "
"left/right, top/bottom borders, respectively. Default: None, which means no padding."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:22 of
msgid ""
"It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is "
"done after padding, the padding seems to be done at a random offset. Default: False."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:27 of
msgid ""
"Pixel pad_val value for constant fill. If a tuple of length 3, it is used to pad_val R, G, B channels "
"respectively. Default: 0."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:31 of
msgid ""
"Type of padding. Defaults to \"constant\". Should be one of the following:  - ``constant``: Pads with a "
"constant value, this value is specified   with pad_val. - ``edge``: pads with the last value at the edge of "
"the image. - ``reflect``: Pads with reflection of image without repeating the   last value on the edge. For "
"example, padding [1, 2, 3, 4]   with 2 elements on both sides in reflect mode will result   in [3, 2, 1, 2, "
"3, 4, 3, 2]. - ``symmetric``: Pads with reflection of image repeating the last   value on the edge. For "
"example, padding [1, 2, 3, 4] with   2 elements on both sides in symmetric mode will result in   [2, 1, 1, "
"2, 3, 4, 4, 3]."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:31 of
msgid "Type of padding. Defaults to \"constant\". Should be one of the following:"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:34 of
msgid "``constant``: Pads with a constant value, this value is specified with pad_val."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:36 of
msgid "``edge``: pads with the last value at the edge of the image."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:37 of
msgid ""
"``reflect``: Pads with reflection of image without repeating the last value on the edge. For example, "
"padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop:41 of
msgid ""
"``symmetric``: Pads with reflection of image repeating the last value on the edge. For example, padding [1, "
"2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop.transform:1 of
msgid "Transform function to randomly crop images."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop.transform:6 of
msgid "Randomly cropped results, 'img_shape'     key in result dict is updated according to crop size."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomCrop.transform:8 of
msgid "Randomly cropped results, 'img_shape'"
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.RandomErasing.rst:7
msgid "RandomErasing"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:11 of
msgid "Probability that image will be randomly erased. Default: 0.5"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:14 of
msgid "Minimum erased area / input image area Default: 0.02"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:17 of
msgid "Maximum erased area / input image area Default: 0.4"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:20 of
msgid ""
"Aspect ratio range of erased area. if float, it will be converted to (aspect_ratio, 1/aspect_ratio) "
"Default: (3/10, 10/3)"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:24 of
msgid ""
"Fill method in erased area, can be:  - const (default): All pixels are assign with the same value. - rand: "
"each pixel is assigned with a random value in [0, 255]"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:24 of
msgid "Fill method in erased area, can be:"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:26 of
msgid "const (default): All pixels are assign with the same value."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:27 of
msgid "rand: each pixel is assigned with a random value in [0, 255]"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:29 of
msgid "Base color filled in erased area. Defaults to (128, 128, 128)."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:32 of
msgid ""
"If set and ``mode`` is 'rand', fill erased area with random color from normal distribution "
"(mean=fill_color, std=fill_std); If not set, fill erased area with random color from uniform distribution "
"(0~255). Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:40 of
msgid "See `Random Erasing Data Augmentation <https://arxiv.org/pdf/1708.04896.pdf>`_"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:43 of
msgid ""
"This paper provided 4 modes: RE-R, RE-M, RE-0, RE-255, and use RE-M as default. The config of these 4 modes "
"are:"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:46 of
msgid "RE-R: RandomErasing(mode='rand')"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:47 of
msgid "RE-M: RandomErasing(mode='const', fill_color=(123.67, 116.3, 103.5))"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:48 of
msgid "RE-0: RandomErasing(mode='const', fill_color=0)"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing:49 of
msgid "RE-255: RandomErasing(mode='const', fill_color=255)"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing.transform:1 of
msgid "Results dict from pipeline"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomErasing.transform:4 of
msgid "Results after the transformation."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.RandomResizedCrop.rst:7
msgid "RandomResizedCrop"
msgstr ""

#: mmcls.datasets.transforms.processing.RandomResizedCrop:3 of
msgid ""
"A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of "
"3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to given size."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomResizedCrop:16 of
msgid ""
"Desired output scale of the crop. If size is an int instead of sequence like (h, w), a square crop (size, "
"size) is made."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomResizedCrop:30 of
msgid ""
"Interpolation method, accepted values are 'nearest', 'bilinear', 'bicubic', 'area', 'lanczos'. Defaults to "
"'bilinear'."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform:6 of
msgid ""
"Randomly resized cropped results, 'img_shape'     key in result dict is updated according to crop size."
msgstr ""

#: mmcls.datasets.transforms.processing.RandomResizedCrop.transform:8 of
msgid "Randomly resized cropped results, 'img_shape'"
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.ResizeEdge.rst:7
msgid "ResizeEdge"
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:14 of
msgid "scale"
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:15 of
msgid "scale_factor"
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:17 of
msgid "The edge scale to resizing."
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:19 of
msgid "The edge to resize. Defaults to 'short'."
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:21 of
msgid ""
"Image resize backend, choices are 'cv2' and 'pillow'. These two backends generates slightly different "
"results. Defaults to 'cv2'."
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge:25 of
msgid ""
"Interpolation method, accepted values are \"nearest\", \"bilinear\", \"bicubic\", \"area\", \"lanczos\" for "
"'cv2' backend, \"nearest\", \"bilinear\" for 'pillow' backend. Defaults to 'bilinear'."
msgstr ""

#: mmcls.datasets.transforms.processing.ResizeEdge.transform:6 of
msgid "Resized results, 'img', 'scale', 'scale_factor', 'img_shape' keys are updated in result dict."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Rotate.rst:7
msgid "Rotate"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:3 of
msgid ""
"The angle used for rotate. Positive values stand for clockwise rotation. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:8 of
msgid ""
"Center point (w, h) of the rotation in the source image. If None, the center of the image will be used. "
"Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:12 of
msgid "Isotropic scale factor. Defaults to 1.0."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:14 mmcls.datasets.transforms.auto_augment.Shear:7
#: mmcls.datasets.transforms.auto_augment.Translate:9 of
msgid ""
"Pixel pad_val value for constant fill. If a sequence of length 3, it is used to pad_val R, G, B channels "
"respectively. Defaults to 128."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:18 of
msgid "The probability for performing rotate therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:21 of
msgid "The probability that turns the angle negative, which should be in range [0,1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Rotate:24 mmcls.datasets.transforms.auto_augment.Translate:22 of
msgid ""
"Interpolation method. Options are 'nearest', 'bilinear', 'bicubic', 'area', 'lanczos'. Defaults to "
"'nearest'."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Sharpness.rst:7
msgid "Sharpness"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Sharpness:3 of
msgid ""
"The magnitude used for adjusting sharpness. A positive magnitude would enhance the sharpness and a negative "
"magnitude would make the image bulr. A magnitude=0 gives the origin img. If None, generate from "
"``magnitude_range``, see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Sharpness:9 of
msgid ""
"The probability for performing sharpness adjusting therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Shear.rst:7
msgid "Shear"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Shear:3 of
msgid ""
"The magnitude used for shear. If None, generate from ``magnitude_range``, see :class:`BaseAugTransform`. "
"Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Shear:11 of
msgid "The probability for performing shear therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Shear:14 of
msgid "The shearing direction. Options are 'horizontal' and 'vertical'. Defaults to 'horizontal'."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Shear:20 of
msgid ""
"Interpolation method. Options are 'nearest', 'bilinear', 'bicubic', 'area', 'lanczos'. Defaults to "
"'bicubic'."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Solarize.rst:7
msgid "Solarize"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Solarize:3 of
msgid ""
"The threshold above which the pixels value will be inverted. If None, generate from ``magnitude_range``, "
"see :class:`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Solarize:7 mmcls.datasets.transforms.auto_augment.SolarizeAdd:10 of
msgid "The probability for solarizing therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.SolarizeAdd.rst:7
msgid "SolarizeAdd"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.SolarizeAdd:3 of
msgid ""
"The value to be added to pixels below the thr. If None, generate from ``magnitude_range``, see :class:"
"`BaseAugTransform`. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.SolarizeAdd:7 of
msgid "The threshold below which the pixels value will be adjusted."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.ToNumpy.rst:7
msgid "ToNumpy"
msgstr ""

#: mmcls.datasets.transforms.formatting.ToNumpy:5 mmcls.datasets.transforms.formatting.ToNumpy:9 of
msgid "``*keys**``"
msgstr ""

#: mmcls.datasets.transforms.formatting.ToNumpy:11 of
msgid "The dtype of the converted numpy array. Defaults to None."
msgstr ""

#: mmcls.datasets.transforms.formatting.ToNumpy.transform:1 of
msgid "Method to convert object to :obj:`numpy.ndarray`."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.ToPIL.rst:7
msgid "ToPIL"
msgstr ""

#: mmcls.datasets.transforms.formatting.ToPIL.transform:1 of
msgid "Method to convert images to :obj:`PIL.Image.Image`."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Translate.rst:7
msgid "Translate"
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Translate:3 of
msgid ""
"The magnitude used for translate. Note that the offset is calculated by magnitude * size in the "
"corresponding direction. With a magnitude of 1, the whole image will be moved out of the range. If None, "
"generate from ``magnitude_range``, see :class:`BaseAugTransform`."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Translate:13 of
msgid "The probability for performing translate therefore should be in range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.transforms.auto_augment.Translate:16 of
msgid "The translating direction. Options are 'horizontal' and 'vertical'. Defaults to 'horizontal'."
msgstr ""

#: ../../api/generated/mmcls.datasets.transforms.Transpose.rst:7
msgid "Transpose"
msgstr ""

#: mmcls.datasets.transforms.formatting.Transpose:11 of
msgid "The fields to convert to tensor."
msgstr ""

#: mmcls.datasets.transforms.formatting.Transpose:13 of
msgid "The output dimensions order."
msgstr ""

#: mmcls.datasets.transforms.formatting.Transpose.transform:1 of
msgid "Method to transpose array."
msgstr ""

#: ../../api/generated/mmcls.engine.hooks.ClassNumCheckHook.rst:7
msgid "ClassNumCheckHook"
msgstr ""

#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_test:1 of
msgid "Check whether the test dataset is compatible with head."
msgstr ""

#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_test:3
#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_train:3
#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_val:3 of
msgid "`IterBasedRunner`): Iter based Runner."
msgstr ""

#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_train:1 of
msgid "Check whether the training dataset is compatible with head."
msgstr ""

#: mmcls.engine.hooks.class_num_check_hook.ClassNumCheckHook.before_val:1 of
msgid "Check whether the validation dataset is compatible with head."
msgstr ""

#: ../../api/generated/mmcls.engine.hooks.PreciseBNHook.rst:7
msgid "PreciseBNHook"
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:3 of
msgid ""
"Recompute and update the batch norm stats to make them more precise. During training both BN stats and the "
"weight are changing after every iteration, so the running average can not precisely reflect the actual "
"stats of the current model."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:8 of
msgid ""
"With this hook, the BN stats are recomputed with fixed weights, to make the running average more precise. "
"Specifically, it computes the true average of per-batch mean/variance instead of the running average. See "
"Sec. 3 of the paper `Rethinking Batch in BatchNorm <https://arxiv.org/abs/2105.07576>` for details."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:14 of
msgid ""
"This hook will update BN stats, so it should be executed before ``CheckpointHook`` and ``EMAHook``, "
"generally set its priority to \"ABOVE_NORMAL\"."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:18 of
msgid "The number of samples to update the bn stats. Defaults to 8192."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:21 of
msgid "Perform precise bn interval. If the train loop is"
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:23 mmcls.engine.hooks.precise_bn_hook.PreciseBNHook:25 of
msgid "train loop is `IterBasedTrainLoop` or `by_epoch=False`, its unit is 'iter'. Defaults to 1."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_epoch:1
#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_iter:1 of
msgid "Calculate prcise BN and broadcast BN stats across GPUs."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_epoch:3
#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_iter:3 of
msgid "`Runner`): The runner of the training process."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_iter:4 of
msgid "The index of the current batch in the train loop."
msgstr ""

#: mmcls.engine.hooks.precise_bn_hook.PreciseBNHook.after_train_iter:6 of
msgid "Data from dataloader. Defaults to None."
msgstr ""

#: ../../api/generated/mmcls.engine.hooks.PrepareProtoBeforeValLoopHook.rst:7
msgid "PrepareProtoBeforeValLoopHook"
msgstr ""

#: mmcls.engine.hooks.retriever_hooks.PrepareProtoBeforeValLoopHook:3 of
msgid ""
"Since the encoders of the retriever changes during training, the prototype changes accordingly. So the "
"`prototype_vecs` needs to be regenerated before validation loop."
msgstr ""

#: ../../api/generated/mmcls.engine.hooks.SetAdaptiveMarginsHook.rst:7
msgid "SetAdaptiveMarginsHook"
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook:4 of
msgid ""
"A PyTorch implementation of paper `Google Landmark Recognition 2020 Competition Third Place Solution "
"<https://arxiv.org/abs/2010.05350>`_. The margins will be :math:`\\text{f}(n) = (marginMax - marginMin) · "
"norm(n^p) + marginMin`. The `n` indicates the number of occurrences of a category."
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook:10 of
msgid "Lower bound of margins. Defaults to 0.05."
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook:12 of
msgid "Upper bound of margins. Defaults to 0.5."
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook:14 of
msgid "The power of category freqercy. Defaults to -0.25."
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook.before_train:1 of
msgid "change the margins in ArcFaceClsHead."
msgstr ""

#: mmcls.engine.hooks.margin_head_hooks.SetAdaptiveMarginsHook.before_train:3 of
msgid "`Runner`): Runner."
msgstr ""

#: ../../api/generated/mmcls.engine.hooks.VisualizationHook.rst:7
msgid "VisualizationHook"
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:1 of
msgid "Classification Visualization Hook. Used to visualize validation and testing prediction results."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:4 of
msgid "If ``out_dir`` is specified, all storage backends are ignored and save the image to the ``out_dir``."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:6 of
msgid ""
"If ``show`` is True, plot the result image in a window, please confirm you are able to access the graphical "
"interface."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:9 of
msgid "Whether to enable this hook. Defaults to False."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:11 of
msgid "The interval of samples to visualize. Defaults to 5000."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:13 of
msgid "Whether to display the drawn image. Defaults to False."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:15 of
msgid ""
"directory where painted images will be saved in the testing process. If None, handle with the backends of "
"the visualizer. Defaults to None."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook:19 of
msgid "other keyword arguments of :meth:`mmcls.visualization.ClsVisualizer.add_datasample`."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter:1 of
msgid "Visualize every ``self.interval`` samples during test."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter:3 of
msgid "The runner of the testing process."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter:5 of
msgid "The index of the current batch in the test loop."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter:7
#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter:7 of
msgid "Data from dataloader."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_test_iter:9
#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter:9 of
msgid "Outputs from model."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter:1 of
msgid "Visualize every ``self.interval`` samples during validation."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter:3 of
msgid "The runner of the validation process."
msgstr ""

#: mmcls.engine.hooks.visualization_hook.VisualizationHook.after_val_iter:5 of
msgid "The index of the current batch in the val loop."
msgstr ""

#: ../../api/generated/mmcls.engine.optimizers.Lamb.rst:7
msgid "Lamb"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:3 of
msgid ""
"This class is copied from `timm`_. The LAMB was proposed in `Large Batch Optimization for Deep Learning - "
"Training BERT in 76 minutes`_."
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:11 of
msgid "iterable of parameters to optimize or dicts defining"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:14 of
msgid "learning rate. (default: 1e-3)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:16 of
msgid "coefficients used for computing running averages of gradient and its norm. (default: (0.9, 0.999))"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:19 of
msgid "term added to the denominator to improve numerical stability. (default: 1e-8)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:22 of
msgid "weight decay (L2 penalty) (default: 0)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:24 of
msgid "whether apply (1-beta2) to grad when calculating running averages of gradient. (default: True)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:27 of
msgid "value used to clip global grad norm (default: 1.0)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:30 of
msgid "enable LAMBC trust ratio clipping (default: False)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb:32 of
msgid "Apply adaptive learning rate to 0.0 weight decay parameter (default: False)"
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb.step:1 of
msgid "Performs a single optimization step."
msgstr ""

#: mmcls.engine.optimizers.lamb.Lamb.step:3 of
msgid "A closure that reevaluates the model and returns the loss."
msgstr ""

#: ../../api/generated/mmcls.evaluation.Accuracy.rst:7
msgid "Accuracy"
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy:3 of
msgid ""
"For either binary classification or multi-class classification, the accuracy is the fraction of correct "
"predictions in all predictions:"
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy:6 of
msgid "\\text{Accuracy} = \\frac{N_{\\text{correct}}}{N_{\\text{all}}}"
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy:10 of
msgid ""
"If the ground truth label matches one of the best **k** predictions, the sample will be regard as a "
"positive prediction. If the parameter is a tuple, all of top-k accuracy will be calculated and outputted "
"together. Defaults to 1."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy:15 of
msgid ""
"If a float, predictions with score lower than the threshold will be regard as the negative prediction. If "
"None, not apply threshold. If the parameter is a tuple, accuracy based on all thresholds will be calculated "
"and outputted together. Defaults to 0."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:22
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:58 mmcls.evaluation.metrics.single_label.Accuracy:21
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:57 of
msgid ""
"Device name used for collecting results from different ranks during distributed training. Must be 'cpu' or "
"'gpu'. Defaults to 'cpu'."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:26
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:62 mmcls.evaluation.metrics.single_label.Accuracy:25
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:61 of
msgid ""
"The prefix that will be added in the metric names to disambiguate homonymous metrics of different "
"evaluators. If prefix is not provided in the argument, self.default_prefix will be used instead. Defaults "
"to None."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:1 of
msgid "Calculate the accuracy."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:3
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:3 of
msgid "The prediction results. It can be labels (N, ), or scores of every class (N, C)."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:7
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:7 of
msgid "The target of each prediction with shape (N, )."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:10
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:10 of
msgid ""
"Predictions with scores under the thresholds are considered negative. It's only used when ``pred`` is "
"scores. None means no thresholds. Defaults to (0., )."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:15 of
msgid ""
"Predictions with scores under the thresholds are considered negative. It's only used when ``pred`` is "
"scores. Defaults to (0., )."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:20 of
msgid ""
"Accuracy.  - torch.Tensor: If the ``pred`` is a sequence of label instead of   score (number of dimensions "
"is 1). Only return a top-1 accuracy   tensor, and ignore the argument ``topk` and ``thrs``. - "
"List[List[torch.Tensor]]: If the ``pred`` is a sequence of score   (number of dimensions is 2). Return the "
"accuracy on each ``topk``   and ``thrs``. And the first dim is ``topk``, the second dim is   ``thrs``."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:20 of
msgid "Accuracy."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:22 of
msgid ""
"torch.Tensor: If the ``pred`` is a sequence of label instead of score (number of dimensions is 1). Only "
"return a top-1 accuracy tensor, and ignore the argument ``topk` and ``thrs``."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.calculate:25 of
msgid ""
"List[List[torch.Tensor]]: If the ``pred`` is a sequence of score (number of dimensions is 2). Return the "
"accuracy on each ``topk`` and ``thrs``. And the first dim is ``topk``, the second dim is ``thrs``."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics:1
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics:1 of
msgid "Compute the metrics from processed results."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics:3
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics:3 of
msgid "The processed results of each batch."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.compute_metrics:6
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.compute_metrics:6 of
msgid "The computed metrics. The keys are the names of the metrics, and the values are corresponding results."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
#: mmcls.evaluation.metrics.single_label.Accuracy.process:1
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.process:1 of
msgid "Process one batch of data samples."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.process:3
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.process:3 of
msgid ""
"The processed results should be stored in ``self.results``, which will be used to computed the metrics when "
"all batches have been processed."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.process:6
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.process:6 of
msgid "A batch of data from the dataloader."
msgstr ""

#: mmcls.evaluation.metrics.single_label.Accuracy.process:7
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.process:7 of
msgid "A batch of outputs from the model."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:2
msgid "mmcls.evaluation.AveragePrecision"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:3 of
msgid ""
"AveragePrecision (AP) summarizes a precision-recall curve as the weighted mean of maximum precisions "
"obtained for any r'>r, where r is the recall:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:6 of
msgid ""
"\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n"
"\n"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:9 of
msgid "Note that no approximation is involved since the curve is piecewise constant."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:12 of
msgid ""
"How to calculate the final metrics from every category. It supports two modes:  - `\"macro\"`: Calculate "
"metrics for each category, and calculate   the mean value over all categories. The result of this mode   is "
"also called **mAP**. - `None`: Calculate metrics of every category and output directly.  Defaults to \"macro"
"\"."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:12 of
msgid "How to calculate the final metrics from every category. It supports two modes:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:15 of
msgid ""
"`\"macro\"`: Calculate metrics for each category, and calculate the mean value over all categories. The "
"result of this mode is also called **mAP**."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:18
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:54
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:51
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:23 of
msgid "`None`: Calculate metrics of every category and output directly."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:20
#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:56
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:53
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:26 of
msgid "Defaults to \"macro\"."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:33 of
msgid "引用"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.AveragePrecision:34 of
msgid ""
"`Wikipedia entry for the Average precision <https://en.wikipedia.org/w/index.php?"
"title=Information_retrieval& oldid=793358396#Average_precision>`_"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:13
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:13
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:13
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:13
msgid "Methods"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
msgid ""
":py:obj:`__init__ <mmcls.evaluation.AveragePrecision.__init__>`\\ \\(\\[average\\, collect\\_device\\, "
"prefix\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
msgid ""
":py:obj:`calculate <mmcls.evaluation.AveragePrecision.calculate>`\\ \\(pred\\, target\\[\\, average\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid "Calculate the average precision for a single class."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
msgid ":py:obj:`compute_metrics <mmcls.evaluation.AveragePrecision.compute_metrics>`\\ \\(results\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
msgid ":py:obj:`evaluate <mmcls.evaluation.AveragePrecision.evaluate>`\\ \\(size\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid "Evaluate the model performance of the whole dataset after processing all batches."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:25:<autosummary>:1
msgid ":py:obj:`process <mmcls.evaluation.AveragePrecision.process>`\\ \\(data\\_batch\\, data\\_samples\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:27
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:27
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:27
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:27
msgid "Attributes"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:31:<autosummary>:1
msgid ":py:obj:`dataset_meta <mmcls.evaluation.AveragePrecision.dataset_meta>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:31:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:31:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:31:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:31:<autosummary>:1
msgid "Meta info of the dataset."
msgstr ""

#: ../../api/generated/mmcls.evaluation.AveragePrecision.rst:31:<autosummary>:1
msgid ":py:obj:`default_prefix <mmcls.evaluation.AveragePrecision.default_prefix>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:2
msgid "mmcls.evaluation.MultiLabelMetric"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:4
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:4 of
msgid ""
"The collection of metrics is for single-label multi-class classification. And all these metrics are based "
"on the confusion matrix of every category:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:11
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:11 of
msgid "All metrics can be formulated use variables above:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:13
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:13 of
msgid "**Precision** is the fraction of correct predictions in all predictions:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:15
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:15 of
msgid ""
"\\text{Precision} = \\frac{TP}{TP+FP}\n"
"\n"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:18
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:18 of
msgid "**Recall** is the fraction of correct predictions in all targets:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:20
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:20 of
msgid ""
"\\text{Recall} = \\frac{TP}{TP+FN}\n"
"\n"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:23
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:23 of
msgid "**F1-score** is the harmonic mean of the precision and recall:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:25
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:25 of
msgid ""
"\\text{F1-score} = \\frac{2\\times\\text{Recall}\\times\\text{Precision}}{\\text{Recall}+"
"\\text{Precision}}\n"
"\n"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:28
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:28 of
msgid "**Support** is the number of samples:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:30
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:30 of
msgid ""
"\\text{Support} = TP + TN + FN + FP\n"
"\n"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:33 of
msgid ""
"Predictions with scores under the threshold are considered as negative. If None, the ``topk`` predictions "
"will be considered as positive. If the ``topk`` is also None, use ``thr=0.5`` as default. Defaults to None."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:38 of
msgid ""
"Predictions with the k-th highest scores are considered as positive. If None, use ``thr`` to determine "
"positive predictions. If both ``thr`` and ``topk`` are not None, use ``thr``. Defaults to None."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:43
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:40 of
msgid ""
"The detailed metric items to evaluate, select from \"precision\", \"recall\", \"f1-score\" and \"support\". "
"Defaults to ``('precision', 'recall', 'f1-score')``."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:47
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:44 of
msgid ""
"How to calculate the final metrics from the confusion matrix of every category. It supports three modes:  - "
"`\"macro\"`: Calculate metrics for each category, and calculate   the mean value over all categories. - `"
"\"micro\"`: Average the confusion matrix over all categories and   calculate metrics on the mean confusion "
"matrix. - `None`: Calculate metrics of every category and output directly.  Defaults to \"macro\"."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:47
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:44
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:15 of
msgid ""
"How to calculate the final metrics from the confusion matrix of every category. It supports three modes:"
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:50
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:47
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:19 of
msgid "`\"macro\"`: Calculate metrics for each category, and calculate the mean value over all categories."
msgstr ""

#: mmcls.evaluation.metrics.multi_label.MultiLabelMetric:52
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:49
#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:21 of
msgid ""
"`\"micro\"`: Average the confusion matrix over all categories and calculate metrics on the mean confusion "
"matrix."
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
msgid ""
":py:obj:`__init__ <mmcls.evaluation.MultiLabelMetric.__init__>`\\ \\(\\[thr\\, topk\\, items\\, average"
"\\, ...\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
msgid ""
":py:obj:`calculate <mmcls.evaluation.MultiLabelMetric.calculate>`\\ \\(pred\\, target\\[\\, pred\\_indices"
"\\, ...\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid "Calculate the precision, recall, f1-score."
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
msgid ":py:obj:`compute_metrics <mmcls.evaluation.MultiLabelMetric.compute_metrics>`\\ \\(results\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
msgid ":py:obj:`evaluate <mmcls.evaluation.MultiLabelMetric.evaluate>`\\ \\(size\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:25:<autosummary>:1
msgid ":py:obj:`process <mmcls.evaluation.MultiLabelMetric.process>`\\ \\(data\\_batch\\, data\\_samples\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:31:<autosummary>:1
msgid ":py:obj:`dataset_meta <mmcls.evaluation.MultiLabelMetric.dataset_meta>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.MultiLabelMetric.rst:31:<autosummary>:1
msgid ":py:obj:`default_prefix <mmcls.evaluation.MultiLabelMetric.default_prefix>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.SingleLabelMetric.rst:7
msgid "SingleLabelMetric"
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:33 of
msgid ""
"If a float, predictions with score lower than the threshold will be regard as the negative prediction. If "
"None, only the top-1 prediction will be regard as the positive prediction. If the parameter is a tuple, "
"accuracy based on all thresholds will be calculated and outputted together. Defaults to 0."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric:55
#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:37 of
msgid "The number of classes. Defaults to None."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:1 of
msgid "Calculate the precision, recall, f1-score and support."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:15 of
msgid ""
"How to calculate the final metrics from the confusion matrix of every category. It supports three modes:  - "
"`\"macro\"`: Calculate metrics for each category, and calculate   the mean value over all categories. - `"
"\"micro\"`: Average the confusion matrix over all categories   and calculate metrics on the mean confusion "
"matrix. - `None`: Calculate metrics of every category and output   directly.  Defaults to \"macro\"."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:28 of
msgid ""
"The number of classes. If the ``pred`` is label instead of scores, this argument is required. Defaults to "
"None."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:33 of
msgid ""
"The tuple contains precision, recall and f1-score. And the type of each item is:  - torch.Tensor: If the "
"``pred`` is a sequence of label instead of   score (number of dimensions is 1). Only returns a tensor for   "
"each metric. The shape is (1, ) if ``classwise`` is False, and   (C, ) if ``classwise`` is True. - "
"List[torch.Tensor]: If the ``pred`` is a sequence of score   (number of dimensions is 2). Return the "
"metrics on each ``thrs``.   The shape of tensor is (1, ) if ``classwise`` is False, and (C, )   if "
"``classwise`` is True."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:33 of
msgid "The tuple contains precision, recall and f1-score. And the type of each item is:"
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:36 of
msgid ""
"torch.Tensor: If the ``pred`` is a sequence of label instead of score (number of dimensions is 1). Only "
"returns a tensor for each metric. The shape is (1, ) if ``classwise`` is False, and (C, ) if ``classwise`` "
"is True."
msgstr ""

#: mmcls.evaluation.metrics.single_label.SingleLabelMetric.calculate:40 of
msgid ""
"List[torch.Tensor]: If the ``pred`` is a sequence of score (number of dimensions is 2). Return the metrics "
"on each ``thrs``. The shape of tensor is (1, ) if ``classwise`` is False, and (C, ) if ``classwise`` is "
"True."
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:2
msgid "mmcls.evaluation.VOCAveragePrecision"
msgstr ""

#: mmcls.evaluation.metrics.voc_multi_label.VOCAveragePrecision:3
#: mmcls.evaluation.metrics.voc_multi_label.VOCMultiLabelMetric:6 of
msgid ""
"Whether to map the difficult labels as positive in one-hot ground truth for evaluation. If it set to True, "
"map difficult gt labels to positive ones(1), If it set to False, map difficult gt labels to negative "
"ones(0). Defaults to None, the difficult labels will be set to '-1'."
msgstr ""

#: mmcls.evaluation.metrics.voc_multi_label.VOCAveragePrecision:9 of
msgid "Refers to `AveragePrecision` for detailed docstrings."
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid ""
":py:obj:`__init__ <mmcls.evaluation.VOCAveragePrecision.__init__>`\\ \\(\\*arg\\[\\, difficult\\_as"
"\\_positive\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid ""
":py:obj:`calculate <mmcls.evaluation.VOCAveragePrecision.calculate>`\\ \\(pred\\, target\\[\\, average\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid ":py:obj:`compute_metrics <mmcls.evaluation.VOCAveragePrecision.compute_metrics>`\\ \\(results\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid ":py:obj:`evaluate <mmcls.evaluation.VOCAveragePrecision.evaluate>`\\ \\(size\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:25:<autosummary>:1
msgid ""
":py:obj:`process <mmcls.evaluation.VOCAveragePrecision.process>`\\ \\(data\\_batch\\, data\\_samples\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:31:<autosummary>:1
msgid ":py:obj:`dataset_meta <mmcls.evaluation.VOCAveragePrecision.dataset_meta>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCAveragePrecision.rst:31:<autosummary>:1
msgid ":py:obj:`default_prefix <mmcls.evaluation.VOCAveragePrecision.default_prefix>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:2
msgid "mmcls.evaluation.VOCMultiLabelMetric"
msgstr ""

#: mmcls.evaluation.metrics.voc_multi_label.VOCMultiLabelMetric:4 of
msgid "It includes precision, recall, f1-score and support."
msgstr ""

#: mmcls.evaluation.metrics.voc_multi_label.VOCMultiLabelMetric:12 of
msgid "Refers to `MultiLabelMetric` for detailed docstrings."
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid ""
":py:obj:`__init__ <mmcls.evaluation.VOCMultiLabelMetric.__init__>`\\ \\(\\*arg\\[\\, difficult\\_as"
"\\_positive\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid ""
":py:obj:`calculate <mmcls.evaluation.VOCMultiLabelMetric.calculate>`\\ \\(pred\\, target\\[\\, pred"
"\\_indices\\, ...\\]\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid ":py:obj:`compute_metrics <mmcls.evaluation.VOCMultiLabelMetric.compute_metrics>`\\ \\(results\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid ":py:obj:`evaluate <mmcls.evaluation.VOCMultiLabelMetric.evaluate>`\\ \\(size\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:25:<autosummary>:1
msgid ""
":py:obj:`process <mmcls.evaluation.VOCMultiLabelMetric.process>`\\ \\(data\\_batch\\, data\\_samples\\)"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:31:<autosummary>:1
msgid ":py:obj:`dataset_meta <mmcls.evaluation.VOCMultiLabelMetric.dataset_meta>`\\"
msgstr ""

#: ../../api/generated/mmcls.evaluation.VOCMultiLabelMetric.rst:31:<autosummary>:1
msgid ":py:obj:`default_prefix <mmcls.evaluation.VOCMultiLabelMetric.default_prefix>`\\"
msgstr ""

#: ../../api/generated/mmcls.models.backbones.AlexNet.rst:7
msgid "AlexNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.alexnet.AlexNet:1 of
msgid "`AlexNet <https://en.wikipedia.org/wiki/AlexNet>`_ backbone."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet:3 of
msgid "The input for AlexNet is a 224x224 RGB image."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet:5 mmcls.models.backbones.lenet.LeNet5:5 of
msgid ""
"number of classes for classification. The default value is -1, which uses the backbone as a feature "
"extractor without the top classifier."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.CSPDarkNet.rst:7
msgid "CSPDarkNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.cspnet.CSPDarkNet:1 of
msgid "CSP-Darknet backbone used in YOLOv4."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:3 of
msgid "Depth of CSP-Darknet. Default: 53."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:5 mmcls.models.backbones.mobileone.MobileOne:20
#: mmcls.models.backbones.regnet.RegNet:17 mmcls.models.backbones.repmlp.RepMLPNet:17
#: mmcls.models.backbones.resnest.ResNeSt:21 mmcls.models.backbones.resnet.ResNet:8
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:10 mmcls.models.backbones.resnext.ResNeXt:13
#: mmcls.models.backbones.seresnet.SEResNet:10 mmcls.models.backbones.seresnext.SEResNeXt:15 of
msgid "Number of input image channels. Default: 3."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:7 of
msgid "Output from which stages. Default: (3, )."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:10 mmcls.models.backbones.cspnet.CSPResNeXt:8
#: mmcls.models.backbones.cspnet.CSPResNet:8 mmcls.models.backbones.resnest.ResNeSt:48
#: mmcls.models.backbones.resnet.ResNet:35 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:39
#: mmcls.models.backbones.resnext.ResNeXt:40 mmcls.models.backbones.seresnet.SEResNet:37
#: mmcls.models.backbones.seresnext.SEResNeXt:42 of
msgid "Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:13 mmcls.models.backbones.cspnet.CSPResNeXt:11
#: mmcls.models.backbones.cspnet.CSPResNet:11 of
msgid "Config dict for convolution layer. Default: None."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:15 mmcls.models.backbones.cspnet.CSPResNeXt:13
#: mmcls.models.backbones.cspnet.CSPResNet:13 of
msgid "Dictionary to construct and config norm layer. Default: dict(type='BN', requires_grad=True)."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:18 mmcls.models.backbones.cspnet.CSPResNeXt:16
#: mmcls.models.backbones.cspnet.CSPResNet:16 of
msgid "Config dict for activation layer. Default: dict(type='LeakyReLU', negative_slope=0.1)."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:21 mmcls.models.backbones.cspnet.CSPResNeXt:19
#: mmcls.models.backbones.cspnet.CSPResNet:19 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch "
"Norm and its variants only."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPDarkNet:25 mmcls.models.backbones.cspnet.CSPResNeXt:23
#: mmcls.models.backbones.cspnet.CSPResNet:23 of
msgid "Initialization config dict. Default: None."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.CSPNet.rst:7
msgid "CSPNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.cspnet.CSPNet:1 of
msgid "The abstract CSP Network class."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:3 of
msgid ""
"A Pytorch implementation of `CSPNet: A New Backbone that can Enhance Learning Capability of CNN <https://"
"arxiv.org/abs/1911.11929>`_"
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:6 of
msgid ""
"This class is an abstract class because the Cross Stage Partial Network (CSPNet) is a kind of universal "
"network structure, and you network block to implement networks like CSPResNet, CSPResNeXt and CSPDarkNet."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:11 of
msgid ""
"The architecture of the CSPNet. It should have the following keys:  - block_fn (Callable): A function or "
"class to return a block   module, and it should accept at least ``in_channels``,   ``out_channels``, "
"``expansion``, ``drop_path_rate``, ``norm_cfg``   and ``act_cfg``. - in_channels (Tuple[int]): The number "
"of input channels of each   stage. - out_channels (Tuple[int]): The number of output channels of each   "
"stage. - num_blocks (Tuple[int]): The number of blocks in each stage. - expansion_ratio (float | "
"Tuple[float]): The expansion ratio in   the expand convolution of each stage. Defaults to 0.5. - "
"bottle_ratio (float | Tuple[float]): The expansion ratio of   blocks in each stage. Defaults to 2. - "
"has_downsampler (bool | Tuple[bool]): Whether to add a   downsample convolution in each stage. Defaults to "
"True - down_growth (bool | Tuple[bool]): Whether to expand the channels   in the downsampler layer of each "
"stage. Defaults to False. - block_args (dict | Tuple[dict], optional): The extra arguments to   the blocks "
"in each stage. Defaults to None."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:11 of
msgid "The architecture of the CSPNet. It should have the following keys:"
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:14 of
msgid ""
"block_fn (Callable): A function or class to return a block module, and it should accept at least "
"``in_channels``, ``out_channels``, ``expansion``, ``drop_path_rate``, ``norm_cfg`` and ``act_cfg``."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:18 of
msgid "in_channels (Tuple[int]): The number of input channels of each stage."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:20 of
msgid "out_channels (Tuple[int]): The number of output channels of each stage."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:22 of
msgid "num_blocks (Tuple[int]): The number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:23 of
msgid ""
"expansion_ratio (float | Tuple[float]): The expansion ratio in the expand convolution of each stage. "
"Defaults to 0.5."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:25 of
msgid "bottle_ratio (float | Tuple[float]): The expansion ratio of blocks in each stage. Defaults to 2."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:27 of
msgid ""
"has_downsampler (bool | Tuple[bool]): Whether to add a downsample convolution in each stage. Defaults to "
"True"
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:29 of
msgid ""
"down_growth (bool | Tuple[bool]): Whether to expand the channels in the downsampler layer of each stage. "
"Defaults to False."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:31 of
msgid ""
"block_args (dict | Tuple[dict], optional): The extra arguments to the blocks in each stage. Defaults to "
"None."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:34 of
msgid "A function or class to return a stem module. And it should accept ``in_channels``."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:22 mmcls.models.backbones.convnext.ConvNeXt:20
#: mmcls.models.backbones.cspnet.CSPNet:37 mmcls.models.backbones.densenet.DenseNet:22
#: mmcls.models.backbones.hornet.HorNet:21 mmcls.models.backbones.hrnet.HRNet:25
#: mmcls.models.backbones.mobilevit.MobileViT:25 mmcls.models.backbones.repvgg.RepVGG:21
#: mmcls.models.backbones.res2net.Res2Net:12 mmcls.models.backbones.timm_backbone.TIMMBackbone:22 of
msgid "Number of input image channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:39 of
msgid "Output from which stages. Defaults to -1, which means the last stage."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:42 mmcls.models.backbones.davit.DaViT:50
#: mmcls.models.backbones.efficientformer.EfficientFormer:35 mmcls.models.backbones.hornet.HorNet:33
#: mmcls.models.backbones.res2net.Res2Net:35 mmcls.models.backbones.swin_transformer.SwinTransformer:48
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:48 mmcls.models.backbones.van.VAN:32
#: mmcls.models.backbones.vision_transformer.VisionTransformer:51 of
msgid ""
"Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Defaults to -1."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:45 of
msgid "The config dict for conv layers in blocks. Defaults to None, which means use Conv2d."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:48 of
msgid "The config dict for norm layers. Defaults to ``dict(type='BN', eps=1e-5)``."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:51 of
msgid "The config dict for activation functions. Defaults to ``dict(type='LeakyReLU', inplace=True)``."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:54 mmcls.models.backbones.davit.DaViT:53
#: mmcls.models.backbones.efficientnet.EfficientNet:20 mmcls.models.backbones.hrnet.HRNet:33
#: mmcls.models.backbones.mobileone.MobileOne:40 mmcls.models.backbones.repvgg.RepVGG:53
#: mmcls.models.backbones.res2net.Res2Net:41 mmcls.models.backbones.swin_transformer.SwinTransformer:51
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:51 mmcls.models.backbones.van.VAN:35 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch "
"Norm and its variants only. Defaults to False."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPNet:58 of
msgid "The initialization settings. Defaults to ``dict(type='Kaiming', layer='Conv2d'))``."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.CSPResNeXt.rst:7
msgid "CSPResNeXt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.cspnet.CSPResNeXt:1 of
msgid "CSP-ResNeXt backbone."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPResNeXt:3 of
msgid "Depth of CSP-ResNeXt. Default: 50."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPResNeXt:5 mmcls.models.backbones.cspnet.CSPResNet:5 of
msgid "Output from which stages. Default: (4, )."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.CSPResNet.rst:7
msgid "CSPResNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.cspnet.CSPResNet:1 of
msgid "CSP-ResNet backbone."
msgstr ""

#: mmcls.models.backbones.cspnet.CSPResNet:3 of
msgid "Depth of CSP-ResNet. Default: 50."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.Conformer.rst:7
msgid "Conformer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.conformer.Conformer:1 of
msgid "Conformer backbone."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:3 of
msgid ""
"A PyTorch implementation of : `Conformer: Local Features Coupling Global Representations for Visual "
"Recognition <https://arxiv.org/abs/2105.03889>`_"
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:6 of
msgid "Conformer architecture. Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:8 of
msgid "The patch size. Defaults to 16."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:10 of
msgid "The base number of channels in CNN network. Defaults to 64."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:13 of
msgid "The expansion ratio of FFN network in transformer block. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:16 of
msgid "Whether use class token or not. Defaults to True."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:19 mmcls.models.backbones.deit.DistilledVisionTransformer:33
#: mmcls.models.backbones.deit3.DeiT3:38 mmcls.models.backbones.mlp_mixer.MlpMixer:29
#: mmcls.models.backbones.t2t_vit.T2T_ViT:23 mmcls.models.backbones.vision_transformer.VisionTransformer:33
#: mmcls.models.utils.inverted_residual.InvertedResidual:26 of
msgid "stochastic depth rate. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:21 mmcls.models.backbones.convmixer.ConvMixer:33
#: mmcls.models.backbones.convnext.ConvNeXt:39 mmcls.models.backbones.davit.DaViT:57
#: mmcls.models.backbones.deit.DistilledVisionTransformer:27 mmcls.models.backbones.deit3.DeiT3:32
#: mmcls.models.backbones.densenet.DenseNet:42 mmcls.models.backbones.edgenext.EdgeNeXt:56
#: mmcls.models.backbones.t2t_vit.T2T_ViT:17 mmcls.models.backbones.vision_transformer.VisionTransformer:27 of
msgid "Output from which stages. Defaults to -1, means the last stage."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:24 mmcls.models.backbones.deit.DistilledVisionTransformer:58
#: mmcls.models.backbones.deit3.DeiT3:66 mmcls.models.backbones.efficientformer.EfficientFormer:48
#: mmcls.models.backbones.hrnet.HRNet:47 mmcls.models.backbones.mlp_mixer.MlpMixer:41
#: mmcls.models.backbones.repvgg.RepVGG:59 mmcls.models.backbones.res2net.Res2Net:51
#: mmcls.models.backbones.vision_transformer.VisionTransformer:70
#: mmcls.models.classifiers.base.BaseClassifier:3 of
msgid "Initialization config dict. Defaults to None."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ConvMixer.rst:7
msgid "ConvMixer"
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:1 of
msgid "ConvMixer.                              ."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:3 of
msgid "A PyTorch implementation of : `Patches Are All You Need? <https://arxiv.org/pdf/2201.09792.pdf>`_"
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:6 of
msgid ""
"Modified from the `official repo <https://github.com/locuslab/convmixer/blob/main/convmixer.py>`_ and `timm "
"<https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convmixer.py>`_."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``ConvMixer.arch_settings``. And "
"if dict, it should include the following two keys:  - embed_dims (int): The dimensions of patch embedding. "
"- depth (int): Number of repetitions of ConvMixer Layer. - patch_size (int): The patch size. - kernel_size "
"(int): The kernel size of depthwise conv layers.  Defaults to '768/32'."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``ConvMixer.arch_settings``. And "
"if dict, it should include the following two keys:"
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:15 of
msgid "embed_dims (int): The dimensions of patch embedding."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:16 of
msgid "depth (int): Number of repetitions of ConvMixer Layer."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:17 of
msgid "patch_size (int): The patch size."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:18 of
msgid "kernel_size (int): The kernel size of depthwise conv layers."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:20 of
msgid "Defaults to '768/32'."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:24 of
msgid "The size of one patch in the patch embed layer. Defaults to 7."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:27 mmcls.models.backbones.densenet.DenseNet:36
#: mmcls.models.backbones.mobileone.MobileOne:31 mmcls.models.backbones.repvgg.RepVGG:41 of
msgid "The config dict for norm layers. Defaults to ``dict(type='BN')``."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:30 of
msgid "The config dict for activation after each convolution. Defaults to ``dict(type='GELU')``."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:36 mmcls.models.backbones.convnext.ConvNeXt:42
#: mmcls.models.backbones.densenet.DenseNet:45 mmcls.models.backbones.edgenext.EdgeNeXt:59
#: mmcls.models.backbones.efficientnet.EfficientNet:8 mmcls.models.backbones.poolformer.PoolFormer:53 of
msgid "Stages to be frozen (all param fixed). Defaults to 0, which means not freezing any parameters."
msgstr ""

#: mmcls.models.backbones.convmixer.ConvMixer:39 mmcls.models.backbones.densenet.DenseNet:48
#: mmcls.models.backbones.mobileone.MobileOne:44 mmcls.models.backbones.mobilevit.MobileViT:47
#: mmcls.models.backbones.replknet.RepLKNet:59 mmcls.models.backbones.repmlp.RepMLPNet:51
#: mmcls.models.classifiers.base.BaseClassifier:14 mmcls.models.utils.inverted_residual.InvertedResidual:31 of
msgid "Initialization config dict."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ConvNeXt.rst:7
msgid "ConvNeXt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.convnext.ConvNeXt:1 of
msgid "ConvNeXt."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:3 of
msgid "A PyTorch implementation of : `A ConvNet for the 2020s <https://arxiv.org/pdf/2201.03545.pdf>`_"
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:6 of
msgid ""
"Modified from the `official repo <https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext."
"py>`_ and `timm <https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convnext.py>`_."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``ConvNeXt.arch_settings``. And if "
"dict, it should include the following two keys:  - depths (list[int]): Number of blocks at each stage. - "
"channels (list[int]): The number of channels at each stage.  Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``ConvNeXt.arch_settings``. And if "
"dict, it should include the following two keys:"
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:15 of
msgid "depths (list[int]): Number of blocks at each stage."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:16 mmcls.models.backbones.edgenext.EdgeNeXt:14 of
msgid "channels (list[int]): The number of channels at each stage."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:18 mmcls.models.backbones.hornet.HorNet:19
#: mmcls.models.backbones.swin_transformer.SwinTransformer:19
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:21 mmcls.models.backbones.van.VAN:18 of
msgid "Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:22 of
msgid "The size of one patch in the stem layer. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:25 mmcls.models.backbones.poolformer.PoolFormer:20 of
msgid "The config dict for norm layers. Defaults to ``dict(type='LN2d', eps=1e-6)``."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:28 mmcls.models.backbones.efficientformer.EfficientFormer:38
#: mmcls.models.backbones.poolformer.PoolFormer:23 of
msgid "The config dict for activation between pointwise convolution. Defaults to ``dict(type='GELU')``."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:31 of
msgid "Whether to use linear layer to do pointwise convolution. Defaults to True."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:34 mmcls.models.backbones.efficientformer.EfficientFormer:43
#: mmcls.models.backbones.hornet.HorNet:23 mmcls.models.backbones.poolformer.PoolFormer:46 of
msgid "Stochastic depth rate. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:36 of
msgid "Init value for Layer Scale. Defaults to 1e-6."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:45 mmcls.models.backbones.hornet.HorNet:39 of
msgid ""
"Whether to globally average the feature map before the final norm layer. In the official repo, it's only "
"used in classification task. Defaults to True."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:49 mmcls.models.backbones.davit.DaViT:60
#: mmcls.models.backbones.efficientnet.EfficientNet:24 mmcls.models.backbones.hornet.HorNet:36
#: mmcls.models.backbones.hrnet.HRNet:37 mmcls.models.backbones.repvgg.RepVGG:47
#: mmcls.models.backbones.res2net.Res2Net:45 mmcls.models.backbones.swin_transformer.SwinTransformer:45
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:45
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:34
#: mmcls.models.classifiers.timm.TimmClassifier:25 mmcls.models.utils.inverted_residual.InvertedResidual:28 of
msgid ""
"Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. "
"Defaults to False."
msgstr ""

#: mmcls.models.backbones.convnext.ConvNeXt:52 mmcls.models.backbones.poolformer.PoolFormer:56
#: mmcls.models.backbones.tnt.TNT:41 of
msgid "Initialization config dict"
msgstr ""

#: ../../api/generated/mmcls.models.backbones.DaViT.rst:7
msgid "DaViT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.davit.DaViT:1 of
msgid "DaViT."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:3 of
msgid ""
"A PyTorch implement of : `DaViT: Dual Attention Vision Transformers <https://arxiv.org/abs/2204.03645v1>`_"
msgstr ""

#: mmcls.models.backbones.davit.DaViT:6 of
msgid "Inspiration from https://github.com/dingmyu/davit"
msgstr ""

#: mmcls.models.backbones.davit.DaViT:9 of
msgid ""
"DaViT architecture. If use string, choose from 'tiny', 'small', 'base' and 'large', 'huge', 'giant'. If use "
"dict, it should have below keys:  - **embed_dims** (int): The dimensions of embedding. - **depths** "
"(List[int]): The number of blocks in each stage. - **num_heads** (List[int]): The number of heads in "
"attention   modules of each stage.  Defaults to 't'."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:9 of
msgid ""
"DaViT architecture. If use string, choose from 'tiny', 'small', 'base' and 'large', 'huge', 'giant'. If use "
"dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.davit.DaViT:13 mmcls.models.backbones.deit.DistilledVisionTransformer:10
#: mmcls.models.backbones.deit3.DeiT3:15 mmcls.models.backbones.mlp_mixer.MlpMixer:10
#: mmcls.models.backbones.mvit.MViT:14 mmcls.models.backbones.swin_transformer.SwinTransformer:14
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:14
#: mmcls.models.backbones.vision_transformer.VisionTransformer:10 of
msgid "**embed_dims** (int): The dimensions of embedding."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:14 mmcls.models.backbones.hornet.HorNet:14
#: mmcls.models.backbones.swin_transformer.SwinTransformer:15
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:15 mmcls.models.backbones.van.VAN:14 of
msgid "**depths** (List[int]): The number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:15 mmcls.models.backbones.swin_transformer.SwinTransformer:16
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:16 of
msgid "**num_heads** (List[int]): The number of heads in attention modules of each stage."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:18 of
msgid "Defaults to 't'."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:20 mmcls.models.backbones.repmlp.RepMLPNet:19
#: mmcls.models.backbones.swin_transformer.SwinTransformer:25
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:27 of
msgid "The patch size in patch embedding. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:23 mmcls.models.backbones.deit.DistilledVisionTransformer:25
#: mmcls.models.backbones.deit3.DeiT3:30 mmcls.models.backbones.efficientformer.EfficientFormer:20
#: mmcls.models.backbones.mvit.MViT:25 mmcls.models.backbones.swin_transformer.SwinTransformer:28
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:30 mmcls.models.backbones.van.VAN:23
#: mmcls.models.backbones.vision_transformer.VisionTransformer:25 of
msgid "The num of input channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:25 mmcls.models.backbones.swin_transformer.SwinTransformer:30
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:32 of
msgid "The height and width of the window. Defaults to 7."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:27 of
msgid "The expansion ratio of feedforward network hidden layer channels. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:30 mmcls.models.backbones.deit.DistilledVisionTransformer:35
#: mmcls.models.backbones.deit3.DeiT3:40 mmcls.models.backbones.vision_transformer.VisionTransformer:35 of
msgid "Whether to add bias for qkv in attention modules. Defaults to True."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:33 mmcls.models.backbones.mvit.MViT:31
#: mmcls.models.backbones.swin_transformer.SwinTransformer:34
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:37 mmcls.models.backbones.van.VAN:27 of
msgid "Stochastic depth rate. Defaults to 0.1."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:35 mmcls.models.backbones.swin_transformer.SwinTransformer:36 of
msgid "Whether to output the feature map of a stage after the following downsample layer. Defaults to False."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:38 mmcls.models.backbones.swin_transformer.SwinTransformer:55
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:55
#: mmcls.models.utils.attention.ShiftWindowMSA:15 of
msgid ""
"If True, pad the small feature map to the window size, which is common used in detection and segmentation. "
"If False, avoid shifting window and shrink the window size to the size of feature map, which is common used "
"in classification. Defaults to False."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:44 mmcls.models.backbones.swin_transformer.SwinTransformer:61
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:61 mmcls.models.backbones.van.VAN:39 of
msgid "Config dict for normalization layer for all output features. Defaults to ``dict(type='LN')``"
msgstr ""

#: mmcls.models.backbones.davit.DaViT:47 mmcls.models.backbones.swin_transformer.SwinTransformer:64
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:64 of
msgid "Extra config dict for each stage. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.davit.DaViT:63 mmcls.models.backbones.hornet.HorNet:43
#: mmcls.models.backbones.mvit.MViT:75 mmcls.models.backbones.swin_transformer.SwinTransformer:70
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:73 mmcls.models.backbones.t2t_vit.T2T_ViT:46
#: mmcls.models.backbones.twins.PCPVT:41 mmcls.models.backbones.twins.SVT:42 mmcls.models.backbones.van.VAN:45
#: mmcls.models.utils.attention.MultiheadAttention:35 of
msgid "The Config for initialization. Defaults to None."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.DeiT3.rst:7
msgid "DeiT3"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.deit3.DeiT3:1 of
msgid "DeiT3 backbone."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:3 of
msgid "A PyTorch implement of : `DeiT III: Revenge of the ViT <https://arxiv.org/pdf/2204.07118.pdf>`_"
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:6 of
msgid "The differences between DeiT3 & VisionTransformer:"
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:8 of
msgid "Use LayerScale."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:9 of
msgid "Concat cls token after adding pos_embed."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:11 of
msgid ""
"DeiT3 architecture. If use string, choose from 'small', 'base', 'medium', 'large' and 'huge'. If use dict, "
"it should have below keys:  - **embed_dims** (int): The dimensions of embedding. - **num_layers** (int): "
"The number of transformer encoder layers. - **num_heads** (int): The number of heads in attention modules. "
"- **feedforward_channels** (int): The hidden dimensions in   feedforward modules.  Defaults to 'base'."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:11 of
msgid ""
"DeiT3 architecture. If use string, choose from 'small', 'base', 'medium', 'large' and 'huge'. If use dict, "
"it should have below keys:"
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:11 mmcls.models.backbones.deit3.DeiT3:16
#: mmcls.models.backbones.vision_transformer.VisionTransformer:11 of
msgid "**num_layers** (int): The number of transformer encoder layers."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:12 mmcls.models.backbones.deit3.DeiT3:17
#: mmcls.models.backbones.vision_transformer.VisionTransformer:12 of
msgid "**num_heads** (int): The number of heads in attention modules."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:13 mmcls.models.backbones.deit3.DeiT3:18
#: mmcls.models.backbones.vision_transformer.VisionTransformer:13 of
msgid "**feedforward_channels** (int): The hidden dimensions in feedforward modules."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:21 mmcls.models.backbones.mlp_mixer.MlpMixer:16
#: mmcls.models.backbones.mvit.MViT:21 mmcls.models.backbones.vision_transformer.VisionTransformer:16 of
msgid "Defaults to 'base'."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:18 mmcls.models.backbones.deit3.DeiT3:23
#: mmcls.models.backbones.swin_transformer.SwinTransformer:21
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:23 mmcls.models.backbones.t2t_vit.T2T_ViT:6
#: mmcls.models.backbones.vision_transformer.VisionTransformer:18 of
msgid ""
"The expected input image shape. Because we support dynamic input shape, just set the argument to the most "
"common input image shape. Defaults to 224."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:22 mmcls.models.backbones.deit3.DeiT3:27
#: mmcls.models.backbones.mlp_mixer.MlpMixer:20 mmcls.models.backbones.vision_transformer.VisionTransformer:22
#: of
msgid "The patch size in patch embedding. Defaults to 16."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:30 mmcls.models.backbones.deit3.DeiT3:35
#: mmcls.models.backbones.mlp_mixer.MlpMixer:26 mmcls.models.backbones.twins.PCPVT:27
#: mmcls.models.backbones.vision_transformer.VisionTransformer:30 of
msgid "Probability of an element to be zeroed. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:38 mmcls.models.backbones.deit3.DeiT3:43
#: mmcls.models.backbones.mlp_mixer.MlpMixer:31 mmcls.models.backbones.t2t_vit.T2T_ViT:25
#: mmcls.models.backbones.twins.PCPVT:35 mmcls.models.backbones.twins.SVT:36
#: mmcls.models.backbones.vision_transformer.VisionTransformer:38 mmcls.models.utils.embed.PatchMerging:35 of
msgid "Config dict for normalization layer. Defaults to ``dict(type='LN')``."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:41 mmcls.models.backbones.deit3.DeiT3:46
#: mmcls.models.backbones.repmlp.RepMLPNet:42 mmcls.models.backbones.t2t_vit.T2T_ViT:28
#: mmcls.models.backbones.vision_transformer.VisionTransformer:41 of
msgid "Whether to add a additional layer to normalize final feature map. Defaults to True."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:44 mmcls.models.backbones.deit3.DeiT3:49
#: mmcls.models.backbones.t2t_vit.T2T_ViT:31 mmcls.models.backbones.vision_transformer.VisionTransformer:44 of
msgid "Whether concatenating class token into image tokens as transformer input. Defaults to True."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:47 mmcls.models.backbones.deit3.DeiT3:52
#: mmcls.models.backbones.t2t_vit.T2T_ViT:34 mmcls.models.backbones.vision_transformer.VisionTransformer:54 of
msgid "Whether output the cls_token. If set True, ``with_cls_token`` must be True. Defaults to True."
msgstr ""

#: mmcls.models.backbones.deit3.DeiT3:55 of
msgid "Whether to use layer_scale in  DeiT3. Defaults to True."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:50 mmcls.models.backbones.deit3.DeiT3:58
#: mmcls.models.backbones.t2t_vit.T2T_ViT:37 mmcls.models.backbones.vision_transformer.VisionTransformer:62 of
msgid "Select the interpolate mode for position embeding vector resize. Defaults to \"bicubic\"."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:53 mmcls.models.backbones.deit3.DeiT3:61
#: mmcls.models.backbones.mlp_mixer.MlpMixer:36 mmcls.models.backbones.vision_transformer.VisionTransformer:65
#: of
msgid "Configs of patch embeding. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:55 mmcls.models.backbones.deit3.DeiT3:63
#: mmcls.models.backbones.t2t_vit.T2T_ViT:43 mmcls.models.backbones.vision_transformer.VisionTransformer:67 of
msgid "Configs of each transformer layer in encoder. Defaults to an empty dict."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.DenseNet.rst:7
msgid "DenseNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.densenet.DenseNet:1 of
msgid "DenseNet."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:3 of
msgid ""
"A PyTorch implementation of : `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993."
"pdf>`_"
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:6 of
msgid ""
"Modified from the `official repo <https://github.com/liuzhuang13/DenseNet>`_ and `pytorch <https://github."
"com/pytorch/vision/blob/main/torchvision/models/densenet.py>`_."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``DenseNet.arch_settings``. And if "
"dict, it should include the following two keys:  - growth_rate (int): Each layer of DenseBlock produce `k` "
"feature   maps. Here refers `k` as the growth rate of the network. - depths (list[int]): Number of repeated "
"layers in each DenseBlock. - init_channels (int): The output channels of stem layers.  Defaults to '121'."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:11 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``DenseNet.arch_settings``. And if "
"dict, it should include the following two keys:"
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:15 of
msgid ""
"growth_rate (int): Each layer of DenseBlock produce `k` feature maps. Here refers `k` as the growth rate of "
"the network."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:17 of
msgid "depths (list[int]): Number of repeated layers in each DenseBlock."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:18 of
msgid "init_channels (int): The output channels of stem layers."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:20 of
msgid "Defaults to '121'."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:24 of
msgid "Refers to channel expansion parameter of 1x1 convolution layer. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:27 of
msgid "Drop rate of Dropout Layer. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:29 of
msgid "The reduction rate of transition layers. Defaults to 0.5."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:32 of
msgid ""
"If True, uses checkpointing. Much more memory efficient, but slower. Defaults to False. See `\"paper\" "
"<https://arxiv.org/pdf/1707.06990.pdf>`_."
msgstr ""

#: mmcls.models.backbones.densenet.DenseNet:39 of
msgid "The config dict for activation after each convolution. Defaults to ``dict(type='ReLU')``."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.DistilledVisionTransformer.rst:7
msgid "DistilledVisionTransformer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.deit.DistilledVisionTransformer:1 of
msgid "Distilled Vision Transformer."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:3 of
msgid ""
"A PyTorch implement of : `Training data-efficient image transformers & distillation through attention "
"<https://arxiv.org/abs/2012.12877>`_"
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:6 of
msgid ""
"Vision Transformer architecture. If use string, choose from 'small', 'base', 'large', 'deit-tiny', 'deit-"
"small' and 'deit-base'. If use dict, it should have below keys:  - **embed_dims** (int): The dimensions of "
"embedding. - **num_layers** (int): The number of transformer encoder layers. - **num_heads** (int): The "
"number of heads in attention modules. - **feedforward_channels** (int): The hidden dimensions in   "
"feedforward modules.  Defaults to 'deit-base'."
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:6
#: mmcls.models.backbones.vision_transformer.VisionTransformer:6 of
msgid ""
"Vision Transformer architecture. If use string, choose from 'small', 'base', 'large', 'deit-tiny', 'deit-"
"small' and 'deit-base'. If use dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.deit.DistilledVisionTransformer:16 of
msgid "Defaults to 'deit-base'."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.EdgeNeXt.rst:7
msgid "EdgeNeXt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.edgenext.EdgeNeXt:1 of
msgid "EdgeNeXt."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:3 of
msgid ""
"A PyTorch implementation of: `EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile "
"Vision Applications <https://arxiv.org/abs/2206.10589>`_"
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:7 of
msgid "Inspiration from https://github.com/mmaaz60/EdgeNeXt"
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:10 of
msgid ""
"The model's architecture. If string, it should be one of architectures in ``EdgeNeXt.arch_settings``. And "
"if dict, it should include the following keys:  - channels (list[int]): The number of channels at each "
"stage. - depths (list[int]): The number of blocks at each stage. - num_heads (list[int]): The number of "
"heads at each stage.  Defaults to 'xxsmall'."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:10 of
msgid ""
"The model's architecture. If string, it should be one of architectures in ``EdgeNeXt.arch_settings``. And "
"if dict, it should include the following keys:"
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:15 of
msgid "depths (list[int]): The number of blocks at each stage."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:16 of
msgid "num_heads (list[int]): The number of heads at each stage."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:18 of
msgid "Defaults to 'xxsmall'."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:20 of
msgid "The number of input channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:23 of
msgid "The number of global blocks. Defaults to [0, 1, 1, 1]."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:26 of
msgid "The type of global blocks. Defaults to ['None', 'SDTA', 'SDTA', 'SDTA']."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:29 of
msgid "Stochastic depth dropout rate. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:32 of
msgid "Initial value of layer scale. Defaults to 1e-6."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:35 of
msgid "Whether to use linear layer to do pointwise convolution. Defaults to False."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:38 of
msgid "The number of channel ratio in MLP layers. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:41 of
msgid "The kernel size of convolutional layers at each stage. Defaults to [3, 5, 7, 9]."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:44 of
msgid ""
"Whether to use positional embedding in Channel Self-Attention. Defaults to [False, True, False, False]."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:47 of
msgid "Whether to use positional embedding for whole network. Defaults to False."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:50 of
msgid "The number of channel groups used for SDTA at each stage. Defaults to [2, 2, 3, 4]."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:53 of
msgid "The config of normalization layer. Defaults to ``dict(type='LN2d', eps=1e-6)``."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:62 of
msgid "Whether to globally average the feature map before the final norm layer. Defaults to True."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:65 of
msgid "The config of activation layer. Defaults to ``dict(type='GELU')``."
msgstr ""

#: mmcls.models.backbones.edgenext.EdgeNeXt:68 of
msgid "Config for initialization. Defaults to None."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.EfficientFormer.rst:7
msgid "EfficientFormer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.efficientformer.EfficientFormer:1 of
msgid "EfficientFormer."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:3 of
msgid ""
"A PyTorch implementation of EfficientFormer introduced by: `EfficientFormer: Vision Transformers at "
"MobileNet Speed <https://arxiv.org/abs/2206.01191>`_"
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:6 of
msgid "Modified from the `official repo <https://github.com/snap-research/EfficientFormer>`."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:9 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``EfficientFormer.arch_settings``. "
"And if dict, it should include the following 4 keys:  - layers (list[int]): Number of blocks at each stage. "
"- embed_dims (list[int]): The number of channels at each stage. - downsamples (list[int]): Has downsample "
"or not in the four stages. - vit_num (int): The num of vit blocks in the last stage.  Defaults to 'l1'."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:9 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``EfficientFormer.arch_settings``. "
"And if dict, it should include the following 4 keys:"
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:13 mmcls.models.backbones.poolformer.PoolFormer:13
#: of
msgid "layers (list[int]): Number of blocks at each stage."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:14 mmcls.models.backbones.poolformer.PoolFormer:14
#: of
msgid "embed_dims (list[int]): The number of channels at each stage."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:15 of
msgid "downsamples (list[int]): Has downsample or not in the four stages."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:16 of
msgid "vit_num (int): The num of vit blocks in the last stage."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:18 of
msgid "Defaults to 'l1'."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:22 of
msgid "The pooling size of ``Meta4D`` blocks. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:24 of
msgid "The dimension ratio of multi-head attention mechanism in ``Meta4D`` blocks. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:27 of
msgid ""
"Whether to reshape the feature map from (B, N, C) to (B, C, H, W) in the last stage, when the ``vit-num`` "
"in ``arch`` is not 0. Defaults to False. Usually set to True in downstream tasks."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:32 of
msgid "Output from which stages. Defaults to -1."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:41 mmcls.models.backbones.poolformer.PoolFormer:44
#: mmcls.models.backbones.twins.SVT:29 of
msgid "Dropout rate. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.efficientformer.EfficientFormer:45 of
msgid "Whether to use use_layer_scale in MetaFormer block. Defaults to True."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.EfficientNet.rst:7
msgid "EfficientNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.efficientnet.EfficientNet:1 of
msgid "EfficientNet backbone."
msgstr ""

#: mmcls.models.backbones.efficientnet.EfficientNet:3 of
msgid "Architecture of efficientnet. Defaults to b0."
msgstr ""

#: mmcls.models.backbones.efficientnet.EfficientNet:5 of
msgid "Output from which stages. Defaults to (6, )."
msgstr ""

#: mmcls.models.backbones.efficientnet.EfficientNet:11 mmcls.models.backbones.mobilevit.MobileViT:38
#: mmcls.models.utils.inverted_residual.InvertedResidual:17 of
msgid "Config dict for convolution layer. Defaults to None, which means using conv2d."
msgstr ""

#: mmcls.models.backbones.efficientnet.EfficientNet:14 mmcls.models.backbones.mobilevit.MobileViT:41 of
msgid "Config dict for normalization layer. Defaults to dict(type='BN')."
msgstr ""

#: mmcls.models.backbones.efficientnet.EfficientNet:17 mmcls.models.backbones.mobilevit.MobileViT:44 of
msgid "Config dict for activation layer. Defaults to dict(type='Swish')."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.HRNet.rst:7
msgid "HRNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.hrnet.HRNet:1 of
msgid "HRNet backbone."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:3 of
msgid "`High-Resolution Representations for Labeling Pixels and Regions <https://arxiv.org/abs/1904.04514>`_."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:6 of
msgid ""
"The preset HRNet architecture, includes 'w18', 'w30', 'w32', 'w40', 'w44', 'w48', 'w64'. It will only be "
"used if extra is ``None``. Defaults to 'w32'."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:10 of
msgid ""
"Detailed configuration for each stage of HRNet. There must be 4 stages, the configuration for each stage "
"must have 5 keys:  - num_modules (int): The number of HRModule in this stage. - num_branches (int): The "
"number of branches in the HRModule. - block (str): The type of convolution block. Please choose between   "
"'BOTTLENECK' and 'BASIC'. - num_blocks (tuple): The number of blocks in each branch.   The length must be "
"equal to num_branches. - num_channels (tuple): The number of base channels in each branch.   The length "
"must be equal to num_branches.  Defaults to None."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:10 of
msgid ""
"Detailed configuration for each stage of HRNet. There must be 4 stages, the configuration for each stage "
"must have 5 keys:"
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:14 of
msgid "num_modules (int): The number of HRModule in this stage."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:15 of
msgid "num_branches (int): The number of branches in the HRModule."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:16 of
msgid "block (str): The type of convolution block. Please choose between 'BOTTLENECK' and 'BASIC'."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:18 of
msgid "num_blocks (tuple): The number of blocks in each branch. The length must be equal to num_branches."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:20 of
msgid ""
"num_channels (tuple): The number of base channels in each branch. The length must be equal to num_branches."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:27 of
msgid "Dictionary to construct and config conv layer. Defaults to None."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:30 of
msgid "Dictionary to construct and config norm layer. Defaults to ``dict(type='BN')``."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:40 of
msgid ""
"Whether to use zero init for last norm layer in resblocks to let them behave as identity. Defaults to False."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet:43 of
msgid ""
"Whether to output multi-level features produced by multiple branches. If False, only the first level "
"feature will be output. Defaults to True."
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet.forward:1 mmcls.models.backbones.inception_v3.InceptionV3.forward:1
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:1
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:1 of
msgid "Forward function."
msgstr ""

#: mmcls.models.backbones.HRNet.norm1:1 of
msgid "the normalization layer named \"norm1\""
msgstr ""

#: mmcls.models.backbones.HRNet.norm1 mmcls.models.backbones.HRNet.norm2
#: mmcls.models.classifiers.base.BaseClassifier of
msgid "type"
msgstr ""

#: mmcls.models.backbones.HRNet.norm1:3 mmcls.models.backbones.HRNet.norm2:3 of
msgid "nn.Module"
msgstr ""

#: mmcls.models.backbones.HRNet.norm2:1 of
msgid "the normalization layer named \"norm2\""
msgstr ""

#: mmcls.models.backbones.hrnet.HRNet.train:1 of
msgid "Convert the model into training mode will keeping the normalization layer freezed."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.HorNet.rst:7
msgid "HorNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.hornet.HorNet:1 of
msgid "HorNet backbone."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:3 of
msgid ""
"A PyTorch implementation of paper `HorNet: Efficient High-Order Spatial Interactions with Recursive Gated "
"Convolutions <https://arxiv.org/abs/2207.14284>`_ . Inspiration from https://github.com/raoyongming/HorNet"
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:8 of
msgid ""
"HorNet architecture.  If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, it "
"should have below keys:  - **base_dim** (int): The base dimensions of embedding. - **depths** (List[int]): "
"The number of blocks in each stage. - **orders** (List[int]): The number of order of gnConv in each     "
"stage. - **dw_cfg** (List[dict]): The Config for dw conv.  Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:8 of
msgid "HorNet architecture."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:10 of
msgid ""
"If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:13 of
msgid "**base_dim** (int): The base dimensions of embedding."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:15 of
msgid "**orders** (List[int]): The number of order of gnConv in each"
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:16 of
msgid "stage."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:17 of
msgid "**dw_cfg** (List[dict]): The Config for dw conv."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:25 of
msgid "Scaling parameter of gflayer outputs. Defaults to 1/3."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:27 of
msgid "Whether to use use_layer_scale in HorNet block. Defaults to True."
msgstr ""

#: mmcls.models.backbones.hornet.HorNet:30 mmcls.models.backbones.repmlp.RepMLPNet:22
#: mmcls.models.backbones.resnet.ResNet:22 mmcls.models.backbones.van.VAN:29 of
msgid "Output from which stages. Default: ``(3, )``."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.InceptionV3.rst:7
msgid "InceptionV3"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.inception_v3.InceptionV3:1 of
msgid "Inception V3 backbone."
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:3 of
msgid ""
"A PyTorch implementation of `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/"
"abs/1512.00567>`_"
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:6 of
msgid ""
"This implementation is modified from https://github.com/pytorch/vision/blob/main/torchvision/models/"
"inception.py. Licensed under the BSD 3-Clause License."
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:10 of
msgid "The number of categroies. Defaults to 1000."
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:12 of
msgid ""
"Whether to enable the auxiliary branch. If False, the auxiliary logits output will be None. Defaults to "
"False."
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:15 of
msgid "Dropout rate. Defaults to 0.5."
msgstr ""

#: mmcls.models.backbones.inception_v3.InceptionV3:17 of
msgid ""
"The config of initialization. Defaults to use trunc normal with ``std=0.1`` for all Conv2d and Linear "
"layers and constant with ``val=1`` for all BatchNorm2d layers."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.LeNet5.rst:7
msgid "LeNet5"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.lenet.LeNet5:1 of
msgid "`LeNet5 <https://en.wikipedia.org/wiki/LeNet>`_ backbone."
msgstr ""

#: mmcls.models.backbones.lenet.LeNet5:3 of
msgid "The input for LeNet-5 is a 32×32 grayscale image."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MViT.rst:7
msgid "MViT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mvit.MViT:1 of
msgid "Multi-scale ViT v2."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:3 of
msgid ""
"A PyTorch implement of : `MViTv2: Improved Multiscale Vision Transformers for Classification and Detection "
"<https://arxiv.org/abs/2112.01526>`_"
msgstr ""

#: mmcls.models.backbones.mvit.MViT:6 of
msgid ""
"Inspiration from `the official implementation <https://github.com/facebookresearch/mvit>`_ and `the "
"detectron2 implementation <https://github.com/facebookresearch/detectron2>`_"
msgstr ""

#: mmcls.models.backbones.mvit.MViT:10 of
msgid ""
"MViT architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, it should "
"have below keys:  - **embed_dims** (int): The dimensions of embedding. - **num_layers** (int): The number "
"of layers. - **num_heads** (int): The number of heads in attention   modules of the initial layer. - "
"**downscale_indices** (List[int]): The layer indices to downscale   the feature map.  Defaults to 'base'."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:10 of
msgid ""
"MViT architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, it should "
"have below keys:"
msgstr ""

#: mmcls.models.backbones.mvit.MViT:15 of
msgid "**num_layers** (int): The number of layers."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:16 of
msgid "**num_heads** (int): The number of heads in attention modules of the initial layer."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:18 of
msgid "**downscale_indices** (List[int]): The layer indices to downscale the feature map."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:23 of
msgid "The expected input image shape. Defaults to 224."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:27 of
msgid ""
"The output scale indices. They should not exceed the length of ``downscale_indices``. Defaults to -1, which "
"means the last scale."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:33 mmcls.models.backbones.swin_transformer.SwinTransformer:39
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:39 of
msgid "If True, add absolute position embedding to the patch embedding. Defaults to False."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:36 of
msgid "Select the interpolate mode for absolute position embedding vector resize. Defaults to \"bicubic\"."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:39 of
msgid "kernel size for qkv pooling layers. Defaults to (3, 3)."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:42 of
msgid "The magnification for ``embed_dims`` in the downscale layers. Defaults to 2."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:45 of
msgid "The magnification for ``num_heads`` in the downscale layers. Defaults to 2."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:48 of
msgid "The stride size for kv pooling in the initial layer. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:51 of
msgid "Whether to enable the spatial relative position embedding. Defaults to True."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:54 of
msgid "Whether to enable the residual connection after attention pooling. Defaults to True."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:57 of
msgid ""
"Whether to multiply the ``embed_dims`` in attention layers. If False, multiply it in MLP layers. Defaults "
"to True."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:61 of
msgid "If True, zero initialize relative positional parameters. Defaults to False."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:64 of
msgid "Ratio of hidden dimensions in MLP layers. Defaults to 4.0."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:67 of
msgid "enable bias for qkv if True. Defaults to True."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:69 of
msgid ""
"Config dict for normalization layer for all output features. Defaults to ``dict(type='LN', eps=1e-6)``."
msgstr ""

#: mmcls.models.backbones.mvit.MViT:72 of
msgid "Config dict for the patch embedding layer. Defaults to ``dict(kernel_size=7, stride=4, padding=3)``."
msgstr ""

#: mmcls.models.backbones.mvit.MViT.forward:1 of
msgid "Forward the MViT."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MlpMixer.rst:7
msgid "MlpMixer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mlp_mixer.MlpMixer:1 of
msgid "Mlp-Mixer backbone."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:3 of
msgid ""
"Pytorch implementation of `MLP-Mixer: An all-MLP Architecture for Vision <https://arxiv.org/pdf/2105.01601."
"pdf>`_"
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:6 of
msgid ""
"MLP Mixer architecture. If use string, choose from 'small', 'base' and 'large'. If use dict, it should have "
"below keys:  - **embed_dims** (int): The dimensions of embedding. - **num_layers** (int): The number of MLP "
"blocks. - **tokens_mlp_dims** (int): The hidden dimensions for tokens FFNs. - **channels_mlp_dims** (int): "
"The The hidden dimensions for   channels FFNs.  Defaults to 'base'."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:6 of
msgid ""
"MLP Mixer architecture. If use string, choose from 'small', 'base' and 'large'. If use dict, it should have "
"below keys:"
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:11 of
msgid "**num_layers** (int): The number of MLP blocks."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:12 of
msgid "**tokens_mlp_dims** (int): The hidden dimensions for tokens FFNs."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:13 of
msgid "**channels_mlp_dims** (int): The The hidden dimensions for channels FFNs."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:18 of
msgid "The input image shape. Defaults to 224."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:23 of
msgid "Output from which layer. Defaults to -1, means the last layer."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:34 of
msgid "The activation config for FFNs. Default GELU."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:38 of
msgid "Configs of each mixer block layer. Defaults to an empty dict."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MobileNetV2.rst:7
msgid "MobileNetV2"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mobilenet_v2.MobileNetV2:1 of
msgid "MobileNetV2 backbone."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:3 of
msgid "Width multiplier, multiply number of channels in each layer by this amount. Default: 1.0."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:6 of
msgid "Output from which stages. Default: (7, )."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:9 mmcls.models.backbones.mobilenet_v3.MobileNetV3:15
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:12 mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:9 of
msgid "Stages to be frozen (all param fixed). Default: -1, which means not freezing any parameters."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:12 mmcls.models.backbones.mobilenet_v3.MobileNetV3:6
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:15 mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:12
#: mmcls.models.utils.se_layer.SELayer:16 of
msgid "Config dict for convolution layer. Default: None, which means using conv2d."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:15 mmcls.models.backbones.mobilenet_v3.MobileNetV3:9
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:18 mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:15
#: of
msgid "Config dict for normalization layer. Default: dict(type='BN')."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:18 of
msgid "Config dict for activation layer. Default: dict(type='ReLU6')."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:21 mmcls.models.backbones.mobilenet_v3.MobileNetV3:18
#: mmcls.models.backbones.regnet.RegNet:33 mmcls.models.backbones.resnest.ResNeSt:55
#: mmcls.models.backbones.resnet.ResNet:42 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:46
#: mmcls.models.backbones.resnext.ResNeXt:47 mmcls.models.backbones.seresnet.SEResNet:44
#: mmcls.models.backbones.seresnext.SEResNeXt:49 mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:24
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:21 mmcls.models.backbones.vgg.VGG:23 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch "
"Norm and its variants only. Default: False."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:25 mmcls.models.backbones.mobilenet_v3.MobileNetV3:22
#: mmcls.models.backbones.regnet.RegNet:37 mmcls.models.backbones.resnest.ResNeSt:59
#: mmcls.models.backbones.resnet.ResNet:46 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:50
#: mmcls.models.backbones.resnext.ResNeXt:51 mmcls.models.backbones.seresnet.SEResNet:48
#: mmcls.models.backbones.seresnext.SEResNeXt:53 mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:28
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:25 of
msgid ""
"Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. "
"Default: False."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:1 of
msgid "Stack InvertedResidual blocks to build a layer for MobileNetV2."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:3 of
msgid "out_channels of block."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:5 of
msgid "number of blocks."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:7 of
msgid "stride of the first block. Default: 1"
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:9 of
msgid "Expand the number of channels of the hidden layer in InvertedResidual by this ratio. Default: 6."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MobileNetV3.rst:7
msgid "MobileNetV3"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mobilenet_v3.MobileNetV3:1 of
msgid "MobileNetV3 backbone."
msgstr ""

#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:3 of
msgid "Architecture of mobilnetv3, from {small, large}. Default: small."
msgstr ""

#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:12 of
msgid "Output from which stages. Default: None, which means output tensors from final stage."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MobileOne.rst:7
msgid "MobileOne"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mobileone.MobileOne:1 of
msgid "MobileOne backbone."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:3 of
msgid ""
"A PyTorch impl of : `An Improved One millisecond Mobile Backbone <https://arxiv.org/pdf/2206.04040.pdf>`_"
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:6 of
msgid ""
"MobileOne architecture. If use string, choose from 's0', 's1', 's2', 's3' and 's4'. If use dict, it should "
"have below keys:  - num_blocks (Sequence[int]): Number of blocks in each stage. - width_factor "
"(Sequence[float]): Width factor in each stage. - num_conv_branches (Sequence[int]): Number of conv "
"branches   in each stage. - num_se_blocks (Sequence[int]): Number of SE layers in each   stage, all the SE "
"layers are placed in the subsequent order   in each stage.  Defaults to 's0'."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:6 of
msgid ""
"MobileOne architecture. If use string, choose from 's0', 's1', 's2', 's3' and 's4'. If use dict, it should "
"have below keys:"
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:10 of
msgid "num_blocks (Sequence[int]): Number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:11 of
msgid "width_factor (Sequence[float]): Width factor in each stage."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:12 of
msgid "num_conv_branches (Sequence[int]): Number of conv branches in each stage."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:14 of
msgid ""
"num_se_blocks (Sequence[int]): Number of SE layers in each stage, all the SE layers are placed in the "
"subsequent order in each stage."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:18 of
msgid "Defaults to 's0'."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:22 mmcls.models.backbones.repvgg.RepVGG:26
#: mmcls.models.backbones.res2net.Res2Net:22 mmcls.models.backbones.twins.PCPVT:22 of
msgid "Output from which stages. Defaults to ``(3, )``."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:25 mmcls.models.backbones.repvgg.RepVGG:35 of
msgid "Stages to be frozen (all param fixed). -1 means not freezing any parameters. Defaults to -1."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:28 mmcls.models.backbones.repvgg.RepVGG:38 of
msgid "The config dict for conv layers. Defaults to None."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:34 mmcls.models.backbones.repvgg.RepVGG:44
#: mmcls.models.utils.inverted_residual.InvertedResidual:23 of
msgid "Config dict for activation layer. Defaults to ``dict(type='ReLU')``."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne:37 mmcls.models.backbones.repvgg.RepVGG:50 of
msgid "Whether to switch the model structure to deployment mode. Defaults to False."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne.switch_to_deploy:1 of
msgid "switch the model to deploy mode, which has smaller amount of parameters and calculations."
msgstr ""

#: mmcls.models.backbones.mobileone.MobileOne.train:1 of
msgid "switch the mobile to train mode or not."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.MobileViT.rst:7
msgid "MobileViT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.mobilevit.MobileViT:1 of
msgid "MobileViT backbone."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:3 of
msgid ""
"A PyTorch implementation of : `MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision "
"Transformer <https://arxiv.org/pdf/2110.02178.pdf>`_"
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:6 of
msgid ""
"Modified from the `official repo <https://github.com/apple/ml-cvnets/blob/main/cvnets/models/classification/"
"mobilevit.py>`_ and `timm <https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/"
"mobilevit.py>`_."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:11 of
msgid ""
"Architecture of MobileViT.  - If a string, choose from \"small\", \"x_small\" and \"xx_small\".  - If a "
"list, every item should be also a list, and the first item   of the sub-list can be chosen from "
"\"moblienetv2\" and \"mobilevit\",   which indicates the type of this layer sequence. If \"mobilenetv2\",   "
"the other items are the arguments of :attr:`~MobileViT.make_mobilenetv2_layer`   (except ``in_channels``) "
"and if \"mobilevit\", the other items are   the arguments of :attr:`~MobileViT.make_mobilevit_layer`   "
"(except ``in_channels``).  Defaults to \"small\"."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:11 of
msgid "Architecture of MobileViT."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:13 of
msgid "If a string, choose from \"small\", \"x_small\" and \"xx_small\"."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:15 of
msgid ""
"If a list, every item should be also a list, and the first item of the sub-list can be chosen from "
"\"moblienetv2\" and \"mobilevit\", which indicates the type of this layer sequence. If \"mobilenetv2\", the "
"other items are the arguments of :attr:`~MobileViT.make_mobilenetv2_layer` (except ``in_channels``) and if "
"\"mobilevit\", the other items are the arguments of :attr:`~MobileViT.make_mobilevit_layer` (except "
"``in_channels``)."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:23 of
msgid "Defaults to \"small\"."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:27 of
msgid "Channels of stem layer.  Defaults to 16."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:29 of
msgid "Channels expand factor of last layer. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:32 of
msgid "Output from which stages. Defaults to (4, )."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT:35 of
msgid "Stages to be frozen (all param fixed). Defaults to -1, which means not freezing any parameters."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:1 of
msgid "Build mobilenetv2 layer, which consists of several InvertedResidual layers."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:4
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:4 of
msgid "The input channels."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:6
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:6 of
msgid "The output channels."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:8
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:8 of
msgid "The stride of the first 3x3 convolution in the ``InvertedResidual`` layers."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:11 of
msgid "The number of ``InvertedResidual`` blocks."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilenetv2_layer:13
#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:18 of
msgid "adjusts number of channels of the hidden layer in ``InvertedResidual`` by this amount. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:1 of
msgid "Build mobilevit layer, which consists of one InvertedResidual and one MobileVitBlock."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:11 of
msgid "The channels of the transformer layers."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:13 of
msgid "The mid-channels of the feedforward network in transformer layers."
msgstr ""

#: mmcls.models.backbones.mobilevit.MobileViT.make_mobilevit_layer:16 of
msgid "The number of transformer blocks."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.PCPVT.rst:7
msgid "PCPVT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.twins.PCPVT:1 of
msgid "The backbone of Twins-PCPVT."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:3 mmcls.models.backbones.twins.SVT:3 of
msgid ""
"This backbone is the implementation of `Twins: Revisiting the Design of Spatial Attention in Vision "
"Transformers <https://arxiv.org/abs/1512.03385>`_."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:7 of
msgid ""
"PCPVT architecture, a str value in arch zoo or a detailed configuration dict with 7 keys, and the length of "
"all the values in dict should be the same:  - depths (List[int]): The number of encoder layers in each "
"stage. - embed_dims (List[int]): Embedding dimension in each stage. - patch_sizes (List[int]): The patch "
"sizes in each stage. - num_heads (List[int]): Numbers of attention head in each stage. - strides "
"(List[int]): The strides in each stage. - mlp_ratios (List[int]): The ratios of mlp in each stage. - "
"sr_ratios (List[int]): The ratios of GSA-encoder layers in each   stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:7 of
msgid ""
"PCPVT architecture, a str value in arch zoo or a detailed configuration dict with 7 keys, and the length of "
"all the values in dict should be the same:"
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:11 mmcls.models.backbones.twins.SVT:11 of
msgid "depths (List[int]): The number of encoder layers in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:12 mmcls.models.backbones.twins.SVT:12 of
msgid "embed_dims (List[int]): Embedding dimension in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:13 mmcls.models.backbones.twins.SVT:13 of
msgid "patch_sizes (List[int]): The patch sizes in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:14 mmcls.models.backbones.twins.SVT:14 of
msgid "num_heads (List[int]): Numbers of attention head in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:15 mmcls.models.backbones.twins.SVT:15 of
msgid "strides (List[int]): The strides in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:16 mmcls.models.backbones.twins.SVT:16 of
msgid "mlp_ratios (List[int]): The ratios of mlp in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:17 mmcls.models.backbones.twins.SVT:17 of
msgid "sr_ratios (List[int]): The ratios of GSA-encoder layers in each stage."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:20 mmcls.models.backbones.twins.SVT:22 of
msgid "Number of input channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:25 mmcls.models.backbones.twins.SVT:27 of
msgid "Enable bias for qkv if True. Defaults to False."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:30 of
msgid "The drop out rate for attention layer. Defaults to 0.0"
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:33 of
msgid "Stochastic depth rate. Defaults to 0.0."
msgstr ""

#: mmcls.models.backbones.twins.PCPVT:38 mmcls.models.backbones.twins.SVT:39 of
msgid "Add extra norm after each stage. Defaults to False."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.PoolFormer.rst:7
msgid "PoolFormer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.poolformer.PoolFormer:1 of
msgid "PoolFormer."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:3 of
msgid ""
"A PyTorch implementation of PoolFormer introduced by: `MetaFormer is Actually What You Need for Vision "
"<https://arxiv.org/abs/2111.11418>`_"
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:6 of
msgid ""
"Modified from the `official repo <https://github.com/sail-sg/poolformer/blob/main/models/poolformer.py>`."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:9 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``PoolFormer.arch_settings``. And "
"if dict, it should include the following two keys:  - layers (list[int]): Number of blocks at each stage. - "
"embed_dims (list[int]): The number of channels at each stage. - mlp_ratios (list[int]): Expansion ratio of "
"MLPs. - layer_scale_init_value (float): Init value for Layer Scale.  Defaults to 'S12'."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:9 of
msgid ""
"The model's architecture. If string, it should be one of architecture in ``PoolFormer.arch_settings``. And "
"if dict, it should include the following two keys:"
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:15 of
msgid "mlp_ratios (list[int]): Expansion ratio of MLPs."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:16 of
msgid "layer_scale_init_value (float): Init value for Layer Scale."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:18 of
msgid "Defaults to 'S12'."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:26 of
msgid "The patch size of input image patch embedding. Defaults to 7."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:29 of
msgid "The stride of input image patch embedding. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:32 of
msgid "The padding of input image patch embedding. Defaults to 2."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:35 of
msgid "The patch size of downsampling patch embedding. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:38 of
msgid "The stride of downsampling patch embedding. Defaults to 2."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:41 of
msgid "The padding of downsampling patch embedding. Defaults to 1."
msgstr ""

#: mmcls.models.backbones.poolformer.PoolFormer:48 of
msgid ""
"Output from which network position. Index 0-6 respectively corresponds to [stage1, downsampling, stage2, "
"downsampling, stage3, downsampling, stage4] Defaults to -1, means the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.RegNet.rst:7
msgid "RegNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.regnet.RegNet:1 of
msgid "RegNet backbone."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:3 of
msgid "More details can be found in `paper <https://arxiv.org/abs/2003.13678>`_ ."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:5 of
msgid ""
"The parameter of RegNets. - w0 (int): initial width - wa (float): slope of width - wm (float): quantization "
"parameter to quantize the width - depth (int): depth of the backbone - group_w (int): width of group - "
"bot_mul (float): bottleneck ratio, i.e. expansion of bottleneck."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:13 of
msgid "Strides of the first block of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:15 of
msgid "Base channels after stem layer."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:19 mmcls.models.backbones.vgg.VGG:11 of
msgid "Dilation of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:21 of
msgid "Output from which stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:23 of
msgid ""
"`pytorch` or `caffe`. If set to \"pytorch\", the stride-two layer is the 3x3 conv layer, otherwise the "
"stride-two layer is the first 1x1 conv layer. Default: \"pytorch\"."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:27 of
msgid "Stages to be frozen (all param fixed). -1 means not freezing any parameters. Default: -1."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:30 of
msgid "dictionary to construct and config norm layer. Default: dict(type='BN', requires_grad=True)."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:40 of
msgid ""
"whether to use zero init for last norm layer in resblocks to let them behave as identity. Default: True."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:1 of
msgid "Adjusts the compatibility of widths and groups."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:3 of
msgid "Width of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:5 of
msgid "Bottleneck ratio."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:7 of
msgid "number of groups in each stage"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:10 of
msgid "The adjusted widths and groups of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:1 of
msgid "Generates per block width from RegNet parameters."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:3 of
msgid "Initial width of the backbone"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:5 of
msgid "Slope of the quantized linear function"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:7 of
msgid "Parameter used to quantize the width."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:9 of
msgid "Depth of the backbone."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:11 of
msgid "The divisor of channels. Defaults to 8."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:14 of
msgid "tuple containing:     - list: Widths of each stage.     - int: The number of stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:17 of
msgid "tuple containing:"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:17 of
msgid "list: Widths of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:18 of
msgid "int: The number of stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:1 of
msgid "Gets widths/stage_blocks of network at each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:3 of
msgid "Width in each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:6 of
msgid "width and depth of each stage"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:1 of
msgid "Converts a float to closest non-zero int divisible by divior."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:3 of
msgid "Original number to be quantized."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:5 of
msgid "Divisor used to quantize the number."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:8 of
msgid "quantized number that is divisible by devisor."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.RepLKNet.rst:7
msgid "RepLKNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.replknet.RepLKNet:1 of
msgid "RepLKNet backbone."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:3 of
msgid ""
"A PyTorch impl of : `Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs <https://"
"arxiv.org/abs/2203.06717>`_"
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:7 of
msgid ""
"The parameter of RepLKNet. If it's a dict, it should contain the following keys:  - large_kernel_sizes "
"(Sequence[int]):     Large kernel size in each stage. - layers (Sequence[int]): Number of blocks in each "
"stage. - channels (Sequence[int]): Number of channels in each stage. - small_kernel (int): size of the "
"parallel small kernel. - dw_ratio (float): The intermediate channels     expansion ratio of the block."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:7 of
msgid "The parameter of RepLKNet. If it's a dict, it should contain the following keys:"
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:10 of
msgid "large_kernel_sizes (Sequence[int]):"
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:11 of
msgid "Large kernel size in each stage."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:12 of
msgid "layers (Sequence[int]): Number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:13 of
msgid "channels (Sequence[int]): Number of channels in each stage."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:14 of
msgid "small_kernel (int): size of the parallel small kernel."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:15 of
msgid "dw_ratio (float): The intermediate channels"
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:16 of
msgid "expansion ratio of the block."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:18 of
msgid "Number of input image channels. Default to  3."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:20 of
msgid "Mlp expansion ratio. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:22 of
msgid "Output from which stages. Default to  (3, )."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:25 of
msgid "Strides of the first block of each stage. Default to  (2, 2, 2, 2)."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:28 of
msgid "Dilation of each stage. Default to  (1, 1, 1, 1)."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:31 of
msgid "Stages to be frozen (all param fixed). -1 means not freezing any parameters. Default to  -1."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:35 of
msgid "The config dict for conv layers. Default to None."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:38 of
msgid "The config dict for norm layers. Default to  ``dict(type='BN')``."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:41 of
msgid "Config dict for activation layer. Default to  ``dict(type='ReLU')``."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:44 of
msgid ""
"Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. "
"Default to False."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:47 of
msgid "Whether to switch the model structure to deployment mode. Default to False."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:50 of
msgid ""
"Construct and config norm layer or not. Using True will normalize the intermediate features for downstream "
"dense prediction tasks."
msgstr ""

#: mmcls.models.backbones.replknet.RepLKNet:55 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch "
"Norm and its variants only. Default to False."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.RepMLPNet.rst:7
msgid "RepMLPNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.repmlp.RepMLPNet:1 of
msgid "RepMLPNet backbone."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:3 of
msgid ""
"A PyTorch impl of : `RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image "
"Recognition <https://arxiv.org/abs/2105.01883>`_"
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:7 of
msgid ""
"RepMLP architecture. If use string, choose from 'base' and 'b'. If use dict, it should have below keys:  - "
"channels (List[int]): Number of blocks in each stage. - depths (List[int]): The number of blocks in each "
"branch. - sharesets_nums (List[int]): RepVGG Block that declares   the need to apply group convolution."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:7 of
msgid ""
"RepMLP architecture. If use string, choose from 'base' and 'b'. If use dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:10 of
msgid "channels (List[int]): Number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:11 of
msgid "depths (List[int]): The number of blocks in each branch."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:12 of
msgid "sharesets_nums (List[int]): RepVGG Block that declares the need to apply group convolution."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:15 of
msgid "The size of input image. Defaults: 224."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:25 of
msgid "The conv kernels in the GlobalPerceptron. Default: None."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:28 of
msgid "The reducation ratio in the GlobalPerceptron. Default: 4."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:31 of
msgid "The number of sharesets in the PartitionPerceptron. Default 1."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:34 mmcls.models.backbones.resnest.ResNeSt:51
#: mmcls.models.backbones.resnet.ResNet:38 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:42
#: mmcls.models.backbones.resnext.ResNeXt:43 mmcls.models.backbones.seresnet.SEResNet:40
#: mmcls.models.backbones.seresnext.SEResNeXt:45 mmcls.models.utils.embed.HybridEmbed:17 of
msgid "The config dict for conv layers. Default: None."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:36 of
msgid "The config dict for norm layers. Default: dict(type='BN', requires_grad=True)."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:39 mmcls.models.backbones.swin_transformer.SwinTransformer:67
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:67 of
msgid "Extra config dict for patch embedding. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:45 mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:21
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:18 of
msgid "Config dict for activation layer. Default: dict(type='ReLU')."
msgstr ""

#: mmcls.models.backbones.repmlp.RepMLPNet:48 of
msgid "Whether to switch the model structure to deployment mode. Default: False."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.RepVGG.rst:7
msgid "RepVGG"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.repvgg.RepVGG:1 of
msgid "RepVGG backbone."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:3 of
msgid ""
"A PyTorch impl of : `RepVGG: Making VGG-style ConvNets Great Again <https://arxiv.org/abs/2101.03697>`_"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:6 of
msgid ""
"RepVGG architecture. If use string, choose from 'A0', 'A1`', 'A2', 'B0', 'B1', 'B1g2', 'B1g4', 'B2', "
"'B2g2', 'B2g4', 'B3', 'B3g2', 'B3g4'  or 'D2se'. If use dict, it should have below keys:  - **num_blocks** "
"(Sequence[int]): Number of blocks in each stage. - **width_factor** (Sequence[float]): Width deflator in "
"each stage. - **group_layer_map** (dict | None): RepVGG Block that declares   the need to apply group "
"convolution. - **se_cfg** (dict | None): SE Layer config. - **stem_channels** (int, optional): The stem "
"channels, the final   stem channels will be   ``min(stem_channels, base_channels*width_factor[0])``.   If "
"not set here, 64 is used by default in the code."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:6 of
msgid ""
"RepVGG architecture. If use string, choose from 'A0', 'A1`', 'A2', 'B0', 'B1', 'B1g2', 'B1g4', 'B2', "
"'B2g2', 'B2g4', 'B3', 'B3g2', 'B3g4'  or 'D2se'. If use dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:11 of
msgid "**num_blocks** (Sequence[int]): Number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:12 of
msgid "**width_factor** (Sequence[float]): Width deflator in each stage."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:13 of
msgid "**group_layer_map** (dict | None): RepVGG Block that declares the need to apply group convolution."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:15 of
msgid "**se_cfg** (dict | None): SE Layer config."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:16 of
msgid ""
"**stem_channels** (int, optional): The stem channels, the final stem channels will be ``min(stem_channels, "
"base_channels*width_factor[0])``. If not set here, 64 is used by default in the code."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:23 of
msgid "Base channels of RepVGG backbone, work with width_factor together. Defaults to 64."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:29 of
msgid "Strides of the first block of each stage. Defaults to ``(2, 2, 2, 2)``."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:32 mmcls.models.backbones.res2net.Res2Net:19 of
msgid "Dilation of each stage. Defaults to ``(1, 1, 1, 1)``."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:57 of
msgid "Whether to use the MTSPPF block. Defaults to False."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.Res2Net.rst:7
msgid "Res2Net"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.res2net.Res2Net:1 of
msgid "Res2Net backbone."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:3 of
msgid ""
"A PyTorch implement of : `Res2Net: A New Multi-scale Backbone Architecture <https://arxiv.org/"
"pdf/1904.01169.pdf>`_"
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:6 of
msgid "Depth of Res2Net, choose from {50, 101, 152}."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:8 of
msgid "Scales used in Res2Net. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:10 of
msgid "Basic width of each scale. Defaults to 26."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:14 of
msgid "Number of Res2Net stages. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:16 of
msgid "Strides of the first block of each stage. Defaults to ``(1, 2, 2, 2)``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:25 of
msgid ""
"\"pytorch\" or \"caffe\". If set to \"pytorch\", the stride-two layer is the 3x3 conv layer, otherwise the "
"stride-two layer is the first 1x1 conv layer. Defaults to \"pytorch\"."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:29 of
msgid "Replace 7x7 conv in input stem with 3 3x3 conv. Defaults to True."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:32 of
msgid "Use AvgPool instead of stride conv when downsampling in the bottle2neck. Defaults to True."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:38 of
msgid "Dictionary to construct and config norm layer. Defaults to ``dict(type='BN', requires_grad=True)``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:48 of
msgid ""
"Whether to use zero init for last norm layer in resblocks to let them behave as identity. Defaults to True."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNeSt.rst:7
msgid "ResNeSt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnest.ResNeSt:1 of
msgid "ResNeSt backbone."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:3 of
msgid "Please refer to the `paper <https://arxiv.org/pdf/2004.08955.pdf>`__ for details."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:6 of
msgid "Network depth, from {50, 101, 152, 200}."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:8 mmcls.models.backbones.resnext.ResNeXt:8
#: mmcls.models.backbones.seresnext.SEResNeXt:8 of
msgid "Groups of conv2 in Bottleneck. Default: 32."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:10 mmcls.models.backbones.resnext.ResNeXt:10
#: mmcls.models.backbones.seresnext.SEResNeXt:10 of
msgid "Width per group of conv2 in Bottleneck. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:13 of
msgid "Radix of SpltAtConv2d. Default: 2"
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:15 of
msgid "Reduction factor of SplitAttentionConv2d. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:18 of
msgid "Whether to use average pool for stride in Bottleneck. Default: True."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:23 mmcls.models.backbones.resnet.ResNet:10
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:12 mmcls.models.backbones.resnext.ResNeXt:15
#: mmcls.models.backbones.seresnet.SEResNet:12 mmcls.models.backbones.seresnext.SEResNeXt:17 of
msgid "Output channels of the stem layer. Default: 64."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:25 mmcls.models.backbones.resnet.ResNet:14
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:16 mmcls.models.backbones.resnext.ResNeXt:17
#: mmcls.models.backbones.seresnet.SEResNet:14 mmcls.models.backbones.seresnext.SEResNeXt:19 of
msgid "Stages of the network. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:27 mmcls.models.backbones.resnet.ResNet:16
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:18 mmcls.models.backbones.resnext.ResNeXt:19
#: mmcls.models.backbones.seresnet.SEResNet:16 mmcls.models.backbones.seresnext.SEResNeXt:21 of
msgid "Strides of the first block of each stage. Default: ``(1, 2, 2, 2)``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:30 mmcls.models.backbones.resnet.ResNet:19
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:21 mmcls.models.backbones.resnext.ResNeXt:22
#: mmcls.models.backbones.seresnet.SEResNet:19 mmcls.models.backbones.seresnext.SEResNeXt:24 of
msgid "Dilation of each stage. Default: ``(1, 1, 1, 1)``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:33 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:24
#: mmcls.models.backbones.resnext.ResNeXt:25 mmcls.models.backbones.seresnet.SEResNet:22
#: mmcls.models.backbones.seresnext.SEResNeXt:27 of
msgid ""
"Output from which stages. If only one stage is specified, a single tensor (feature map) is returned, "
"otherwise multiple stages are specified, a tuple of tensors will be returned. Default: ``(3, )``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:38 mmcls.models.backbones.resnet.ResNet:25
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:29 mmcls.models.backbones.resnext.ResNeXt:30
#: mmcls.models.backbones.seresnet.SEResNet:27 mmcls.models.backbones.seresnext.SEResNeXt:32 of
msgid ""
"`pytorch` or `caffe`. If set to \"pytorch\", the stride-two layer is the 3x3 conv layer, otherwise the "
"stride-two layer is the first 1x1 conv layer."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:42 mmcls.models.backbones.resnet.ResNet:29
#: mmcls.models.backbones.resnext.ResNeXt:34 mmcls.models.backbones.seresnet.SEResNet:31
#: mmcls.models.backbones.seresnext.SEResNeXt:36 of
msgid "Replace 7x7 conv in input stem with 3 3x3 conv. Default: False."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:45 mmcls.models.backbones.resnet.ResNet:32
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:36 mmcls.models.backbones.resnext.ResNeXt:37
#: mmcls.models.backbones.seresnet.SEResNet:34 mmcls.models.backbones.seresnext.SEResNeXt:39 of
msgid "Use AvgPool instead of stride conv when downsampling in the bottleneck. Default: False."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:53 mmcls.models.backbones.resnet.ResNet:40
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:44 mmcls.models.backbones.resnext.ResNeXt:45
#: mmcls.models.backbones.seresnet.SEResNet:42 mmcls.models.backbones.seresnext.SEResNeXt:47 of
msgid "The config dict for norm layers."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:62 mmcls.models.backbones.resnet.ResNet:49
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:53 mmcls.models.backbones.resnext.ResNeXt:54
#: mmcls.models.backbones.seresnet.SEResNet:51 mmcls.models.backbones.seresnext.SEResNeXt:56 of
msgid ""
"Whether to use zero init for last norm layer in resblocks to let them behave as identity. Default: True."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNeXt.rst:7
msgid "ResNeXt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnext.ResNeXt:1 of
msgid "ResNeXt backbone."
msgstr ""

#: mmcls.models.backbones.resnext.ResNeXt:3 of
msgid "Please refer to the `paper <https://arxiv.org/abs/1611.05431>`__ for details."
msgstr ""

#: mmcls.models.backbones.resnext.ResNeXt:6 mmcls.models.backbones.seresnet.SEResNet:6
#: mmcls.models.backbones.seresnext.SEResNeXt:6 of
msgid "Network depth, from {50, 101, 152}."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNet.rst:7
msgid "ResNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnet.ResNet:1 of
msgid "ResNet backbone."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:3 of
msgid "Please refer to the `paper <https://arxiv.org/abs/1512.03385>`__ for details."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:6 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:8 of
msgid "Network depth, from {18, 34, 50, 101, 152}."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:12 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:14 of
msgid "Middle channels of the first stage. Default: 64."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNetV1c.rst:7
msgid "ResNetV1c"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnet.ResNetV1c:1 of
msgid "ResNetV1c backbone."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1c:3 mmcls.models.backbones.resnet.ResNetV1d:3 of
msgid "This variant is described in `Bag of Tricks. <https://arxiv.org/pdf/1812.01187.pdf>`_."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1c:6 of
msgid ""
"Compared with default ResNet(ResNetV1b), ResNetV1c replaces the 7x7 conv in the input stem with three 3x3 "
"convs."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNetV1d.rst:7
msgid "ResNetV1d"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnet.ResNetV1d:1 of
msgid "ResNetV1d backbone."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1d:6 of
msgid ""
"Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv in the input stem with three 3x3 "
"convs. And in the downsampling block, a 2x2 avg_pool with stride 2 is added before conv, whose stride is "
"changed to 1."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ResNet_CIFAR.rst:7
msgid "ResNet_CIFAR"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:1 of
msgid "ResNet backbone for CIFAR."
msgstr ""

#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:3 of
msgid ""
"Compared to standard ResNet, it uses `kernel_size=3` and `stride=1` in conv1, and does not apply "
"MaxPoolinng after stem. It has been proven to be more efficient than standard ResNet in other public "
"codebase, e.g., `https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py`."
msgstr ""

#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:33 of
msgid "This network has specific designed stem, thus it is asserted to be False."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.SEResNeXt.rst:7
msgid "SEResNeXt"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.seresnext.SEResNeXt:1 of
msgid "SEResNeXt backbone."
msgstr ""

#: mmcls.models.backbones.seresnet.SEResNet:3 mmcls.models.backbones.seresnext.SEResNeXt:3 of
msgid "Please refer to the `paper <https://arxiv.org/abs/1709.01507>`__ for details."
msgstr ""

#: mmcls.models.backbones.seresnet.SEResNet:8 mmcls.models.backbones.seresnext.SEResNeXt:13 of
msgid "Squeeze ratio in SELayer. Default: 16."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.SEResNet.rst:7
msgid "SEResNet"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.seresnet.SEResNet:1 of
msgid "SEResNet backbone."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.SVT.rst:7
msgid "SVT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.twins.SVT:1 of
msgid "The backbone of Twins-SVT."
msgstr ""

#: mmcls.models.backbones.twins.SVT:7 of
msgid ""
"SVT architecture, a str value in arch zoo or a detailed configuration dict with 8 keys, and the length of "
"all the values in dict should be the same:  - depths (List[int]): The number of encoder layers in each "
"stage. - embed_dims (List[int]): Embedding dimension in each stage. - patch_sizes (List[int]): The patch "
"sizes in each stage. - num_heads (List[int]): Numbers of attention head in each stage. - strides "
"(List[int]): The strides in each stage. - mlp_ratios (List[int]): The ratios of mlp in each stage. - "
"sr_ratios (List[int]): The ratios of GSA-encoder layers in each   stage. - windiow_sizes (List[int]): The "
"window sizes in LSA-encoder layers   in each stage."
msgstr ""

#: mmcls.models.backbones.twins.SVT:7 of
msgid ""
"SVT architecture, a str value in arch zoo or a detailed configuration dict with 8 keys, and the length of "
"all the values in dict should be the same:"
msgstr ""

#: mmcls.models.backbones.twins.SVT:19 of
msgid "windiow_sizes (List[int]): The window sizes in LSA-encoder layers in each stage."
msgstr ""

#: mmcls.models.backbones.twins.SVT:24 of
msgid "Output from which stages. Defaults to (3, )."
msgstr ""

#: mmcls.models.backbones.twins.SVT:31 of
msgid "Dropout ratio of attention weight. Defaults to 0.0"
msgstr ""

#: mmcls.models.backbones.twins.SVT:34 of
msgid "Stochastic depth rate. Defaults to 0.2."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ShuffleNetV1.rst:7
msgid "ShuffleNetV1"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:1 of
msgid "ShuffleNetV1 backbone."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:3 of
msgid "The number of groups to be used in grouped 1x1 convolutions in each ShuffleUnit. Default: 3."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:6 mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:3 of
msgid "Width multiplier - adjusts the number of channels in each layer by this amount. Default: 1.0."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:9 of
msgid "Output from which stages. Default: (2, )"
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:1 of
msgid "Stack ShuffleUnit blocks to make a layer."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:3 of
msgid "out_channels of the block."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:5 of
msgid "Number of blocks."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:7 of
msgid ""
"Whether is the first ShuffleUnit of a sequential ShuffleUnits. Default: False, which means using the "
"grouped 1x1 convolution."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.ShuffleNetV2.rst:7
msgid "ShuffleNetV2"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:1 of
msgid "ShuffleNetV2 backbone."
msgstr ""

#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:6 of
msgid "Output from which stages. Default: (0, 1, 2, 3)."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.SwinTransformer.rst:7
msgid "SwinTransformer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.swin_transformer.SwinTransformer:1 of
msgid "Swin Transformer."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:3 of
msgid ""
"A PyTorch implement of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://"
"arxiv.org/abs/2103.14030>`_"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:7
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:7 of
msgid "Inspiration from https://github.com/microsoft/Swin-Transformer"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:10 of
msgid ""
"Swin Transformer architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, "
"it should have below keys:  - **embed_dims** (int): The dimensions of embedding. - **depths** (List[int]): "
"The number of blocks in each stage. - **num_heads** (List[int]): The number of heads in attention   modules "
"of each stage.  Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:10
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:10 of
msgid ""
"Swin Transformer architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, "
"it should have below keys:"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:32
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:35 mmcls.models.backbones.van.VAN:25 of
msgid "Dropout rate after embedding. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:42
#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:42 of
msgid "Select the interpolate mode for absolute position embeding vector resize. Defaults to \"bicubic\"."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.SwinTransformerV2.rst:7
msgid "SwinTransformerV2"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:1 of
msgid "Swin Transformer V2."
msgstr ""

#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:3 of
msgid ""
"A PyTorch implement of : `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/"
"abs/2111.09883>`_"
msgstr ""

#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:10 of
msgid ""
"Swin Transformer architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If use dict, "
"it should have below keys:  - **embed_dims** (int): The dimensions of embedding. - **depths** (List[int]): "
"The number of blocks in each stage. - **num_heads** (List[int]): The number of heads in attention   modules "
"of each stage. - **extra_norm_every_n_blocks** (int): Add extra norm at the end   of main branch every n "
"blocks.  Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:18 of
msgid "**extra_norm_every_n_blocks** (int): Add extra norm at the end of main branch every n blocks."
msgstr ""

#: mmcls.models.backbones.swin_transformer_v2.SwinTransformerV2:70 of
msgid "Pretrained window sizes of each layer."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.T2T_ViT.rst:7
msgid "T2T_ViT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.t2t_vit.T2T_ViT:1 of
msgid "Tokens-to-Token Vision Transformer (T2T-ViT)"
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:3 of
msgid ""
"A PyTorch implementation of `Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet "
"<https://arxiv.org/abs/2101.11986>`_"
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:10 mmcls.models.utils.attention.ShiftWindowMSA:3
#: mmcls.models.utils.attention.WindowMSA:4 mmcls.models.utils.attention.WindowMSAV2:8
#: mmcls.models.utils.position_encoding.ConditionalPositionEncoding:6 of
msgid "Number of input channels."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:12 of
msgid "Embedding dimension."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:14 of
msgid "Num of transformer layers in encoder. Defaults to 14."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:20 of
msgid "Dropout rate after position embedding. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:40 of
msgid "Extra config of Tokens-to-Token module. Defaults to an empty dict."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.TIMMBackbone.rst:7
msgid "TIMMBackbone"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.timm_backbone.TIMMBackbone:1 of
msgid "Wrapper to use backbones from timm library."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:3 of
msgid ""
"More details can be found in `timm <https://github.com/rwightman/pytorch-image-models>`_. See especially "
"the document for `feature extraction <https://rwightman.github.io/pytorch-image-models/feature_extraction/"
">`_."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:8 of
msgid "Name of timm model to instantiate."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:10 of
msgid ""
"Whether to extract feature pyramid (multi-scale feature maps from the deepest layer at each stride). For "
"Vision Transformer models that do not support this argument, set this False. Defaults to False."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:15 of
msgid "Whether to load pretrained weights. Defaults to False."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:18 of
msgid ""
"Path of checkpoint to load at the last of ``timm.create_model``. Defaults to empty string, which means not "
"loading."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:24 of
msgid "Initialization config dict of OpenMMLab projects. Defaults to None."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:27 of
msgid "Other timm & model specific arguments."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.TNT.rst:7
msgid "TNT"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.tnt.TNT:1 of
msgid "Transformer in Transformer."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:3 of
msgid "A PyTorch implement of: `Transformer in Transformer <https://arxiv.org/abs/2103.00112>`_"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:6 of
msgid "Inspiration from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tnt.py"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:9 of
msgid "Vision Transformer architecture Default: 'b'"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:12 of
msgid "Input image size. Defaults to 224"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:14 of
msgid "The patch size. Deault to 16"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:16 of
msgid "Number of input channels. Defaults to 3"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:18 of
msgid "A ratio to calculate the hidden_dims in ffn layer. Default: 4"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:21 of
msgid "Enable bias for qkv if True. Default False"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:23 of
msgid "Probability of an element to be zeroed after the feed forward layer. Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:26 of
msgid "The drop out rate for attention layer. Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:29 of
msgid "stochastic depth rate. Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:31 of
msgid "The activation config for FFNs. Defaults to GELU."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:33 of
msgid "Config dict for normalization layer. Default layer normalization"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:36 of
msgid ""
"The stride of the conv2d layer. We use a conv2d layer and a unfold layer to implement image to pixel "
"embedding."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:39 of
msgid "The number of fully-connected layers for FFNs. Default 2"
msgstr ""

#: ../../api/generated/mmcls.models.backbones.VAN.rst:7
msgid "VAN"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.van.VAN:1 of
msgid "Visual Attention Network."
msgstr ""

#: mmcls.models.backbones.van.VAN:3 of
msgid "A PyTorch implement of : `Visual Attention Network <https://arxiv.org/pdf/2202.09741v2.pdf>`_"
msgstr ""

#: mmcls.models.backbones.van.VAN:6 of
msgid "Inspiration from https://github.com/Visual-Attention-Network/VAN-Classification"
msgstr ""

#: mmcls.models.backbones.van.VAN:9 of
msgid ""
"Visual Attention Network architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If "
"use dict, it should have below keys:  - **embed_dims** (List[int]): The dimensions of embedding. - "
"**depths** (List[int]): The number of blocks in each stage. - **ffn_ratios** (List[int]): The number of "
"expansion ratio of   feedforward network hidden layer channels.  Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.van.VAN:9 of
msgid ""
"Visual Attention Network architecture. If use string, choose from 'tiny', 'small', 'base' and 'large'. If "
"use dict, it should have below keys:"
msgstr ""

#: mmcls.models.backbones.van.VAN:13 of
msgid "**embed_dims** (List[int]): The dimensions of embedding."
msgstr ""

#: mmcls.models.backbones.van.VAN:15 of
msgid ""
"**ffn_ratios** (List[int]): The number of expansion ratio of feedforward network hidden layer channels."
msgstr ""

#: mmcls.models.backbones.van.VAN:20 of
msgid "The patch size in patch embeddings. Defaults to [7, 3, 3, 3]."
msgstr ""

#: mmcls.models.backbones.van.VAN:42 of
msgid "The extra config of each block. Defaults to empty dicts."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.VGG.rst:7
msgid "VGG"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.vgg.VGG:1 of
msgid "VGG backbone."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:3 of
msgid "Depth of vgg, from {11, 13, 16, 19}."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:5 of
msgid "Use BatchNorm or not."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:7 of
msgid "number of classes for classification."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:9 of
msgid "VGG stages, normally 5."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:13 of
msgid ""
"Output from which stages. When it is None, the default behavior depends on whether num_classes is "
"specified. If num_classes <= 0, the default value is (4, ), output the last feature map before classifier. "
"If num_classes > 0, the default value is (5, ), output the classification score. Default: None."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:20 of
msgid "Stages to be frozen (all param fixed). -1 means not freezing any parameters."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:27 of
msgid "Whether to use ceil_mode of MaxPool. Default: False."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:29 of
msgid "Whether to keep the last pooling before classifier. Default: True."
msgstr ""

#: ../../api/generated/mmcls.models.backbones.VisionTransformer.rst:7
msgid "VisionTransformer"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1 mmcls.models.backbones.vision_transformer.VisionTransformer:1 of
msgid "Vision Transformer."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:3 of
msgid ""
"A PyTorch implement of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale "
"<https://arxiv.org/abs/2010.11929>`_"
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:6 of
msgid ""
"Vision Transformer architecture. If use string, choose from 'small', 'base', 'large', 'deit-tiny', 'deit-"
"small' and 'deit-base'. If use dict, it should have below keys:  - **embed_dims** (int): The dimensions of "
"embedding. - **num_layers** (int): The number of transformer encoder layers. - **num_heads** (int): The "
"number of heads in attention modules. - **feedforward_channels** (int): The hidden dimensions in   "
"feedforward modules.  Defaults to 'base'."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:47 of
msgid ""
"Whether or not to use the mean patch token for classification. If True, the model will only take the "
"average of all patch tokens. Defaults to False."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:57 of
msgid "Whether or not use BEiT-style. Defaults to False."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:59 of
msgid "The initialization value for the learnable scaling of attention and FFN. Defaults to 0.1."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:1 of
msgid "Interface for backward-compatibility."
msgstr ""

#: ../../api/generated/mmcls.models.build_backbone.rst:2
msgid "mmcls.models.build\\_backbone"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1 mmcls.models.builder.build_backbone:1 of
msgid "Build backbone."
msgstr ""

#: ../../api/generated/mmcls.models.build_classifier.rst:2
msgid "mmcls.models.build\\_classifier"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1 mmcls.models.builder.build_classifier:1 of
msgid "Build classifier."
msgstr ""

#: ../../api/generated/mmcls.models.build_head.rst:2
msgid "mmcls.models.build\\_head"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1 mmcls.models.builder.build_head:1 of
msgid "Build head."
msgstr ""

#: ../../api/generated/mmcls.models.build_loss.rst:2
msgid "mmcls.models.build\\_loss"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1 mmcls.models.builder.build_loss:1 of
msgid "Build loss."
msgstr ""

#: ../../api/generated/mmcls.models.build_neck.rst:2
msgid "mmcls.models.build\\_neck"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1 mmcls.models.builder.build_neck:1 of
msgid "Build neck."
msgstr ""

#: ../../api/generated/mmcls.models.classifiers.BaseClassifier.rst:7
msgid "BaseClassifier"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1 mmcls.models.classifiers.base.BaseClassifier:1 of
msgid "Base class for classifiers."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier:6 of
msgid ""
"The config for preprocessing input data. If None, it will use \"BaseDataPreprocessor\" as type, see :class:"
"`mmengine.model.BaseDataPreprocessor` for more details. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier:16 of
msgid "dict"
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier:20 of
msgid ""
"An extra data pre-processing module, which processes data from dataloader to the format accepted by :meth:"
"`forward`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier:24 of
msgid ":obj:`mmengine.model.BaseDataPreprocessor`"
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feat:1
#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:1 of
msgid "Extract features from the input tensor with shape (N, C, ...)."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feat:3 of
msgid "The sub-classes are recommended to implement this method to extract features from backbone and neck."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feat:6
#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:3 of
msgid "A batch of inputs. The shape of it should be ``(num_samples, num_channels, *img_shape)``."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feats:1 of
msgid "Extract features from a sequence of input tensor."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feats:3 of
msgid "A sequence of input tensor. It can be used in augmented inference."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feats:6 of
msgid "Other keyword arguments accepted by :meth:`extract_feat`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.extract_feats:8 of
msgid "Features of every input tensor."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:1
#: mmcls.models.classifiers.image.ImageClassifier.forward:1 of
msgid "The unified entry for a forward process in both training and test."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:3
#: mmcls.models.classifiers.image.ImageClassifier.forward:3 of
msgid "The method should accept three modes: \"tensor\", \"predict\" and \"loss\":"
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:5
#: mmcls.models.classifiers.image.ImageClassifier.forward:5 of
msgid ""
"\"tensor\": Forward the whole network and return tensor or tuple of tensor without any post-processing, "
"same as a common nn.Module."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:7 of
msgid ""
"\"predict\": Forward and return the predictions, which are fully processed to a list of :obj:"
"`BaseDataElement`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:9
#: mmcls.models.classifiers.image.ImageClassifier.forward:9 of
msgid "\"loss\": Forward and return a dict of losses according to the given inputs and data samples."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:12
#: mmcls.models.classifiers.image.ImageClassifier.forward:12 of
msgid ""
"Note that this method doesn't handle neither back propagation nor optimizer updating, which are done in "
"the :meth:`train_step`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:15
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss:3
#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict:3
#: mmcls.models.classifiers.image.ImageClassifier.forward:15
#: mmcls.models.classifiers.image.ImageClassifier.loss:3
#: mmcls.models.classifiers.image.ImageClassifier.predict:3
#: mmcls.models.classifiers.timm.TimmClassifier.loss:3 mmcls.models.classifiers.timm.TimmClassifier.predict:3
#: of
msgid "The input tensor with shape (N, C, ...) in general."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:18
#: mmcls.models.classifiers.image.ImageClassifier.forward:18 of
msgid "The annotation data of every samples. It's required if ``mode=\"loss\"``. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:22
#: mmcls.models.classifiers.image.ImageClassifier.forward:22 of
msgid "Return what kind of value. Defaults to 'tensor'."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:25 of
msgid ""
"The return type depends on ``mode``.  - If ``mode=\"tensor\"``, return a tensor or a tuple of tensor. - If "
"``mode=\"predict\"``, return a list of   :obj:`mmengine.BaseDataElement`. - If ``mode=\"loss\"``, return a "
"dict of tensor."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:25
#: mmcls.models.classifiers.image.ImageClassifier.forward:25 of
msgid "The return type depends on ``mode``."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:27
#: mmcls.models.classifiers.image.ImageClassifier.forward:27 of
msgid "If ``mode=\"tensor\"``, return a tensor or a tuple of tensor."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:28 of
msgid "If ``mode=\"predict\"``, return a list of :obj:`mmengine.BaseDataElement`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:30
#: mmcls.models.classifiers.image.ImageClassifier.forward:30 of
msgid "If ``mode=\"loss\"``, return a dict of tensor."
msgstr ""

#: mmcls.models.classifiers.BaseClassifier.with_head:1 of
msgid "Whether the classifier has a head."
msgstr ""

#: mmcls.models.classifiers.BaseClassifier.with_neck:1 of
msgid "Whether the classifier has a neck."
msgstr ""

#: ../../api/generated/mmcls.models.classifiers.HuggingFaceClassifier.rst:7
msgid "HuggingFaceClassifier"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1 mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:1 of
msgid "Image classifiers for HuggingFace model."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:3 of
msgid ""
"This class accepts all positional and keyword arguments of the API ``from_pretrained`` (when "
"``pretrained=True``) and ``from_config`` (when ``pretrained=False``) of `transformers."
"AutoModelForImageClassification`_ and use it to create a model from hugging-face."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:8 of
msgid ""
"It can load checkpoints of hugging-face directly, and the saved checkpoints also can be directly load by "
"hugging-face."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:11 of
msgid "Please confirm that you have installed ``transfromers`` if you want to use it."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:16 of
msgid "The name of the model to use in hugging-face."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:18 of
msgid "Whether to load pretrained checkpoint from hugging-face. Defaults to False."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:21 of
msgid "Other positional arguments of the method `from_pretrained` or `from_config`."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:23
#: mmcls.models.classifiers.timm.TimmClassifier:14 mmcls.models.heads.cls_head.ClsHead:3
#: mmcls.models.heads.linear_head.LinearClsHead:8 mmcls.models.heads.margin_head.ArcFaceClsHead:82 of
msgid "Config of classification loss. Defaults to ``dict(type='CrossEntropyLoss', loss_weight=1.0)``."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:26
#: mmcls.models.classifiers.image.ImageClassifier:17 mmcls.models.classifiers.timm.TimmClassifier:17 of
msgid ""
"The training setting. The acceptable fields are:  - augments (List[dict]): The batch augmentation methods "
"to use.   More details can be found in :mod:`mmcls.model.utils.augment`.  Defaults to None."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:26
#: mmcls.models.classifiers.image.ImageClassifier:17 mmcls.models.classifiers.timm.TimmClassifier:17 of
msgid "The training setting. The acceptable fields are:"
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:29
#: mmcls.models.classifiers.image.ImageClassifier:20 mmcls.models.classifiers.timm.TimmClassifier:20 of
msgid ""
"augments (List[dict]): The batch augmentation methods to use. More details can be found in :mod:`mmcls."
"model.utils.augment`."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:37
#: mmcls.models.classifiers.image.ImageClassifier:25 mmcls.models.classifiers.timm.TimmClassifier:28 of
msgid ""
"The config for preprocessing input data. If None or no specified type, it will use \"ClsDataPreprocessor\" "
"as type. See :class:`ClsDataPreprocessor` for more details. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:42
#: mmcls.models.classifiers.image.ImageClassifier:30 mmcls.models.classifiers.timm.TimmClassifier:33
#: mmcls.models.heads.cls_head.ClsHead:13 mmcls.models.heads.margin_head.ArcFaceClsHead:85 of
msgid "the config to control the initialization. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier:45 of
msgid "Other keyword arguments of the method `from_pretrained` or `from_config`."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss:1
#: mmcls.models.classifiers.image.ImageClassifier.loss:1 mmcls.models.classifiers.timm.TimmClassifier.loss:1
#: of
msgid "Calculate losses from a batch of inputs and data samples."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss:6
#: mmcls.models.classifiers.image.ImageClassifier.loss:6 mmcls.models.classifiers.timm.TimmClassifier.loss:6
#: mmcls.models.heads.cls_head.ClsHead.loss:8
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss:8
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss:8
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss:8 of
msgid "The annotation data of every samples."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss:9
#: mmcls.models.classifiers.timm.TimmClassifier.loss:9 of
msgid "Other keyword arguments of the loss module."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.loss:11
#: mmcls.models.classifiers.image.ImageClassifier.loss:10 mmcls.models.classifiers.timm.TimmClassifier.loss:11
#: mmcls.models.heads.cls_head.ClsHead.loss:13
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss:13
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss:13
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss:13 of
msgid "a dictionary of loss components"
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict:1
#: mmcls.models.classifiers.image.ImageClassifier.predict:1
#: mmcls.models.classifiers.timm.TimmClassifier.predict:1 of
msgid "Predict results from a batch of inputs."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict:6
#: mmcls.models.classifiers.image.ImageClassifier.predict:6
#: mmcls.models.classifiers.timm.TimmClassifier.predict:6 of
msgid "The annotation data of every samples. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.hugging_face.HuggingFaceClassifier.predict:10
#: mmcls.models.classifiers.timm.TimmClassifier.predict:10 of
msgid "The prediction results."
msgstr ""

#: ../../api/generated/mmcls.models.classifiers.ImageClassifier.rst:7
msgid "ImageClassifier"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1 mmcls.models.classifiers.image.ImageClassifier:1 of
msgid "Image classifiers for supervised classification task."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier:3 of
msgid "The backbone module. See :mod:`mmcls.models.backbones`."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier:6 of
msgid "The neck module to process features from backbone. See :mod:`mmcls.models.necks`. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier:9 of
msgid ""
"The head module to do prediction and calculate loss from processed features. See :mod:`mmcls.models.heads`. "
"Notice that if the head is not set, almost all methods cannot be used except :meth:`extract_feat`. Defaults "
"to None."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier:14 of
msgid "The pretrained checkpoint path, support local path and remote path. Defaults to None."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:6 of
msgid ""
"Which stage to output the feature. Choose from:  - \"backbone\": The output of backbone network. Returns a "
"tuple   including multiple stages features. - \"neck\": The output of neck module. Returns a tuple "
"including   multiple stages features. - \"pre_logits\": The feature before the final classification   "
"linear layer. Usually returns a tensor.  Defaults to \"neck\"."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:6 of
msgid "Which stage to output the feature. Choose from:"
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:8 of
msgid "\"backbone\": The output of backbone network. Returns a tuple including multiple stages features."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:10 of
msgid "\"neck\": The output of neck module. Returns a tuple including multiple stages features."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:12 of
msgid "\"pre_logits\": The feature before the final classification linear layer. Usually returns a tensor."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:15 of
msgid "Defaults to \"neck\"."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:18 of
msgid ""
"The output of specified stage. The output depends on detailed implementation. In general, the output of "
"backbone and neck is a tuple and the output of pre_logits is a tensor."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:26 of
msgid "Backbone output"
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:43 of
msgid "Neck output"
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:61 of
msgid "Pre-logits output (without the final linear classifier head)"
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward:7 of
msgid ""
"\"predict\": Forward and return the predictions, which are fully processed to a list of :obj:"
"`ClsDataSample`."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward:25 of
msgid ""
"The return type depends on ``mode``.  - If ``mode=\"tensor\"``, return a tensor or a tuple of tensor. - If "
"``mode=\"predict\"``, return a list of   :obj:`mmcls.structures.ClsDataSample`. - If ``mode=\"loss\"``, "
"return a dict of tensor."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward:28 of
msgid "If ``mode=\"predict\"``, return a list of :obj:`mmcls.structures.ClsDataSample`."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.predict:9 of
msgid "Other keyword arguments accepted by the ``predict`` method of :attr:`head`."
msgstr ""

#: ../../api/generated/mmcls.models.classifiers.TimmClassifier.rst:7
msgid "TimmClassifier"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1 mmcls.models.classifiers.timm.TimmClassifier:1 of
msgid "Image classifiers for pytorch-image-models (timm) model."
msgstr ""

#: mmcls.models.classifiers.timm.TimmClassifier:3 of
msgid ""
"This class accepts all positional and keyword arguments of the function `timm.models.create_model <https://"
"timm.fast.ai/create_model>`_ and use it to create a model from pytorch-image-models."
msgstr ""

#: mmcls.models.classifiers.timm.TimmClassifier:7 of
msgid "It can load checkpoints of timm directly, and the saved checkpoints also can be directly load by timm."
msgstr ""

#: mmcls.models.classifiers.timm.TimmClassifier:10 of
msgid "Please confirm that you have installed ``timm`` if you want to use it."
msgstr ""

#: mmcls.models.classifiers.timm.TimmClassifier:12 of
msgid "All positional arguments of the function `timm.models.create_model`."
msgstr ""

#: mmcls.models.classifiers.timm.TimmClassifier:36 of
msgid "Other keyword arguments of the function `timm.models.create_model`."
msgstr ""

#: ../../api/generated/mmcls.models.heads.ArcFaceClsHead.rst:7
msgid "ArcFaceClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.margin_head.ArcFaceClsHead:1 of
msgid "ArcFace classifier head."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:3 of
msgid ""
"A PyTorch implementation of paper `ArcFace: Additive Angular Margin Loss for Deep Face Recognition <https://"
"arxiv.org/abs/1801.07698>`_ and `Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web "
"Faces <https://link.springer.com/chapter/10.1007/978-3-030-58621-8_43>`_"
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:10 of
msgid "To use ArcFace in config files."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:12 of
msgid "use vanilla ArcFace"
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:27 of
msgid "use SubCenterArcFace with 3 sub-centers"
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:43 of
msgid "use SubCenterArcFace With CountPowerAdaptiveMargins"
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:3 mmcls.models.heads.deit_head.DeiTClsHead:8
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead:3
#: mmcls.models.heads.linear_head.LinearClsHead:3 mmcls.models.heads.margin_head.ArcFaceClsHead:61
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:3 of
msgid "Number of categories excluding the background category."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:6 mmcls.models.heads.deit_head.DeiTClsHead:11
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead:6
#: mmcls.models.heads.linear_head.LinearClsHead:6 mmcls.models.heads.margin_head.ArcFaceClsHead:64
#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:9
#: mmcls.models.heads.stacked_head.StackedLinearClsHead:5
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:6 of
msgid "Number of channels in the input feature map."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:66 of
msgid "Number of subcenters. Defaults to 1."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:68 of
msgid "Scale factor of output logit. Defaults to 64.0."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:70 of
msgid ""
"The penalty margin. Could be the fllowing formats:  - float: The margin, would be same for all the "
"categories. - Sequence[float]: The category-based margins list. - str: A '.txt' file path which contains a "
"list. Each line   represents the margin of a category, and the number in the   i-th row indicates the "
"margin of the i-th class.  Defaults to 0.5."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:70 of
msgid "The penalty margin. Could be the fllowing formats:"
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:72 of
msgid "float: The margin, would be same for all the categories."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:73 of
msgid "Sequence[float]: The category-based margins list."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:74 of
msgid ""
"str: A '.txt' file path which contains a list. Each line represents the margin of a category, and the "
"number in the i-th row indicates the margin of the i-th class."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:78 of
msgid "Defaults to 0.5."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead:80 of
msgid "Avoid theta + m >= PI. Defaults to False."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.forward:1 mmcls.models.heads.conformer_head.ConformerHead.forward:1
#: mmcls.models.heads.deit_head.DeiTClsHead.forward:1
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.forward:1
#: mmcls.models.heads.linear_head.LinearClsHead.forward:1
#: mmcls.models.heads.margin_head.ArcFaceClsHead.forward:1
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.forward:1
#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead.forward:1
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead.forward:1
#: mmcls.models.heads.stacked_head.StackedLinearClsHead.forward:1
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.forward:1 of
msgid "The forward process."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.loss:1
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss:1
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss:1
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss:1 of
msgid "Calculate losses from the classification score."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.loss:3 mmcls.models.heads.cls_head.ClsHead.predict:3
#: mmcls.models.heads.conformer_head.ConformerHead.predict:3
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss:3
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss:3
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss:3
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict:3 of
msgid ""
"The features extracted from the backbone. Multiple stage inputs are acceptable but only the last stage will "
"be used to classify. The shape of every item should be ``(num_samples, num_classes)``."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.loss:11
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.loss:11
#: mmcls.models.heads.margin_head.ArcFaceClsHead.loss:11
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.loss:11 of
msgid "Other keyword arguments to forward the loss module."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.pre_logits:1
#: mmcls.models.heads.conformer_head.ConformerHead.pre_logits:1
#: mmcls.models.heads.deit_head.DeiTClsHead.pre_logits:1
#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.pre_logits:1
#: mmcls.models.heads.linear_head.LinearClsHead.pre_logits:1
#: mmcls.models.heads.margin_head.ArcFaceClsHead.pre_logits:1
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.pre_logits:1
#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead.pre_logits:1
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead.pre_logits:1
#: mmcls.models.heads.stacked_head.StackedLinearClsHead.pre_logits:1
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.pre_logits:1 of
msgid "The process before the final classification head."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``ArcFaceHead``, we just obtain the feature of the last stage."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead.set_margins:1 of
msgid "set margins of arcface head."
msgstr ""

#: mmcls.models.heads.margin_head.ArcFaceClsHead.set_margins:3 of
msgid "The marigins."
msgstr ""

#: ../../api/generated/mmcls.models.heads.CSRAClsHead.rst:7
msgid "CSRAClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.multi_label_csra_head.CSRAClsHead:1 of
msgid "Class-specific residual attention classifier head."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:3 of
msgid ""
"Please refer to the `Residual Attention: A Simple but Effective Method for Multi-Label Recognition (ICCV "
"2021) <https://arxiv.org/abs/2108.02456>`_ for details."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:7
#: mmcls.models.heads.stacked_head.StackedLinearClsHead:3 of
msgid "Number of categories."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:11 of
msgid "Number of residual at tensor heads."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:13 of
msgid "Config of classification loss."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead:15 of
msgid "Lambda that combines global average and max pooling scores."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:9 mmcls.models.heads.multi_label_csra_head.CSRAClsHead:18
#: of
msgid "The extra init config of layers. Defaults to use ``dict(type='Normal', layer='Linear', std=0.01)``."
msgstr ""

#: mmcls.models.heads.multi_label_csra_head.CSRAClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``CSRAClsHead``, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.ClsHead.rst:7
msgid "ClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.cls_head.ClsHead:1 of
msgid "Classification head."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:6 mmcls.models.heads.linear_head.LinearClsHead:11 of
msgid "Top-k accuracy. Defaults to ``(1, )``."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:8 mmcls.models.heads.linear_head.LinearClsHead:13 of
msgid ""
"Whether to calculate accuracy during training. If you use batch augmentations like Mixup and CutMix during "
"training, it is pointless to calculate accuracy. Defaults to False."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``ClsHead``, we just obtain the feature of the last stage."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.predict:1 mmcls.models.heads.conformer_head.ConformerHead.predict:1
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict:1 of
msgid "Inference without augmentation."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.predict:8 mmcls.models.heads.conformer_head.ConformerHead.predict:8
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict:8 of
msgid ""
"The annotation data of every samples. If not None, set ``pred_label`` of the input data samples. Defaults "
"to None."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead.predict:13 mmcls.models.heads.conformer_head.ConformerHead.predict:13
#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.predict:13 of
msgid "A list of data samples which contains the predicted results."
msgstr ""

#: ../../api/generated/mmcls.models.heads.ConformerHead.rst:7
msgid "ConformerHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.conformer_head.ConformerHead:1
#: mmcls.models.heads.linear_head.LinearClsHead:1 of
msgid "Linear classifier head."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``ConformerHead``, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.DeiTClsHead.rst:7
msgid "DeiTClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.deit_head.DeiTClsHead:1 of
msgid "Distilled Vision Transformer classifier head."
msgstr ""

#: mmcls.models.heads.deit_head.DeiTClsHead:3 of
msgid ""
"Comparing with the :class:`VisionTransformerClsHead`, this head adds an extra linear layer to handle the "
"dist token. The final classification score is the average of both linear transformation results of "
"``cls_token`` and ``dist_token``."
msgstr ""

#: mmcls.models.heads.deit_head.DeiTClsHead:13
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:8 of
msgid "Number of the dimensions for hidden layer. Defaults to None, which means no extra hidden layer."
msgstr ""

#: mmcls.models.heads.deit_head.DeiTClsHead:16
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:11 of
msgid "The activation config. Only available during pre-training. Defaults to ``dict(type='Tanh')``."
msgstr ""

#: mmcls.models.heads.deit_head.DeiTClsHead:19
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:14 of
msgid "The extra initialization configs. Defaults to ``dict(type='Constant', layer='Linear', val=0)``."
msgstr ""

#: mmcls.models.heads.deit_head.DeiTClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of list of tensor, and each tensor is the feature of a backbone stage. In "
"``DeiTClsHead``, we obtain the feature of the last stage and forward in hidden layer if exists."
msgstr ""

#: ../../api/generated/mmcls.models.heads.EfficientFormerClsHead.rst:7
msgid "EfficientFormerClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.efficientformer_head.EfficientFormerClsHead:1
#: of
msgid "EfficientFormer classifier head."
msgstr ""

#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead:8 of
msgid "Whether use a additional distilled head. Defaults to True."
msgstr ""

#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead:11 of
msgid "The extra initialization configs. Defaults to ``dict(type='Normal', layer='Linear', std=0.01)``."
msgstr ""

#: mmcls.models.heads.efficientformer_head.EfficientFormerClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In :"
"obj`EfficientFormerClsHead`, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.LinearClsHead.rst:7
msgid "LinearClsHead"
msgstr ""

#: mmcls.models.heads.linear_head.LinearClsHead:18 of
msgid ""
"the config to control the initialization. Defaults to ``dict(type='Normal', layer='Linear', std=0.01)``."
msgstr ""

#: mmcls.models.heads.linear_head.LinearClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``LinearClsHead``, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.MultiLabelClsHead.rst:7
msgid "MultiLabelClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:1 of
msgid "Classification head for multilabel task."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:3
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:3 of
msgid "Config of classification loss. Defaults to dict(type='CrossEntropyLoss', use_sigmoid=True)."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:6
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:6 of
msgid "Predictions with scores under the thresholds are considered as negative. Defaults to None."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:9
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:9 of
msgid "Predictions with the k-th highest scores are considered as positive. Defaults to None."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:12 of
msgid "The extra init config of layers. Defaults to None."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead:18
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:18 of
msgid ""
"If both ``thr`` and ``topk`` are set, use ``thr` to determine positive predictions. If neither is set, use "
"``thr=0.5`` as default."
msgstr ""

#: mmcls.models.heads.multi_label_cls_head.MultiLabelClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``MultiLabelClsHead``, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.MultiLabelLinearClsHead.rst:7
msgid "MultiLabelLinearClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:1 of
msgid "Linear classification head for multilabel task."
msgstr ""

#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:12 of
msgid "The extra init config of layers. Defaults to use dict(type='Normal', layer='Linear', std=0.01)."
msgstr ""

#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage. In "
"``MultiLabelLinearClsHead``, we just obtain the feature of the last stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.StackedLinearClsHead.rst:7
msgid "StackedLinearClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1 mmcls.models.heads.stacked_head.StackedLinearClsHead:1 of
msgid "Classifier head with several hidden fc layer and a output fc layer."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:7 of
msgid "Number of channels in the hidden fc layers."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:10 of
msgid "Dropout rate after each hidden fc layer, except the last layer. Defaults to 0."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:13 of
msgid ""
"Config dict of normalization layer after each hidden fc layer, except the last layer. Defaults to None."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:16 of
msgid ""
"Config dict of activation function after each hidden layer, except the last layer. Defaults to use \"ReLU\"."
msgstr ""

#: mmcls.models.heads.StackedLinearClsHead.fc:1 of
msgid "Full connected layer."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead.pre_logits:3 of
msgid "The input ``feats`` is a tuple of tensor, and each tensor is the feature of a backbone stage."
msgstr ""

#: ../../api/generated/mmcls.models.heads.VisionTransformerClsHead.rst:7
msgid "VisionTransformerClsHead"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:1 of
msgid "Vision Transformer classifier head."
msgstr ""

#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.init_weights:1 of
msgid "\"Init weights of hidden layer if exists."
msgstr ""

#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.pre_logits:3 of
msgid ""
"The input ``feats`` is a tuple of list of tensor, and each tensor is the feature of a backbone stage. In "
"``VisionTransformerClsHead``, we obtain the feature of the last stage and forward in hidden layer if exists."
msgstr ""

#: ../../api/generated/mmcls.models.losses.AsymmetricLoss.rst:7
msgid "AsymmetricLoss"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1 mmcls.models.losses.asymmetric_loss.AsymmetricLoss:1
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:1 of
msgid "asymmetric loss."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:3 of
msgid "positive focusing parameter. Defaults to 0.0."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:6 of
msgid "Negative focusing parameter. We usually set gamma_neg > gamma_pos. Defaults to 4.0."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:9 of
msgid "Probability margin. Defaults to 0.05."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:11 of
msgid "The method used to reduce the loss into a scalar."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:14 mmcls.models.losses.focal_loss.FocalLoss:12 of
msgid "Weight of loss. Defaults to 1.0."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:16 of
msgid "Whether the prediction uses sigmoid instead of softmax. Defaults to True."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:19 of
msgid "The minimum value of the argument of logarithm. Defaults to 1e-8."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:3
#: mmcls.models.losses.focal_loss.FocalLoss.forward:3
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:3 of
msgid "The prediction with shape (N, \\*)."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:5
#: mmcls.models.losses.focal_loss.FocalLoss.forward:5 of
msgid "The ground truth label of the prediction with shape (N, \\*), N or (N,1)."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:8
#: mmcls.models.losses.focal_loss.FocalLoss.forward:8
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:8 of
msgid "Sample-wise loss weight with shape (N, \\*). Defaults to None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:11
#: mmcls.models.losses.focal_loss.FocalLoss.forward:11
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:11
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:9 of
msgid "Average factor that is used to average the loss. Defaults to None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:14
#: mmcls.models.losses.focal_loss.FocalLoss.forward:14
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:14 of
msgid ""
"The method used to reduce the loss into a scalar. Options are \"none\", \"mean\" and \"sum\". Defaults to "
"None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:19
#: mmcls.models.losses.focal_loss.FocalLoss.forward:19
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:19 of
msgid "Loss."
msgstr ""

#: ../../api/generated/mmcls.models.losses.CrossEntropyLoss.rst:7
msgid "CrossEntropyLoss"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1 mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:1 of
msgid "Cross entropy loss."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:3 of
msgid "Whether the prediction uses sigmoid of softmax. Defaults to False."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:6 of
msgid "Whether to use the soft version of CrossEntropyLoss. Defaults to False."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:9
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:23 of
msgid "The method used to reduce the loss. Options are \"none\", \"mean\" and \"sum\". Defaults to 'mean'."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:12
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:26 of
msgid "Weight of the loss. Defaults to 1.0."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:14 of
msgid "The weight for each class with shape (C), C is the number of classes. Default None."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:17 of
msgid ""
"The positive weight for each class with shape (C), C is the number of classes. Only enabled in BCE loss "
"when ``use_sigmoid`` is True. Default None."
msgstr ""

#: ../../api/generated/mmcls.models.losses.FocalLoss.rst:7
msgid "FocalLoss"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1 mmcls.models.losses.focal_loss.FocalLoss:1 of
msgid "Focal loss."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:3 of
msgid "Focusing parameter in focal loss. Defaults to 2.0."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:6 of
msgid "The parameter in balanced form of focal loss. Defaults to 0.25."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:9 of
msgid ""
"The method used to reduce the loss into a scalar. Options are \"none\" and \"mean\". Defaults to 'mean'."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss.forward:1 of
msgid "Sigmoid focal loss."
msgstr ""

#: ../../api/generated/mmcls.models.losses.LabelSmoothLoss.rst:7
msgid "LabelSmoothLoss"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1 mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:1 of
msgid "Initializer for the label smoothed cross entropy loss."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:3 of
msgid ""
"Refers to `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`_"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:6 of
msgid ""
"This decreases gap between output scores and encourages generalization. Labels provided to forward can be "
"one-hot like vectors (NxC) or class indices (Nx1). And this accepts linear combination of one-hot like "
"labels from mixup or cutmix except multi-label task."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:12 of
msgid "The degree of label smoothing."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:14 of
msgid "Number of classes. Defaults to None."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:16 of
msgid "Refers to notes, Options are 'original', 'classy_vision', 'multi_label'. Defaults to 'original'."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:19 of
msgid ""
"Whether the prediction uses sigmoid of softmax. Defaults to None, which means to use sigmoid in "
"\"multi_label\" mode and not use in other modes."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:31 of
msgid "if the mode is **\"original\"**, this will use the same label smooth method as the original paper as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:34 of
msgid ""
"(1-\\epsilon)\\delta_{k, y} + \\frac{\\epsilon}{K}\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:37 of
msgid ""
"where :math:`\\epsilon` is the ``label_smooth_val``, :math:`K` is the ``num_classes`` and :math:`"
"\\delta_{k, y}` is Dirac delta, which equals 1 for :math:`k=y` and 0 otherwise."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:41 of
msgid ""
"if the mode is **\"classy_vision\"**, this will use the same label smooth method as the facebookresearch/"
"ClassyVision repo as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:44 of
msgid ""
"\\frac{\\delta_{k, y} + \\epsilon/K}{1+\\epsilon}\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:47 of
msgid ""
"if the mode is **\"multi_label\"**, this will accept labels from multi-label task and smoothing them as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:50 of
msgid ""
"(1-2\\epsilon)\\delta_{k, y} + \\epsilon\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:1 of
msgid "Label smooth loss."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:5 of
msgid "The ground truth label of the prediction with shape (N, \\*)."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.generate_one_hot_like_label:1 of
msgid "This function takes one-hot or index label vectors and computes one- hot like label vectors (float)"
msgstr ""

#: ../../api/generated/mmcls.models.losses.SeesawLoss.rst:7
msgid "SeesawLoss"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1 mmcls.models.losses.seesaw_loss.SeesawLoss:1 of
msgid "Implementation of seesaw loss."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:3 of
msgid ""
"Refers to `Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) <https://arxiv.org/"
"abs/2008.10032>`_"
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:6 of
msgid "Whether the prediction uses sigmoid of softmax. Only False is supported. Defaults to False."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:9 of
msgid "The ``p`` in the mitigation factor. Defaults to 0.8."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:12 of
msgid "The ``q`` in the compenstation factor. Defaults to 2.0."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:15 of
msgid "The number of classes. Defaults to 1000 for the ImageNet dataset."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:18 of
msgid "The minimal value of divisor to smooth the computation of compensation factor, default to 1e-2."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:21 of
msgid ""
"The method that reduces the loss to a scalar. Options are \"none\", \"mean\" and \"sum\". Defaults to \"mean"
"\"."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:24 of
msgid "The weight of the loss. Defaults to 1.0"
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:3 of
msgid "The prediction with shape (N, C)."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:5 of
msgid "The learning label of the prediction."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:7 of
msgid "Sample-wise loss weight."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:12 of
msgid "The method used to reduce the loss. Options are \"none\", \"mean\" and \"sum\"."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:16 of
msgid "The calculated loss"
msgstr ""

#: ../../api/generated/mmcls.models.necks.GeneralizedMeanPooling.rst:7
msgid "GeneralizedMeanPooling"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1 mmcls.models.necks.gem.GeneralizedMeanPooling:1 of
msgid "Generalized Mean Pooling neck."
msgstr ""

#: mmcls.models.necks.gap.GlobalAveragePooling:3 mmcls.models.necks.gem.GeneralizedMeanPooling:3 of
msgid ""
"Note that we use `view` to remove extra channel after pooling. We do not use `squeeze` as it will also "
"remove the batch dimension when the tensor has a batch dimension of size 1, which can lead to unexpected "
"errors."
msgstr ""

#: mmcls.models.necks.gem.GeneralizedMeanPooling:7 of
msgid "Parameter value. Default: 3."
msgstr ""

#: mmcls.models.necks.gem.GeneralizedMeanPooling:10 of
msgid "epsilon. Default: 1e-6"
msgstr ""

#: mmcls.models.necks.gem.GeneralizedMeanPooling:13 of
msgid "Use clamp before pooling. Default: True"
msgstr ""

#: ../../api/generated/mmcls.models.necks.GlobalAveragePooling.rst:7
msgid "GlobalAveragePooling"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1 mmcls.models.necks.gap.GlobalAveragePooling:1 of
msgid "Global Average Pooling neck."
msgstr ""

#: mmcls.models.necks.gap.GlobalAveragePooling:7 of
msgid "Dimensions of each sample channel, can be one of {1, 2, 3}. Default: 2"
msgstr ""

#: ../../api/generated/mmcls.models.necks.HRFuseScales.rst:7
msgid "HRFuseScales"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1 mmcls.models.necks.hr_fuse.HRFuseScales:1 of
msgid "Fuse feature map of multiple scales in HRNet."
msgstr ""

#: mmcls.models.necks.hr_fuse.HRFuseScales:3 of
msgid "The input channels of all scales."
msgstr ""

#: mmcls.models.necks.hr_fuse.HRFuseScales:5 of
msgid "The channels of fused feature map. Defaults to 2048."
msgstr ""

#: mmcls.models.necks.hr_fuse.HRFuseScales:8 of
msgid "dictionary to construct norm layers. Defaults to ``dict(type='BN', momentum=0.1)``."
msgstr ""

#: mmcls.models.necks.hr_fuse.HRFuseScales:11 of
msgid "Initialization config dict. Defaults to ``dict(type='Normal', layer='Linear', std=0.01))``."
msgstr ""

#: ../../api/generated/mmcls.models.utils.ConditionalPositionEncoding.rst:7
msgid "ConditionalPositionEncoding"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.position_encoding.ConditionalPositionEncoding:1
#: of
msgid "The Conditional Position Encoding (CPE) module."
msgstr ""

#: mmcls.models.utils.position_encoding.ConditionalPositionEncoding:3 of
msgid ""
"The CPE is the implementation of 'Conditional Positional Encodings for Vision Transformers <https://arxiv."
"org/abs/2102.10882>'_."
msgstr ""

#: mmcls.models.utils.position_encoding.ConditionalPositionEncoding:8 of
msgid "The feature dimension. Default: 768."
msgstr ""

#: mmcls.models.utils.position_encoding.ConditionalPositionEncoding:10 of
msgid "Stride of conv layer. Default: 1."
msgstr ""

#: ../../api/generated/mmcls.models.utils.HybridEmbed.rst:7
msgid "HybridEmbed"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.embed.HybridEmbed:1 of
msgid "CNN Feature Map Embedding."
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:3 of
msgid "Extract feature map from CNN, flatten, project to embedding dim."
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:6 of
msgid "CNN backbone"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:8 mmcls.models.utils.embed.PatchEmbed:5 of
msgid "The size of input image. Default: 224"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:10 of
msgid "Size of feature map extracted by CNN backbone. Default: None"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:13 mmcls.models.utils.embed.PatchEmbed:7 of
msgid "The num of input channels. Default: 3"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:15 mmcls.models.utils.embed.PatchEmbed:9 of
msgid "The dimensions of embedding. Default: 768"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:20 of
msgid "The Config for initialization. Default: None."
msgstr ""

#: ../../api/generated/mmcls.models.utils.InvertedResidual.rst:7
msgid "InvertedResidual"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.inverted_residual.InvertedResidual:1 of
msgid "Inverted Residual Block."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:3 of
msgid "The input channels of this module."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:5 of
msgid "The output channels of this module."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:7 of
msgid "The input channels of the depthwise convolution."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:9 of
msgid "The kernel size of the depthwise convolution. Defaults to 3."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:12 of
msgid "The stride of the depthwise convolution. Defaults to 1."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:14 of
msgid "Config dict for se layer. Defaults to None, which means no se layer."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:20 of
msgid "Config dict for normalization layer. Defaults to ``dict(type='BN')``."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:6
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:3 of
msgid "The input tensor."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:6 of
msgid "The output tensor."
msgstr ""

#: ../../api/generated/mmcls.models.utils.LayerScale.rst:7
msgid "LayerScale"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.layer_scale.LayerScale:1 of
msgid "LayerScale layer."
msgstr ""

#: mmcls.models.utils.layer_scale.LayerScale:3 of
msgid "Dimension of input features."
msgstr ""

#: mmcls.models.utils.layer_scale.LayerScale:5 of
msgid "inplace: can optionally do the operation in-place. Defaults to False."
msgstr ""

#: mmcls.models.utils.layer_scale.LayerScale:8 of
msgid ""
"The input data format, could be 'channels_last' or 'channels_first', representing (B, C, H, W) and (B, N, "
"C) format data respectively. Defaults to 'channels_last'."
msgstr ""

#: ../../api/generated/mmcls.models.utils.MultiheadAttention.rst:7
msgid "MultiheadAttention"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.attention.MultiheadAttention:1 of
msgid "Multi-head Attention Module."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:3 of
msgid ""
"This module implements multi-head attention that supports different input dims and embed dims. And it also "
"supports a shortcut from ``value``, which is useful if input dims is not the same with embed dims."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:7 of
msgid "The embedding dimension."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:9 of
msgid "Parallel attention heads."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:11 of
msgid "The input dimension, and if None, use ``embed_dims``. Defaults to None."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:14 of
msgid "Dropout rate of the dropout layer after the attention calculation of query and key. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:17 of
msgid "Dropout rate of the dropout layer after the output projection. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:20 of
msgid "The dropout config before adding the shortcut. Defaults to ``dict(type='Dropout', drop_prob=0.)``."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:23 mmcls.models.utils.attention.WindowMSA:10
#: mmcls.models.utils.attention.WindowMSAV2:14 of
msgid "If True, add a learnable bias to q, k, v. Defaults to True."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:26 mmcls.models.utils.attention.WindowMSA:13 of
msgid "Override default qk scale of ``head_dim ** -0.5`` if set. Defaults to None."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:29 of
msgid "Defaults to True."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:31 of
msgid ""
"Add a shortcut from value to output. It's usually used if ``input_dims`` is different from ``embed_dims``. "
"Defaults to False."
msgstr ""

#: ../../api/generated/mmcls.models.utils.PatchEmbed.rst:7
msgid "PatchEmbed"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.embed.PatchEmbed:1 of
msgid "Image to Patch Embedding."
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:3 of
msgid "We use a conv layer to implement PatchEmbed."
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:11 of
msgid "Config dict for normalization layer. Default: None"
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:14 of
msgid "The config dict for conv layers. Default: None"
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:17 of
msgid "The Config for initialization. Default: None"
msgstr ""

#: ../../api/generated/mmcls.models.utils.PatchMerging.rst:7
msgid "PatchMerging"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.embed.PatchMerging:1 of
msgid "Merge patch feature map."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:3 of
msgid "Modified from mmcv, and this module supports specifying whether to use post-norm."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:6 of
#, python-format
msgid ""
"This layer groups feature map by kernel_size, and applies norm and linear layers to the grouped feature map "
"((used in Swin Transformer)). Our implementation uses :class:`torch.nn.Unfold` to merge patches, which is "
"about 25% faster than the original implementation. However, we need to modify pretrained models for "
"compatibility."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:12 of
msgid "The num of input channels. To gets fully covered by filter and stride you specified."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:15 of
msgid "The num of output channels."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:17 of
msgid "the kernel size in the unfold layer. Defaults to 2."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:20 of
msgid ""
"the stride of the sliding blocks in the unfold layer. Defaults to None, which means to be set as "
"``kernel_size``."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:24 of
msgid ""
"The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support "
"\"same\" and \"corner\" now. Defaults to \"corner\"."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:29 of
msgid "dilation parameter in the unfold layer. Defaults to 1."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:32 of
msgid "Whether to add bias in linear layer or not. Defaults to False."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:38 of
msgid "Whether to use post normalization here. Defaults to False."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:24 mmcls.models.utils.attention.WindowMSA:21
#: mmcls.models.utils.attention.WindowMSAV2:29 mmcls.models.utils.embed.PatchMerging:41 of
msgid "The extra config for initialization. Defaults to None."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:1 of
msgid "Has shape (B, H*W, C_in)."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:3 of
msgid "The spatial shape of x, arrange as (H, W). Default: None."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:7 of
msgid ""
"Contains merged results and its spatial shape.  - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out) - "
"out_size (tuple[int]): Spatial shape of x, arrange as   (Merged_H, Merged_W)."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:7 of
msgid "Contains merged results and its spatial shape."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:9 of
msgid "x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)"
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:10 of
msgid "out_size (tuple[int]): Spatial shape of x, arrange as (Merged_H, Merged_W)."
msgstr ""

#: ../../api/generated/mmcls.models.utils.SELayer.rst:7
msgid "SELayer"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.se_layer.SELayer:1 of
msgid "Squeeze-and-Excitation Module."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:3 of
msgid "The input (and output) channels of the SE layer."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:5 of
msgid ""
"The intermediate channel number of SElayer. Default: None, means the value of ``squeeze_channels`` is "
"``make_divisible(channels // ratio, divisor)``."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:9 of
msgid ""
"Squeeze ratio in SELayer, the intermediate channel will be ``make_divisible(channels // ratio, divisor)``. "
"Only used when ``squeeze_channels`` is None. Default: 16."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:13 of
msgid ""
"The divisor to true divide the channel number. Only used when ``squeeze_channels`` is None. Default: 8."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:19 of
msgid "Whether to return the weight. Default: False."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:21 of
msgid ""
"Config dict for activation layer. If act_cfg is a dict, two activation layers will be configurated by this "
"dict. If act_cfg is a sequence of dicts, the first activation layer will be configurated by the first dict "
"and the second activation layer will be configurated by the second dict. Default: (dict(type='ReLU'), "
"dict(type='Sigmoid'))"
msgstr ""

#: ../../api/generated/mmcls.models.utils.ShiftWindowMSA.rst:7
msgid "ShiftWindowMSA"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.attention.ShiftWindowMSA:1 of
msgid "Shift Window Multihead Self-Attention Module."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:5 mmcls.models.utils.attention.WindowMSA:8
#: mmcls.models.utils.attention.WindowMSAV2:12 mmcls.models.utils.embed.resize_relative_position_bias_table:11
#: of
msgid "Number of attention heads."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:7 mmcls.models.utils.attention.WindowMSA:6
#: mmcls.models.utils.attention.WindowMSAV2:10 of
msgid "The height and width of the window."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:9 of
msgid "The shift step of each window towards right-bottom. If zero, act as regular window-msa. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:12 of
msgid "The dropout_layer used before output. Defaults to dict(type='DropPath', drop_prob=0.)."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:21 of
msgid "To build a window multi-head attention module. Defaults to :class:`WindowMSA`."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:27 of
msgid "Other keyword arguments to build the window multi-head attention module."
msgstr ""

#: ../../api/generated/mmcls.models.utils.WindowMSA.rst:7
msgid "WindowMSA"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1 mmcls.models.utils.attention.WindowMSA:1
#: mmcls.models.utils.attention.WindowMSAV2:1 of
msgid "Window based multi-head self-attention (W-MSA) module with relative position bias."
msgstr ""

#: mmcls.models.utils.attention.WindowMSA:16 mmcls.models.utils.attention.WindowMSAV2:17 of
msgid "Dropout ratio of attention weight. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.WindowMSA:19 mmcls.models.utils.attention.WindowMSAV2:20 of
msgid "Dropout ratio of output. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.WindowMSA.forward:1 mmcls.models.utils.attention.WindowMSAV2.forward:1 of
msgid "input features with shape of (num_windows*B, N, C)"
msgstr ""

#: mmcls.models.utils.attention.WindowMSA.forward:3 mmcls.models.utils.attention.WindowMSAV2.forward:3 of
msgid "mask with shape of (num_windows, Wh*Ww, Wh*Ww), value should be between (-inf, 0]."
msgstr ""

#: ../../api/generated/mmcls.models.utils.WindowMSAV2.rst:7
msgid "WindowMSAV2"
msgstr ""

#: mmcls.models.utils.attention.WindowMSAV2:4 of
msgid ""
"Based on implementation on Swin Transformer V2 original repo. Refers to https://github.com/microsoft/Swin-"
"Transformer/blob/main/models/swin_transformer_v2.py for more details."
msgstr ""

#: mmcls.models.utils.attention.WindowMSAV2:22 of
msgid "The hidden dimensions of the continuous relative position bias network. Defaults to 512."
msgstr ""

#: mmcls.models.utils.attention.WindowMSAV2:25 of
msgid ""
"The height and width of the window in pre-training. Defaults to (0, 0), which means not load pretrained "
"model."
msgstr ""

#: ../../api/generated/mmcls.models.utils.batch_augments.CutMix.rst:7
msgid "CutMix"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:3 of
msgid ""
"CutMix is a method to improve the network's generalization capability. It's proposed in `CutMix: "
"Regularization Strategy to Train Strong Classifiers with Localizable Features <https://arxiv.org/"
"abs/1905.04899>`"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:7 of
msgid ""
"With this method, patches are cut and pasted among training images where the ground truth labels are also "
"mixed proportionally to the area of the patches."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:11 mmcls.models.utils.batch_augments.resizemix.ResizeMix:7
#: of
msgid ""
"Parameters for Beta distribution to generate the mixing ratio. It should be a positive number. More details "
"can be found in :class:`Mixup`."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:15 mmcls.models.utils.batch_augments.resizemix.ResizeMix:22
#: of
msgid ""
"The min/max area ratio of the patches. If not None, the bounding-box of patches is uniform sampled within "
"this ratio range, and the ``alpha`` will be ignored. Otherwise, the bounding-box is generated according to "
"the ``alpha``. Defaults to None."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:21 of
msgid "Whether to apply lambda correction when cutmix bbox clipped by image borders. Defaults to True."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:26 of
msgid ""
"If the ``cutmix_minmax`` is None, how to generate the bounding-box of patches according to the ``alpha``?"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:29 of
msgid ""
"First, generate a :math:`\\lambda`, details can be found in :class:`Mixup`. And then, the area ratio of the "
"bounding-box is calculated by:"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix:33 mmcls.models.utils.batch_augments.resizemix.ResizeMix:45
#: of
msgid ""
"\\text{ratio} = \\sqrt{1-\\lambda}\n"
"\n"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.cutmix_bbox_and_lam:1 of
msgid "Generate bbox and apply lambda correction."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.cutmix_bbox_and_lam:3
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox:5
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox_minmax:8 of
msgid "Image shape as tuple"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.cutmix_bbox_and_lam:5
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox:7 of
msgid "Cutmix lambda value"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.cutmix_bbox_and_lam:7
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox:12
#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox_minmax:10 of
msgid "Number of bbox to generate. Defaults to None"
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.mix:1 mmcls.models.utils.batch_augments.mixup.Mixup.mix:1
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix:1 of
msgid "Mix the batch inputs and batch one-hot format ground truth."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.mix:3 mmcls.models.utils.batch_augments.mixup.Mixup.mix:3
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix:3 of
msgid "A batch of images tensor in the shape of ``(N, C, H, W)``."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.mix:6 mmcls.models.utils.batch_augments.mixup.Mixup.mix:6
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix:6 of
msgid "A batch of one-hot format labels in the shape of ``(N, num_classes)``."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.mix:10 mmcls.models.utils.batch_augments.mixup.Mixup.mix:10
#: mmcls.models.utils.batch_augments.resizemix.ResizeMix.mix:10 of
msgid "The mixed inputs and labels."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox:1 of
msgid ""
"Standard CutMix bounding-box that generates a random square bbox based on lambda value. This implementation "
"includes support for enforcing a border margin as percent of bbox dimensions."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox:9 of
msgid "Percentage of bbox dimension to enforce as margin (reduce amount of box outside image). Defaults to 0."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox_minmax:1 of
msgid ""
"Min-Max CutMix bounding-box Inspired by Darknet cutmix implementation. It generates a random rectangular "
"bbox based on min/max percent values applied to each dimension of the input image."
msgstr ""

#: mmcls.models.utils.batch_augments.cutmix.CutMix.rand_bbox_minmax:5 of
msgid "Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max."
msgstr ""

#: ../../api/generated/mmcls.models.utils.batch_augments.Mixup.rst:7
msgid "Mixup"
msgstr ""

#: mmcls.models.utils.batch_augments.mixup.Mixup:3 of
msgid ""
"Mixup is a method to reduces the memorization of corrupt labels and increases the robustness to adversarial "
"examples. It's proposed in `mixup: Beyond Empirical Risk Minimization <https://arxiv.org/abs/1710.09412>`_"
msgstr ""

#: mmcls.models.utils.batch_augments.mixup.Mixup:8 of
msgid ""
"Parameters for Beta distribution to generate the mixing ratio. It should be a positive number. More details "
"are in the note."
msgstr ""

#: mmcls.models.utils.batch_augments.mixup.Mixup:15 of
msgid ""
"The :math:`\\alpha` (``alpha``) determines a random distribution :math:`Beta(\\alpha, \\alpha)`. For each "
"batch of data, we sample a mixing ratio (marked as :math:`\\lambda`, ``lam``) from the random distribution."
msgstr ""

#: ../../api/generated/mmcls.models.utils.batch_augments.ResizeMix.rst:7
msgid "ResizeMix"
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:3 of
msgid ""
"The ResizeMix will resize an image to a small patch and paste it on another image. It's proposed in "
"`ResizeMix: Mixing Data with Preserved Object Information and True Labels <https://arxiv.org/"
"abs/2012.11101>`_"
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:11 of
msgid "The minimum value of lam. Defaults to 0.1."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:13 of
msgid "The maximum value of lam. Defaults to 0.8."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:15 of
msgid ""
"algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. "
"Defaults to 'bilinear'."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:19 of
msgid "The probability to execute resizemix. It should be in range [0, 1]. Defaults to 1.0."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:28 of
msgid "Whether to apply lambda correction when cutmix bbox clipped by image borders. Defaults to True"
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:31 of
msgid "Any other parameters accpeted by :class:`CutMix`."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:35 of
msgid ""
"The :math:`\\lambda` (``lam``) is the mixing ratio. It's a random variable which follows :math:"
"`Beta(\\alpha, \\alpha)` and is mapped to the range [``lam_min``, ``lam_max``]."
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:39 of
msgid ""
"\\lambda = \\frac{Beta(\\alpha, \\alpha)}\n"
"{\\lambda_{max} - \\lambda_{min}} + \\lambda_{min}\n"
"\n"
msgstr ""

#: mmcls.models.utils.batch_augments.resizemix.ResizeMix:43 of
msgid "And the resize ratio of source images is calculated by :math:`\\lambda`:"
msgstr ""

#: ../../api/generated/mmcls.models.utils.channel_shuffle.rst:2
msgid "mmcls.models.utils.channel\\_shuffle"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.channel_shuffle.channel_shuffle:1 of
msgid "Channel Shuffle operation."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:3 of
msgid "This function enables cross-group information flow for multiple groups convolution layers."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:8 of
msgid "The number of groups to divide the input tensor in the channel dimension."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:12 of
msgid "The output tensor after channel shuffle operation."
msgstr ""

#: ../../api/generated/mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.rst:7
msgid "ClsDataPreprocessor"
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:3 of
msgid "Comparing with the :class:`mmengine.model.ImgDataPreprocessor`,"
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:5 of
msgid "It won't do normalization if ``mean`` is not specified."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:6 of
msgid "It does normalization and color space conversion after stacking batch."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:7 of
msgid "It supports batch augmentations like mixup and cutmix."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:9 of
msgid "It provides the data pre-processing as follows"
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:11 of
msgid "Collate and move data to the target device."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:12 of
msgid ""
"Pad inputs to the maximum size of current batch with defined ``pad_value``. The padding size can be "
"divisible by a defined ``pad_size_divisor``"
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:15 of
msgid "Stack inputs to batch_inputs."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:18 of
msgid "Do batch augmentations like Mixup and Cutmix during training."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:20 of
msgid "The pixel mean of R, G, B channels. Defaults to None."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:23 of
msgid "The pixel standard deviation of R, G, B channels. Defaults to None."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:26 of
msgid "The size of padded image should be divisible by ``pad_size_divisor``. Defaults to 1."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:29 of
msgid "The padded pixel value. Defaults to 0."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:31 of
msgid "whether to convert image from BGR to RGB. Defaults to False."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:34 of
msgid "Whether to generate one-hot format gt-labels and set to data samples. Defaults to False."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor:39 of
msgid ""
"The batch augmentations settings, including \"augments\" and \"probs\". For more details, see :class:`mmcls."
"models.RandomBatchAugment`."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward:1 of
msgid ""
"Perform normalization, padding, bgr2rgb conversion and batch augmentation based on ``BaseDataPreprocessor``."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward:4 of
msgid "data sampled from dataloader."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward:6 of
msgid "Whether to enable training time augmentation."
msgstr ""

#: mmcls.models.utils.data_preprocessor.ClsDataPreprocessor.forward:9 of
msgid "Data in the same format as the model input."
msgstr ""

#: ../../api/generated/mmcls.models.utils.is_tracing.rst:2
msgid "mmcls.models.utils.is\\_tracing"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.helpers.is_tracing:1 of
msgid "Determine whether the model is called during the tracing of code with ``torch.jit.trace``."
msgstr ""

#: ../../api/generated/mmcls.models.utils.make_divisible.rst:2
msgid "mmcls.models.utils.make\\_divisible"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.make_divisible.make_divisible:1 of
msgid "Make divisible function."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:3 of
msgid ""
"This function rounds the channel number down to the nearest value that can be divisible by the divisor."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:6 of
msgid "The original channel number."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:8 of
msgid "The divisor to fully divide the channel number."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:10 of
msgid ""
"The minimum value of the output channel. Default: None, means that the minimum value equal to the divisor."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:13 of
msgid "The minimum ratio of the rounded channel number to the original channel number. Default: 0.9."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:17 of
msgid "The modified output channel number"
msgstr ""

#: ../../api/generated/mmcls.models.utils.resize_pos_embed.rst:2
msgid "mmcls.models.utils.resize\\_pos\\_embed"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.embed.resize_pos_embed:1 of
msgid "Resize pos_embed weights."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:3 of
msgid "Position embedding weights with shape [1, L, C]."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:6 mmcls.models.utils.embed.resize_relative_position_bias_table:3
#: of
msgid "The resolution of downsampled origin training image, in format (H, W)."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:9 mmcls.models.utils.embed.resize_relative_position_bias_table:6
#: of
msgid "The resolution of downsampled new training image, in format (H, W)."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:12 of
msgid ""
"Algorithm used for upsampling. Choose one from 'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. "
"Defaults to 'bicubic'."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:16 of
msgid "The number of extra tokens, such as cls_token. Defaults to 1."
msgstr ""

#: mmcls.models.utils.embed.resize_pos_embed:20 of
msgid "The resized pos_embed of shape [1, L_new, C]"
msgstr ""

#: ../../api/generated/mmcls.models.utils.resize_relative_position_bias_table.rst:2
msgid "mmcls.models.utils.resize\\_relative\\_position\\_bias\\_table"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.embed.resize_relative_position_bias_table:1 of
msgid "Resize relative position bias table."
msgstr ""

#: mmcls.models.utils.embed.resize_relative_position_bias_table:9 of
msgid "The relative position bias of the pretrained model."
msgstr ""

#: mmcls.models.utils.embed.resize_relative_position_bias_table:14 of
msgid "The resized relative position bias table."
msgstr ""

#: ../../api/generated/mmcls.models.utils.to_ntuple.rst:2
msgid "mmcls.models.utils.to\\_ntuple"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1 mmcls.models.utils.helpers._ntuple:1 of
msgid "A `to_tuple` function generator."
msgstr ""

#: mmcls.models.utils.helpers._ntuple:3 of
msgid ""
"It returns a function, this function will repeat the input to a tuple of length ``n`` if the input is not "
"an Iterable object, otherwise, return the input directly."
msgstr ""

#: mmcls.models.utils.helpers._ntuple:7 of
msgid "The number of the target length."
msgstr ""

#: ../../api/generated/mmcls.utils.collect_env.rst:2
msgid "mmcls.utils.collect\\_env"
msgstr ""

#: ../../api/utils.rst:16:<autosummary>:1 mmcls.utils.collect_env.collect_env:1 of
msgid "Collect the information of the running environments."
msgstr ""

#: ../../api/generated/mmcls.utils.register_all_modules.rst:2
msgid "mmcls.utils.register\\_all\\_modules"
msgstr ""

#: ../../api/utils.rst:16:<autosummary>:1 mmcls.utils.setup_env.register_all_modules:1 of
msgid "Register all modules in mmcls into the registries."
msgstr ""

#: mmcls.utils.setup_env.register_all_modules:3 of
msgid ""
"Whether initialize the mmcls default scope. If True, the global default scope will be set to `mmcls`, and "
"all registries will build modules from mmcls's registry node. To understand more about the registry, please "
"refer to https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/registry.md Defaults to True."
msgstr ""

#: ../../api/models.rst:7
msgid "mmcls.models"
msgstr ""

#: ../../api/models.rst:9
msgid ""
"The ``models`` package contains several sub-packages for addressing the different components of a model."
msgstr "``models`` 包中包含了若干子包，分别对应神经网络中不同的组件。"

#: ../../api/models.rst:11
msgid ""
":mod:`~mmcls.models.classifiers`: The top-level module which defines the whole process of a classification "
"model."
msgstr ":mod:`~mmcls.models.classifiers`：定义完整分类模型的顶级模块。"

#: ../../api/models.rst:12
msgid ":mod:`~mmcls.models.backbones`: Usually a feature extraction network, e.g., ResNet, MobileNet."
msgstr ":mod:`~mmcls.models.backbones`：用于特征提取的主干网络结构，如 ResNet、MobileNet。"

#: ../../api/models.rst:13
msgid ":mod:`~mmcls.models.necks`: The component between backbones and heads, e.g., GlobalAveragePooling."
msgstr ":mod:`~mmcls.models.necks`：位于主干网络和头部网络之间的过渡层，如 GlobalAveragePooling。"

#: ../../api/models.rst:14
msgid ""
":mod:`~mmcls.models.heads`: The component for specific tasks. In MMClassification, we provides heads for "
"classification."
msgstr ""
":mod:`~mmcls.models.heads`：用于特定任务的头部网络。在 MMClassification 中，我们提供了若干用于分类任务的头部"
"网络。"

#: ../../api/models.rst:15
msgid ":mod:`~mmcls.models.losses`: Loss functions."
msgstr ":mod:`~mmcls.models.losses`：损失函数"

#: ../../api/models.rst:16
msgid ":mod:`~mmcls.models.utils`: Some helper functions and common components used in various networks."
msgstr ":mod:`~mmcls.models.utils`：一些辅助函数，或是在多个网络中出现的公共模块。"

#: ../../api/models.rst:18
msgid ""
":mod:`~mmcls.models.utils.data_preprocessor`: The component before model to preprocess the inputs, e.g., "
"ClsDataPreprocessor."
msgstr ""
":mod:`~mmcls.models.utils.data_preprocessor`：对网络的输入进行预处理的模块，如 ``ClsDataPreprocessor``。"

#: ../../api/models.rst:19
msgid ":ref:`components`: Common components used in various networks."
msgstr ":ref:`components`：多个网络共用的一些公共模块。"

#: ../../api/models.rst:20
msgid ":ref:`helpers`: Helper functions."
msgstr ":ref:`helpers`：模型中用到的辅助函数。"

#: ../../api/models.rst:23
msgid "Build Functions"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1
msgid ":py:obj:`build_classifier <mmcls.models.build_classifier>`"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1
msgid ":py:obj:`build_backbone <mmcls.models.build_backbone>`"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1
msgid ":py:obj:`build_neck <mmcls.models.build_neck>`"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1
msgid ":py:obj:`build_head <mmcls.models.build_head>`"
msgstr ""

#: ../../api/models.rst:34:<autosummary>:1
msgid ":py:obj:`build_loss <mmcls.models.build_loss>`"
msgstr ""

#: ../../api/models.rst:38
msgid "Classifiers"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1
msgid ":py:obj:`BaseClassifier <mmcls.models.classifiers.BaseClassifier>`"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1
msgid ":py:obj:`ImageClassifier <mmcls.models.classifiers.ImageClassifier>`"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1
msgid ":py:obj:`TimmClassifier <mmcls.models.classifiers.TimmClassifier>`"
msgstr ""

#: ../../api/models.rst:49:<autosummary>:1
msgid ":py:obj:`HuggingFaceClassifier <mmcls.models.classifiers.HuggingFaceClassifier>`"
msgstr ""

#: ../../api/models.rst:53
msgid "Backbones"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`AlexNet <mmcls.models.backbones.AlexNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`CSPDarkNet <mmcls.models.backbones.CSPDarkNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`CSPNet <mmcls.models.backbones.CSPNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`CSPResNeXt <mmcls.models.backbones.CSPResNeXt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`CSPResNet <mmcls.models.backbones.CSPResNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`Conformer <mmcls.models.backbones.Conformer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ConvMixer <mmcls.models.backbones.ConvMixer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid "ConvMixer."
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ConvNeXt <mmcls.models.backbones.ConvNeXt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`DaViT <mmcls.models.backbones.DaViT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`DeiT3 <mmcls.models.backbones.DeiT3>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`DenseNet <mmcls.models.backbones.DenseNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`DistilledVisionTransformer <mmcls.models.backbones.DistilledVisionTransformer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`EdgeNeXt <mmcls.models.backbones.EdgeNeXt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`EfficientFormer <mmcls.models.backbones.EfficientFormer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`EfficientNet <mmcls.models.backbones.EfficientNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`HorNet <mmcls.models.backbones.HorNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`HRNet <mmcls.models.backbones.HRNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`InceptionV3 <mmcls.models.backbones.InceptionV3>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`LeNet5 <mmcls.models.backbones.LeNet5>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MViT <mmcls.models.backbones.MViT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MlpMixer <mmcls.models.backbones.MlpMixer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MobileNetV2 <mmcls.models.backbones.MobileNetV2>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MobileNetV3 <mmcls.models.backbones.MobileNetV3>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MobileOne <mmcls.models.backbones.MobileOne>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`MobileViT <mmcls.models.backbones.MobileViT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`PCPVT <mmcls.models.backbones.PCPVT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`PoolFormer <mmcls.models.backbones.PoolFormer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`RegNet <mmcls.models.backbones.RegNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`RepLKNet <mmcls.models.backbones.RepLKNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`RepMLPNet <mmcls.models.backbones.RepMLPNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`RepVGG <mmcls.models.backbones.RepVGG>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`Res2Net <mmcls.models.backbones.Res2Net>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNeSt <mmcls.models.backbones.ResNeSt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNeXt <mmcls.models.backbones.ResNeXt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNet <mmcls.models.backbones.ResNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNetV1c <mmcls.models.backbones.ResNetV1c>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNetV1d <mmcls.models.backbones.ResNetV1d>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ResNet_CIFAR <mmcls.models.backbones.ResNet_CIFAR>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`SEResNeXt <mmcls.models.backbones.SEResNeXt>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`SEResNet <mmcls.models.backbones.SEResNet>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`SVT <mmcls.models.backbones.SVT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ShuffleNetV1 <mmcls.models.backbones.ShuffleNetV1>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`ShuffleNetV2 <mmcls.models.backbones.ShuffleNetV2>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`SwinTransformer <mmcls.models.backbones.SwinTransformer>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`SwinTransformerV2 <mmcls.models.backbones.SwinTransformerV2>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`T2T_ViT <mmcls.models.backbones.T2T_ViT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`TIMMBackbone <mmcls.models.backbones.TIMMBackbone>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`TNT <mmcls.models.backbones.TNT>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`VAN <mmcls.models.backbones.VAN>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`VGG <mmcls.models.backbones.VGG>`"
msgstr ""

#: ../../api/models.rst:111:<autosummary>:1
msgid ":py:obj:`VisionTransformer <mmcls.models.backbones.VisionTransformer>`"
msgstr ""

#: ../../api/models.rst:115
msgid "Necks"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1
msgid ":py:obj:`GlobalAveragePooling <mmcls.models.necks.GlobalAveragePooling>`"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1
msgid ":py:obj:`GeneralizedMeanPooling <mmcls.models.necks.GeneralizedMeanPooling>`"
msgstr ""

#: ../../api/models.rst:125:<autosummary>:1
msgid ":py:obj:`HRFuseScales <mmcls.models.necks.HRFuseScales>`"
msgstr ""

#: ../../api/models.rst:129
msgid "Heads"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`ClsHead <mmcls.models.heads.ClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`LinearClsHead <mmcls.models.heads.LinearClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`StackedLinearClsHead <mmcls.models.heads.StackedLinearClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`VisionTransformerClsHead <mmcls.models.heads.VisionTransformerClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`EfficientFormerClsHead <mmcls.models.heads.EfficientFormerClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`DeiTClsHead <mmcls.models.heads.DeiTClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`ConformerHead <mmcls.models.heads.ConformerHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`ArcFaceClsHead <mmcls.models.heads.ArcFaceClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`MultiLabelClsHead <mmcls.models.heads.MultiLabelClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`MultiLabelLinearClsHead <mmcls.models.heads.MultiLabelLinearClsHead>`"
msgstr ""

#: ../../api/models.rst:147:<autosummary>:1
msgid ":py:obj:`CSRAClsHead <mmcls.models.heads.CSRAClsHead>`"
msgstr ""

#: ../../api/models.rst:151
msgid "Losses"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1
msgid ":py:obj:`CrossEntropyLoss <mmcls.models.losses.CrossEntropyLoss>`"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1
msgid ":py:obj:`LabelSmoothLoss <mmcls.models.losses.LabelSmoothLoss>`"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1
msgid ":py:obj:`FocalLoss <mmcls.models.losses.FocalLoss>`"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1
msgid ":py:obj:`AsymmetricLoss <mmcls.models.losses.AsymmetricLoss>`"
msgstr ""

#: ../../api/models.rst:163:<autosummary>:1
msgid ":py:obj:`SeesawLoss <mmcls.models.losses.SeesawLoss>`"
msgstr ""

#: ../../api/models.rst:167
msgid "models.utils"
msgstr ""

#: ../../api/models.rst:169
msgid "This package includes some helper functions and common components used in various networks."
msgstr ""

#: ../../api/models.rst:174
msgid "Common Components"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`InvertedResidual <mmcls.models.utils.InvertedResidual>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`SELayer <mmcls.models.utils.SELayer>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`WindowMSA <mmcls.models.utils.WindowMSA>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`WindowMSAV2 <mmcls.models.utils.WindowMSAV2>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`ShiftWindowMSA <mmcls.models.utils.ShiftWindowMSA>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`MultiheadAttention <mmcls.models.utils.MultiheadAttention>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`ConditionalPositionEncoding <mmcls.models.utils.ConditionalPositionEncoding>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`PatchEmbed <mmcls.models.utils.PatchEmbed>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`PatchMerging <mmcls.models.utils.PatchMerging>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`HybridEmbed <mmcls.models.utils.HybridEmbed>`"
msgstr ""

#: ../../api/models.rst:192:<autosummary>:1
msgid ":py:obj:`LayerScale <mmcls.models.utils.LayerScale>`"
msgstr ""

#: ../../api/models.rst:196
msgid "Helper Functions"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`channel_shuffle <mmcls.models.utils.channel_shuffle>`"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`make_divisible <mmcls.models.utils.make_divisible>`"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`resize_pos_embed <mmcls.models.utils.resize_pos_embed>`"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`resize_relative_position_bias_table <mmcls.models.utils.resize_relative_position_bias_table>`"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`to_ntuple <mmcls.models.utils.to_ntuple>`"
msgstr ""

#: ../../api/models.rst:207:<autosummary>:1
msgid ":py:obj:`is_tracing <mmcls.models.utils.is_tracing>`"
msgstr ""

#: ../../api/structures.rst:7
msgid "mmcls.structures"
msgstr ""

#: ../../api/structures.rst:9
msgid "This package includes basic data structures for classification tasks."
msgstr "该包中包含了用于分类任务的基础数据结构。"

#: ../../api/structures.rst:12
msgid "ClsDataSample"
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:1 of
msgid "A data structure interface of classification task."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:3 of
msgid "It's used as interfaces between different components."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample of
msgid "Meta fields"
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:5 of
msgid "**img_shape** (*Tuple*) -- The shape of the corresponding input image. Used for visualization."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:7 of
msgid "**ori_shape** (*Tuple*) -- The original shape of the corresponding image. Used for visualization."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:9 of
msgid "**num_classes** (*int*) -- The number of all categories. Used for label format conversion."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample of
msgid "Data fields"
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:12 of
msgid "**gt_label** (:obj:`~mmengine.structures.LabelData`) -- The ground truth label."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:14 of
msgid "**pred_label** (:obj:`~mmengine.structures.LabelData`) -- The predicted label."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:16 of
msgid "**scores** (*torch.Tensor*) -- The outputs of model."
msgstr ""

#: mmcls.structures.cls_data_sample.ClsDataSample:17 of
msgid "**logits** (*torch.Tensor*) -- The outputs of model without softmax nor sigmoid."
msgstr ""

#: ../../api/utils.rst:7
msgid "mmcls.utils"
msgstr ""

#: ../../api/utils.rst:9
msgid "This package includes some useful helper functions for developing."
msgstr "该包中包含了一些有助于开发的辅助函数。"

#: ../../api/utils.rst:16:<autosummary>:1
msgid ":py:obj:`collect_env <mmcls.utils.collect_env>`"
msgstr ""

#: ../../api/utils.rst:16:<autosummary>:1
msgid ":py:obj:`register_all_modules <mmcls.utils.register_all_modules>`"
msgstr ""

#: ../../api/visualization.rst:7
msgid "mmcls.visualization"
msgstr ""

#: ../../api/visualization.rst:9
msgid "This package includes visualizer components for classification tasks."
msgstr "该包中包含了用于分类任务的一些可视化组件。"

#: ../../api/visualization.rst:12
msgid "ClsVisualizer"
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:1 of
msgid "Universal Visualizer for classification task."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:3 of
msgid "Name of the instance. Defaults to 'visualizer'."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:5 of
msgid "the origin image to draw. The format should be RGB. Defaults to None."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:8 of
msgid "Visual backend config list. Defaults to None."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:11 of
msgid "Save file dir for all storage backends. If it is None, the backend storage will not save any data."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:14 of
msgid "Keyword parameters of figure for saving. Defaults to empty dict."
msgstr ""

#: mmcls.visualization.cls_visualizer.ClsVisualizer:17 of
msgid "Keyword parameters of figure for showing. Defaults to empty dict."
msgstr ""
