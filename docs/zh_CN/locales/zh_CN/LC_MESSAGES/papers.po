# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, OpenMMLab
# This file is distributed under the same license as the MMClassification
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
msgid ""
msgstr ""
"Project-Id-Version: MMClassification\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-22 08:42+0800\n"
"PO-Revision-Date: 2022-11-22 14:24+0800\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Last-Translator: Ma Zerun <mzr1996@163.com>\n"
"Language-Team: \n"
"Language: zh_CN\n"
"X-Generator: Poedit 2.3\n"

#: ../../papers/conformer.md:4
msgid "Conformer"
msgstr ""

#: ../../papers/conformer.md:6
msgid ""
"[Conformer: Local Features Coupling Global Representations for Visual Recognition](https://arxiv.org/"
"abs/2105.03889)"
msgstr ""

#: ../../papers/conformer.md:10 ../../papers/convmixer.md:10 ../../papers/convnext.md:20
#: ../../papers/cspnet.md:10 ../../papers/csra.md:10 ../../papers/davit.md:10 ../../papers/deit.md:10
#: ../../papers/deit3.md:10 ../../papers/densenet.md:10 ../../papers/edgenext.md:10
#: ../../papers/efficientformer.md:10 ../../papers/efficientnet.md:20 ../../papers/hornet.md:10
#: ../../papers/hrnet.md:10 ../../papers/inception_v3.md:10 ../../papers/mlp_mixer.md:10
#: ../../papers/mobilenet_v2.md:10 ../../papers/mobilenet_v3.md:10 ../../papers/mobileone.md:21
#: ../../papers/mobilevit.md:24 ../../papers/mvit.md:10 ../../papers/poolformer.md:10
#: ../../papers/regnet.md:10 ../../papers/replknet.md:10 ../../papers/repmlp.md:10 ../../papers/repvgg.md:10
#: ../../papers/res2net.md:10 ../../papers/resnet.md:28 ../../papers/resnext.md:10 ../../papers/seresnet.md:10
#: ../../papers/shufflenet_v1.md:10 ../../papers/shufflenet_v2.md:10 ../../papers/swin_transformer.md:20
#: ../../papers/swin_transformer_v2.md:28 ../../papers/t2t_vit.md:10 ../../papers/tnt.md:10
#: ../../papers/twins.md:10 ../../papers/van.md:10 ../../papers/vgg.md:10
#: ../../papers/vision_transformer.md:20 ../../papers/wrn.md:10
msgid "Abstract"
msgstr "摘要"

#: ../../papers/conformer.md:12
#, python-format
msgid ""
"Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features "
"but experience difficulty to capture global representations. Within visual transformer, the cascaded self-"
"attention modules can capture long-distance feature dependencies but unfortunately deteriorate local "
"feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take advantage "
"of convolutional operations and self-attention mechanisms for enhanced representation learning. Conformer "
"roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under "
"different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local "
"features and global representations are retained to the maximum extent. Experiments show that Conformer, "
"under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. "
"On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, "
"respectively, demonstrating the great potential to be a general backbone network."
msgstr ""

#: ../../papers/conformer.md:18 ../../papers/convmixer.md:22 ../../papers/convnext.md:85
#: ../../papers/cspnet.md:22 ../../papers/csra.md:18 ../../papers/davit.md:18 ../../papers/deit.md:18
#: ../../papers/deit3.md:18 ../../papers/densenet.md:18 ../../papers/edgenext.md:22
#: ../../papers/efficientformer.md:18 ../../papers/efficientnet.md:31 ../../papers/hornet.md:18
#: ../../papers/hrnet.md:18 ../../papers/inception_v3.md:18 ../../papers/mlp_mixer.md:18
#: ../../papers/mobilenet_v2.md:20 ../../papers/mobilenet_v3.md:18 ../../papers/mobileone.md:151
#: ../../papers/mobilevit.md:89 ../../papers/mvit.md:25 ../../papers/poolformer.md:18
#: ../../papers/regnet.md:18 ../../papers/replknet.md:18 ../../papers/repmlp.md:18 ../../papers/repvgg.md:18
#: ../../papers/res2net.md:18 ../../papers/resnet.md:94 ../../papers/resnext.md:18 ../../papers/seresnet.md:18
#: ../../papers/shufflenet_v1.md:18 ../../papers/shufflenet_v2.md:18 ../../papers/swin_transformer.md:84
#: ../../papers/swin_transformer_v2.md:94 ../../papers/t2t_vit.md:18 ../../papers/tnt.md:18
#: ../../papers/twins.md:18 ../../papers/van.md:18 ../../papers/vgg.md:18
#: ../../papers/vision_transformer.md:89 ../../papers/wrn.md:18
msgid "Results and models"
msgstr "结果和模型"

#: ../../papers/conformer.md:20 ../../papers/convmixer.md:24 ../../papers/convnext.md:68
#: ../../papers/convnext.md:87 ../../papers/cspnet.md:24 ../../papers/davit.md:20 ../../papers/deit.md
#: ../../papers/deit.md:20 ../../papers/deit3.md:20 ../../papers/densenet.md:20 ../../papers/edgenext.md:24
#: ../../papers/efficientformer.md:20 ../../papers/efficientnet.md:33 ../../papers/hornet.md:20
#: ../../papers/hrnet.md:20 ../../papers/inception_v3.md:20 ../../papers/mlp_mixer.md:20
#: ../../papers/mobilenet_v2.md:22 ../../papers/mobilenet_v3.md:20 ../../papers/mobileone.md:153
#: ../../papers/mobilevit.md:91 ../../papers/mvit.md:27 ../../papers/poolformer.md:20
#: ../../papers/regnet.md:20 ../../papers/replknet.md:20 ../../papers/repmlp.md:20 ../../papers/repvgg.md:20
#: ../../papers/res2net.md:20 ../../papers/resnet.md:120 ../../papers/resnext.md:20
#: ../../papers/seresnet.md:20 ../../papers/shufflenet_v1.md:20 ../../papers/shufflenet_v2.md:20
#: ../../papers/swin_transformer.md:97 ../../papers/swin_transformer_v2.md:105 ../../papers/t2t_vit.md:20
#: ../../papers/tnt.md:20 ../../papers/twins.md:20 ../../papers/van.md:20 ../../papers/vgg.md:20
#: ../../papers/vision_transformer.md:109 ../../papers/wrn.md:20
msgid "ImageNet-1k"
msgstr ""

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/csra.md ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md
#: ../../papers/densenet.md ../../papers/edgenext.md ../../papers/efficientformer.md
#: ../../papers/efficientnet.md ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md
#: ../../papers/mlp_mixer.md ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md
#: ../../papers/mobileone.md:86 ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md
#: ../../papers/regnet.md ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md
#: ../../papers/shufflenet_v1.md ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md
#: ../../papers/van.md ../../papers/vgg.md ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Model"
msgstr "模型"

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/csra.md ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md
#: ../../papers/densenet.md ../../papers/edgenext.md ../../papers/efficientformer.md
#: ../../papers/efficientnet.md ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md
#: ../../papers/mlp_mixer.md ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md
#: ../../papers/mobileone.md:86 ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md
#: ../../papers/regnet.md ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md
#: ../../papers/shufflenet_v1.md ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md
#: ../../papers/van.md ../../papers/vgg.md ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Params(M)"
msgstr "参数量（M）"

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/csra.md ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md
#: ../../papers/densenet.md ../../papers/edgenext.md ../../papers/efficientformer.md
#: ../../papers/efficientnet.md ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md
#: ../../papers/mlp_mixer.md ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md
#: ../../papers/mobileone.md:86 ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md
#: ../../papers/regnet.md ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md
#: ../../papers/shufflenet_v1.md ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md
#: ../../papers/van.md ../../papers/vgg.md ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Flops(G)"
msgstr ""

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md ../../papers/densenet.md
#: ../../papers/edgenext.md ../../papers/efficientformer.md ../../papers/efficientnet.md
#: ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md ../../papers/mlp_mixer.md
#: ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md ../../papers/mobileone.md:86
#: ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md ../../papers/regnet.md
#: ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md ../../papers/res2net.md
#: ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md ../../papers/shufflenet_v1.md
#: ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
#: ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md ../../papers/van.md ../../papers/vgg.md
#: ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Top-1 (%)"
msgstr ""

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md ../../papers/densenet.md
#: ../../papers/edgenext.md ../../papers/efficientformer.md ../../papers/efficientnet.md
#: ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md ../../papers/mlp_mixer.md
#: ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md ../../papers/mobileone.md:86
#: ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md ../../papers/regnet.md
#: ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md ../../papers/res2net.md
#: ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md ../../papers/shufflenet_v1.md
#: ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
#: ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md ../../papers/van.md ../../papers/vgg.md
#: ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Top-5 (%)"
msgstr ""

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/csra.md ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md
#: ../../papers/densenet.md ../../papers/edgenext.md ../../papers/efficientformer.md
#: ../../papers/efficientnet.md ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md
#: ../../papers/mlp_mixer.md ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md
#: ../../papers/mobileone.md:86 ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md
#: ../../papers/regnet.md ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md
#: ../../papers/shufflenet_v1.md ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md
#: ../../papers/van.md ../../papers/vgg.md ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Config"
msgstr "配置文件"

#: ../../papers/conformer.md ../../papers/convmixer.md ../../papers/convnext.md:68 ../../papers/cspnet.md
#: ../../papers/csra.md ../../papers/davit.md ../../papers/deit.md ../../papers/deit3.md
#: ../../papers/densenet.md ../../papers/edgenext.md ../../papers/efficientformer.md
#: ../../papers/efficientnet.md ../../papers/hornet.md ../../papers/hrnet.md ../../papers/inception_v3.md
#: ../../papers/mlp_mixer.md ../../papers/mobilenet_v2.md ../../papers/mobilenet_v3.md
#: ../../papers/mobileone.md:86 ../../papers/mobilevit.md:71 ../../papers/mvit.md ../../papers/poolformer.md
#: ../../papers/regnet.md ../../papers/replknet.md ../../papers/repmlp.md ../../papers/repvgg.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/resnext.md ../../papers/seresnet.md
#: ../../papers/shufflenet_v1.md ../../papers/shufflenet_v2.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/t2t_vit.md ../../papers/tnt.md ../../papers/twins.md
#: ../../papers/van.md ../../papers/vgg.md ../../papers/vision_transformer.md:71 ../../papers/wrn.md
msgid "Download"
msgstr "下载"

#: ../../papers/conformer.md
msgid "Conformer-tiny-p16\\*"
msgstr ""

#: ../../papers/conformer.md ../../papers/resnet.md:76
msgid "23.52"
msgstr ""

#: ../../papers/conformer.md
msgid "4.90"
msgstr ""

#: ../../papers/conformer.md
msgid "81.31"
msgstr ""

#: ../../papers/conformer.md
msgid "95.60"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/conformer/conformer-tiny-"
"p16_8xb128_in1k.py)"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/conformer/conformer-tiny-"
"p16_3rdparty_8xb128_in1k_20211206-f6860372.pth)"
msgstr ""

#: ../../papers/conformer.md
msgid "Conformer-small-p32\\*"
msgstr ""

#: ../../papers/conformer.md ../../papers/deit3.md
msgid "38.85"
msgstr ""

#: ../../papers/conformer.md
msgid "7.09"
msgstr ""

#: ../../papers/conformer.md
msgid "81.96"
msgstr ""

#: ../../papers/conformer.md
msgid "96.02"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/conformer/conformer-small-"
"p32_8xb128_in1k.py)"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/conformer/conformer-small-"
"p32_8xb128_in1k_20211206-947a0816.pth)"
msgstr ""

#: ../../papers/conformer.md
msgid "Conformer-small-p16\\*"
msgstr ""

#: ../../papers/conformer.md
msgid "37.67"
msgstr ""

#: ../../papers/conformer.md
msgid "10.31"
msgstr ""

#: ../../papers/conformer.md
msgid "83.32"
msgstr ""

#: ../../papers/conformer.md
msgid "96.46"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/conformer/conformer-small-"
"p16_8xb128_in1k.py)"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/conformer/conformer-small-"
"p16_3rdparty_8xb128_in1k_20211206-3065dcf5.pth)"
msgstr ""

#: ../../papers/conformer.md
msgid "Conformer-base-p16\\*"
msgstr ""

#: ../../papers/conformer.md
msgid "83.29"
msgstr ""

#: ../../papers/conformer.md
msgid "22.89"
msgstr ""

#: ../../papers/conformer.md ../../papers/efficientnet.md
msgid "83.82"
msgstr ""

#: ../../papers/conformer.md ../../papers/twins.md
msgid "96.59"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/conformer/conformer-base-"
"p16_8xb128_in1k.py)"
msgstr ""

#: ../../papers/conformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/conformer/conformer-base-"
"p16_3rdparty_8xb128_in1k_20211206-bfdf8637.pth)"
msgstr ""

#: ../../papers/conformer.md:29
msgid ""
"*Models with * are converted from the [official repo](https://github.com/pengzhiliang/Conformer). The "
"config files of these models are only for validation. We don't ensure these config files' training accuracy "
"and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/conformer.md:31 ../../papers/convmixer.md:34 ../../papers/convnext.md:116
#: ../../papers/cspnet.md:34 ../../papers/csra.md:26 ../../papers/davit.md:32 ../../papers/deit.md:43
#: ../../papers/deit3.md:43 ../../papers/densenet.md:31 ../../papers/edgenext.md:37
#: ../../papers/efficientformer.md:30 ../../papers/efficientnet.md:130 ../../papers/hornet.md:45
#: ../../papers/hrnet.md:36 ../../papers/inception_v3.md:28 ../../papers/mlp_mixer.md:29
#: ../../papers/mobilenet_v2.md:28 ../../papers/mobilenet_v3.md:35 ../../papers/mobileone.md:163
#: ../../papers/mobilevit.md:101 ../../papers/mvit.md:38 ../../papers/poolformer.md:32
#: ../../papers/regnet.md:43 ../../papers/replknet.md:88 ../../papers/repmlp.md:87 ../../papers/repvgg.md:94
#: ../../papers/res2net.md:30 ../../papers/resnet.md:152 ../../papers/resnext.md:29
#: ../../papers/seresnet.md:27 ../../papers/shufflenet_v1.md:26 ../../papers/shufflenet_v2.md:26
#: ../../papers/swin_transformer.md:120 ../../papers/swin_transformer_v2.md:124 ../../papers/t2t_vit.md:30
#: ../../papers/tnt.md:28 ../../papers/twins.md:33 ../../papers/van.md:31 ../../papers/vgg.md:33
#: ../../papers/vision_transformer.md:121 ../../papers/wrn.md:30
msgid "Citation"
msgstr "引用"

#: ../../papers/convmixer.md:4
msgid "ConvMixer"
msgstr ""

#: ../../papers/convmixer.md:6
msgid "[Patches Are All You Need?](https://arxiv.org/abs/2201.09792)"
msgstr ""

#: ../../papers/convmixer.md:14
msgid ""
"Although convolutional networks have been the dominant architecture for vision tasks for many years, recent "
"experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed "
"their performance in some settings. However, due to the quadratic runtime of the self-attention layers in "
"Transformers, ViTs require the use of patch embeddings, which group together small regions of the image "
"into single input features, in order to be applied to larger image sizes. This raises a question: Is the "
"performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly "
"due to using patches as the input representation? In this paper, we present some evidence for the latter: "
"specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and "
"the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of "
"spatial and channel dimensions, and maintains equal size and resolution throughout the network. In "
"contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its "
"simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for "
"similar parameter counts and data set sizes, in addition to outperforming classical vision models such as "
"the ResNet."
msgstr ""

#: ../../papers/convmixer.md
msgid "ConvMixer-768/32\\*"
msgstr ""

#: ../../papers/convmixer.md
msgid "21.11"
msgstr ""

#: ../../papers/convmixer.md
msgid "19.62"
msgstr ""

#: ../../papers/convmixer.md
msgid "80.16"
msgstr ""

#: ../../papers/convmixer.md
msgid "95.08"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convmixer/"
"convmixer-768-32_10xb64_in1k.py)"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convmixer/"
"convmixer-768-32_3rdparty_10xb64_in1k_20220323-bca1f7b8.pth)"
msgstr ""

#: ../../papers/convmixer.md
msgid "ConvMixer-1024/20\\*"
msgstr ""

#: ../../papers/convmixer.md
msgid "24.38"
msgstr ""

#: ../../papers/convmixer.md
msgid "5.55"
msgstr ""

#: ../../papers/convmixer.md
msgid "76.94"
msgstr ""

#: ../../papers/convmixer.md
msgid "93.36"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convmixer/"
"convmixer-1024-20_10xb64_in1k.py)"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convmixer/"
"convmixer-1024-20_3rdparty_10xb64_in1k_20220323-48f8aeba.pth)"
msgstr ""

#: ../../papers/convmixer.md
msgid "ConvMixer-1536/20\\*"
msgstr ""

#: ../../papers/convmixer.md
msgid "51.63"
msgstr ""

#: ../../papers/convmixer.md
msgid "48.71"
msgstr ""

#: ../../papers/convmixer.md
msgid "81.37"
msgstr ""

#: ../../papers/convmixer.md ../../papers/swin_transformer.md:66
msgid "95.61"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convmixer/"
"convmixer-1536-20_10xb64_in1k.py)"
msgstr ""

#: ../../papers/convmixer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convmixer/"
"convmixer-1536_20_3rdparty_10xb64_in1k_20220323-ea5786f3.pth)"
msgstr ""

#: ../../papers/convmixer.md:32
msgid ""
"*Models with * are converted from the [official repo](https://github.com/locuslab/convmixer). The config "
"files of these models are only for inference. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/convnext.md:4
msgid "ConvNeXt"
msgstr ""

#: ../../papers/convnext.md:6
msgid "[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545v1)"
msgstr ""

#: ../../papers/convnext.md:10 ../../papers/efficientnet.md:10 ../../papers/mobileone.md:10
#: ../../papers/mobilevit.md:10 ../../papers/resnet.md:10 ../../papers/swin_transformer.md:10
#: ../../papers/swin_transformer_v2.md:10 ../../papers/vision_transformer.md:10
msgid "Introduction"
msgstr "简介"

#: ../../papers/convnext.md:12
msgid ""
"**ConvNeXt** is initially described in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545v1), which "
"is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers. The ConvNeXt has "
"the pyramid structure and achieve competitive  performance on various vision tasks, with simplicity and "
"efficiency."
msgstr ""

#: ../../papers/convnext.md:34 ../../papers/efficientnet.md:78 ../../papers/mobilevit.md:37
#: ../../papers/resnet.md:42 ../../papers/swin_transformer.md:32 ../../papers/swin_transformer_v2.md:42
#: ../../papers/vision_transformer.md:33
msgid "How to use it?"
msgstr "使用方式"

#: ../../papers/convnext.md:39 ../../papers/efficientnet.md:83 ../../papers/mobileone.md:40
#: ../../papers/mobilevit.md:42 ../../papers/resnet.md:47 ../../papers/swin_transformer.md:37
#: ../../papers/swin_transformer_v2.md:47 ../../papers/vision_transformer.md:38
msgid "Predict image"
msgstr "推理图片"

#: ../../papers/convnext.md:52 ../../papers/efficientnet.md:96 ../../papers/mobileone.md:63
#: ../../papers/mobilevit.md:55 ../../papers/resnet.md:60 ../../papers/swin_transformer.md:50
#: ../../papers/swin_transformer_v2.md:60 ../../papers/vision_transformer.md:51
msgid "Use the model"
msgstr "调用模型"

#: ../../papers/convnext.md:69 ../../papers/efficientnet.md:113 ../../papers/mobileone.md:87
#: ../../papers/mobilevit.md:72 ../../papers/resnet.md:77 ../../papers/swin_transformer.md:67
#: ../../papers/swin_transformer_v2.md:77 ../../papers/vision_transformer.md:72
msgid "Train/Test Command"
msgstr "训练/测试"

#: ../../papers/convnext.md:69 ../../papers/efficientnet.md:113 ../../papers/mobileone.md:87
#: ../../papers/mobilevit.md:72 ../../papers/resnet.md:77 ../../papers/swin_transformer.md:67
#: ../../papers/swin_transformer_v2.md:77 ../../papers/vision_transformer.md:72
msgid ""
"Place the ImageNet dataset to the `data/imagenet/` directory, or prepare datasets according to the [docs]"
"(https://mmclassification.readthedocs.io/en/1.x/user_guides/dataset_prepare.html#prepare-dataset)."
msgstr ""
"将 ImageNet 数据集放置在 `data/imagenet` 目录下，或者根据 [docs](https://mmclassification.readthedocs.io/"
"en/1.x/user_guides/dataset_prepare.html#prepare-dataset) 准备其他数据集。"

#: ../../papers/convnext.md:71 ../../papers/efficientnet.md:115 ../../papers/mobileone.md:89
#: ../../papers/mobilevit.md:74 ../../papers/resnet.md:79 ../../papers/swin_transformer.md:69
#: ../../papers/swin_transformer_v2.md:79 ../../papers/vision_transformer.md:74
msgid "Train:"
msgstr "训练："

#: ../../papers/convnext.md:83
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.ConvNeXt.html#mmcls.models.backbones.ConvNeXt)."
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/cspnet.md ../../papers/csra.md ../../papers/davit.md
#: ../../papers/deit.md ../../papers/deit3.md ../../papers/edgenext.md ../../papers/hornet.md
#: ../../papers/mvit.md ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/van.md ../../papers/vision_transformer.md:71
msgid "Pretrain"
msgstr "预训练"

#: ../../papers/convnext.md:68
msgid "ConvNeXt-T\\*"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/cspnet.md ../../papers/davit.md ../../papers/deit.md
#: ../../papers/deit3.md ../../papers/edgenext.md ../../papers/hornet.md ../../papers/mvit.md
#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76 ../../papers/van.md
#: ../../papers/vision_transformer.md:71
msgid "From scratch"
msgstr "从头训练"

#: ../../papers/convnext.md:68
msgid "28.59"
msgstr ""

#: ../../papers/convnext.md:68
msgid "4.46"
msgstr ""

#: ../../papers/convnext.md:68
msgid "82.05"
msgstr ""

#: ../../papers/convnext.md:68
msgid "95.86"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convnext/convnext-"
"tiny_32xb128_in1k.py)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"tiny_3rdparty_32xb128_in1k_20220124-18abde00.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid "ConvNeXt-S\\*"
msgstr ""

#: ../../papers/convnext.md:68
msgid "50.22"
msgstr ""

#: ../../papers/convnext.md:68
msgid "8.69"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/twins.md
msgid "83.13"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/efficientnet.md ../../papers/swin_transformer.md:66
msgid "96.44"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convnext/convnext-"
"small_32xb128_in1k.py)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"small_3rdparty_32xb128_in1k_20220124-d39b5192.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid "ConvNeXt-B\\*"
msgstr ""

#: ../../papers/convnext.md:68
msgid "88.59"
msgstr ""

#: ../../papers/convnext.md:68
msgid "15.36"
msgstr ""

#: ../../papers/convnext.md:68
msgid "83.85"
msgstr ""

#: ../../papers/convnext.md:68
msgid "96.74"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convnext/convnext-"
"base_32xb128_in1k.py)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"base_3rdparty_32xb128_in1k_20220124-d0915162.pth)"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/deit3.md ../../papers/hornet.md
#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer.md:86
#: ../../papers/swin_transformer_v2.md:76 ../../papers/swin_transformer_v2.md:96
#: ../../papers/vision_transformer.md:71 ../../papers/vision_transformer.md:97
msgid "ImageNet-21k"
msgstr ""

#: ../../papers/convnext.md:68
msgid "85.81"
msgstr ""

#: ../../papers/convnext.md:68
msgid "97.86"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_in21k-"
"pre-3rdparty_32xb128_in1k_20220124-eb2d6ada.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid "ConvNeXt-L\\*"
msgstr ""

#: ../../papers/convnext.md:68
msgid "197.77"
msgstr ""

#: ../../papers/convnext.md:68
msgid "34.37"
msgstr ""

#: ../../papers/convnext.md:68
msgid "84.30"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/efficientnet.md
msgid "96.89"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convnext/convnext-"
"large_64xb64_in1k.py)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"large_3rdparty_64xb64_in1k_20220124-f8a0ded0.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid "86.61"
msgstr ""

#: ../../papers/convnext.md:68
msgid "98.04"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-large_in21k-"
"pre-3rdparty_64xb64_in1k_20220124-2412403d.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid "ConvNeXt-XL\\*"
msgstr ""

#: ../../papers/convnext.md:68
msgid "350.20"
msgstr ""

#: ../../papers/convnext.md:68
msgid "60.93"
msgstr ""

#: ../../papers/convnext.md:68 ../../papers/deit3.md
msgid "86.97"
msgstr ""

#: ../../papers/convnext.md:68
msgid "98.20"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/convnext/convnext-"
"xlarge_64xb64_in1k.py)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-xlarge_in21k-"
"pre-3rdparty_64xb64_in1k_20220124-76b6863d.pth)"
msgstr ""

#: ../../papers/convnext.md:99
msgid ""
"*Models with * are converted from the [official repo](https://github.com/facebookresearch/ConvNeXt). The "
"config files of these models are only for inference. We don't ensure these config files' training accuracy "
"and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/convnext.md:101 ../../papers/hornet.md:33
msgid "Pre-trained Models"
msgstr ""

#: ../../papers/convnext.md:103
msgid "The pre-trained models on ImageNet-1k or ImageNet-21k are used to fine-tune on the downstream tasks."
msgstr ""

#: ../../papers/convnext.md:68
msgid "Training Data"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_3rdparty_32xb128-"
"noema_in1k_20220222-2908964a.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_3rdparty_32xb128-"
"noema_in1k_20220222-fa001ca5.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_3rdparty_32xb128-"
"noema_in1k_20220222-dba4f95f.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"base_3rdparty_in21k_20220124-13b83eec.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-"
"large_3rdparty_in21k_20220124-41b5a79f.pth)"
msgstr ""

#: ../../papers/convnext.md:68
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/convnext/convnext-xlarge_3rdparty_in21k_20220124-"
"f909bad7.pth)"
msgstr ""

#: ../../papers/convnext.md:114
msgid "*Models with * are converted from the [official repo](https://github.com/facebookresearch/ConvNeXt).*"
msgstr ""

#: ../../papers/cspnet.md:4
msgid "CSPNet"
msgstr ""

#: ../../papers/cspnet.md:6
msgid "[CSPNet: A New Backbone that can Enhance Learning Capability of CNN](https://arxiv.org/abs/1911.11929)"
msgstr ""

#: ../../papers/cspnet.md:14
msgid ""
"Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision "
"tasks such as object detection. However, such success greatly relies on costly computation resources, which "
"hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose "
"Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference "
"computations from the network architecture perspective. We attribute the problem to the duplicate gradient "
"information within network optimization. The proposed networks respect the variability of the gradients by "
"integrating feature maps from the beginning and the end of a network stage, which, in our experiments, "
"reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and "
"significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection "
"dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, "
"ResNeXt, and DenseNet. Source code is at this https URL."
msgstr ""

#: ../../papers/cspnet.md
msgid "CSPDarkNet50\\*"
msgstr ""

#: ../../papers/cspnet.md
msgid "27.64"
msgstr ""

#: ../../papers/cspnet.md
msgid "5.04"
msgstr ""

#: ../../papers/cspnet.md
msgid "80.05"
msgstr ""

#: ../../papers/cspnet.md ../../papers/efficientnet.md
msgid "95.07"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/cspnet/cspdarknet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/cspnet/cspdarknet50_3rdparty_8xb32_in1k_20220329-"
"bd275287.pth)"
msgstr ""

#: ../../papers/cspnet.md
msgid "CSPResNet50\\*"
msgstr ""

#: ../../papers/cspnet.md
msgid "21.62"
msgstr ""

#: ../../papers/cspnet.md
msgid "3.48"
msgstr ""

#: ../../papers/cspnet.md ../../papers/resnet.md:76
msgid "79.55"
msgstr ""

#: ../../papers/cspnet.md ../../papers/repvgg.md
msgid "94.68"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/cspnet/cspresnet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/cspnet/cspresnet50_3rdparty_8xb32_in1k_20220329-"
"dd6dddfb.pth)"
msgstr ""

#: ../../papers/cspnet.md
msgid "CSPResNeXt50\\*"
msgstr ""

#: ../../papers/cspnet.md
msgid "20.57"
msgstr ""

#: ../../papers/cspnet.md
msgid "3.11"
msgstr ""

#: ../../papers/cspnet.md
msgid "79.96"
msgstr ""

#: ../../papers/cspnet.md ../../papers/efficientnet.md
msgid "94.96"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/cspnet/cspresnext50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/cspnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/cspnet/"
"cspresnext50_3rdparty_8xb32_in1k_20220329-2cc84d21.pth)"
msgstr ""

#: ../../papers/cspnet.md:32
msgid ""
"*Models with * are converted from the [timm repo](https://github.com/rwightman/pytorch-image-models). The "
"config files of these models are only for inference. We don't ensure these config files' training accuracy "
"and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/csra.md:4
msgid "CSRA"
msgstr ""

#: ../../papers/csra.md:6
msgid ""
"[Residual Attention: A Simple but Effective Method for Multi-Label Recognition](https://arxiv.org/"
"abs/2108.02456)"
msgstr ""

#: ../../papers/csra.md:12
msgid ""
"Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this "
"area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive "
"explanations. To effectively capture different spatial regions occupied by objects from different "
"categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). "
"CSRA generates class-specific features for every category by proposing a simple spatial attention score, "
"and then combines it with the class-agnostic average pooling feature. CSRA achieves state-of-the-art "
"results on multilabel recognition, and at the same time is much simpler than them. Furthermore, with only 4 "
"lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets "
"without any extra training. CSRA is both easy to implement and light in computations, which also enjoys "
"intuitive explanations and visualizations."
msgstr ""

#: ../../papers/csra.md:20
msgid "VOC2007"
msgstr ""

#: ../../papers/csra.md
msgid "mAP"
msgstr ""

#: ../../papers/csra.md
msgid "OF1 (%)"
msgstr ""

#: ../../papers/csra.md
msgid "CF1 (%)"
msgstr ""

#: ../../papers/csra.md
msgid "Resnet101-CSRA"
msgstr ""

#: ../../papers/csra.md
msgid ""
"[ImageNet-1k](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet101_8xb32_in1k_20210831-539c63f8.pth)"
msgstr ""

#: ../../papers/csra.md
msgid "23.55"
msgstr ""

#: ../../papers/csra.md ../../papers/resnet.md:76
msgid "4.12"
msgstr ""

#: ../../papers/csra.md
msgid "94.98"
msgstr ""

#: ../../papers/csra.md ../../papers/vgg.md
msgid "90.80"
msgstr ""

#: ../../papers/csra.md
msgid "89.16"
msgstr ""

#: ../../papers/csra.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/csra/resnet101-"
"csra_1xb16_voc07-448px.py)"
msgstr ""

#: ../../papers/csra.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/csra/resnet101-"
"csra_1xb16_voc07-448px_20220722-29efb40a.pth) | [log](https://download.openmmlab.com/mmclassification/v0/"
"csra/resnet101-csra_1xb16_voc07-448px_20220722-29efb40a.log.json)"
msgstr ""

#: ../../papers/davit.md:4
msgid "DaViT"
msgstr ""

#: ../../papers/davit.md:6
msgid "[DaViT: Dual Attention Vision Transformers](https://arxiv.org/abs/2204.03645v1)"
msgstr ""

#: ../../papers/davit.md:12
msgid ""
"In this work, we introduce Dual Attention Vision Transformers (DaViT), a simple yet effective vision "
"transformer architecture that is able to capture global context while maintaining computational efficiency. "
"We propose approaching the problem from an orthogonal angle: exploiting self-attention mechanisms with both "
"\"spatial tokens\" and \"channel tokens\". With spatial tokens, the spatial dimension defines the token "
"scope, and the channel dimension defines the token feature dimension. With channel tokens, we have the "
"inverse: the channel dimension defines the token scope, and the spatial dimension defines the token feature "
"dimension. We further group tokens along the sequence direction for both spatial and channel tokens to "
"maintain the linear complexity of the entire model. We show that these two self-attentions complement each "
"other: (i) since each channel token contains an abstract representation of the entire image, the channel "
"attention naturally captures global interactions and representations by taking all spatial positions into "
"account when computing attention scores between channels; (ii) the spatial attention refines the local "
"representations by performing fine-grained interactions across spatial locations, which in turn helps the "
"global information modeling in channel attention. Extensive experiments show our DaViT achieves state-of-"
"the-art performance on four different tasks with efficient computations. Without extra data, DaViT-Tiny, "
"DaViT-Small, and DaViT-Base achieve 82.8%, 84.2%, and 84.6% top-1 accuracy on ImageNet-1K with 28.3M, "
"49.7M, and 87.9M parameters, respectively. When we further scale up DaViT with 1.5B weakly supervised image "
"and text pairs, DaViT-Gaint reaches 90.4% top-1 accuracy on ImageNet-1K."
msgstr ""

#: ../../papers/davit.md ../../papers/deit3.md ../../papers/hornet.md ../../papers/res2net.md
#: ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
#: ../../papers/van.md ../../papers/vision_transformer.md:71
msgid "resolution"
msgstr "分辨率"

#: ../../papers/davit.md
msgid "DaViT-T\\*"
msgstr ""

#: ../../papers/davit.md ../../papers/deit3.md ../../papers/hornet.md ../../papers/replknet.md
#: ../../papers/res2net.md ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66 ../../papers/van.md
#: ../../papers/vision_transformer.md:71
msgid "224x224"
msgstr ""

#: ../../papers/davit.md
msgid "28.36"
msgstr ""

#: ../../papers/davit.md
msgid "4.54"
msgstr ""

#: ../../papers/davit.md
msgid "82.24"
msgstr ""

#: ../../papers/davit.md
msgid "96.13"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/davit/davit-tiny_4xb256_in1k.py)"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/davit/davit-tiny_3rdparty_in1k_20221116-700fdf7d."
"pth)"
msgstr ""

#: ../../papers/davit.md
msgid "DaViT-S\\*"
msgstr ""

#: ../../papers/davit.md
msgid "49.74"
msgstr ""

#: ../../papers/davit.md
msgid "8.79"
msgstr ""

#: ../../papers/davit.md
msgid "83.61"
msgstr ""

#: ../../papers/davit.md ../../papers/hornet.md
msgid "96.75"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/davit/davit-small_4xb256_in1k.py)"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/davit/davit-"
"small_3rdparty_in1k_20221116-51a849a6.pth)"
msgstr ""

#: ../../papers/davit.md
msgid "DaViT-B\\*"
msgstr ""

#: ../../papers/davit.md
msgid "87.95"
msgstr ""

#: ../../papers/davit.md ../../papers/vgg.md
msgid "15.5"
msgstr ""

#: ../../papers/davit.md
msgid "84.09"
msgstr ""

#: ../../papers/davit.md ../../papers/efficientnet.md
msgid "96.82"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/davit/davit-base_4xb256_in1k.py)"
msgstr ""

#: ../../papers/davit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/davit/davit-base_3rdparty_in1k_20221116-19e0d956."
"pth)"
msgstr ""

#: ../../papers/davit.md:28
msgid ""
"*Models with * are converted from the [official repo](https://github.com/dingmyu/davit). The config files "
"of these models are only for validation. We don't ensure these config files' training accuracy and welcome "
"you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/davit.md:30
msgid ""
"Note: Inference accuracy is a bit lower than paper result because of inference code for classification "
"doesn't exist."
msgstr ""

#: ../../papers/deit.md:4
msgid "DeiT"
msgstr ""

#: ../../papers/deit.md:6
msgid ""
"[Training data-efficient image transformers & distillation through attention](https://arxiv.org/"
"abs/2012.12877)"
msgstr ""

#: ../../papers/deit.md:12
msgid ""
"Recently, neural networks purely based on attention were shown to address image understanding tasks such as "
"image classification. However, these visual transformers are pre-trained with hundreds of millions of "
"images using an expensive infrastructure, thereby limiting their adoption.   In this work, we produce a "
"competitive convolution-free transformer by training on Imagenet only. We train them on a single computer "
"in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% "
"(single-crop evaluation) on ImageNet with no external data.   More importantly, we introduce a teacher-"
"student strategy specific to transformers. It relies on a distillation token ensuring that the student "
"learns from the teacher through attention. We show the interest of this token-based distillation, "
"especially when using a convnet as a teacher. This leads us to report results competitive with convnets for "
"both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our "
"code and models."
msgstr ""

#: ../../papers/deit.md:22
msgid "The teacher of the distilled version DeiT is RegNetY-16GF."
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-tiny"
msgstr ""

#: ../../papers/deit.md
msgid "5.72"
msgstr ""

#: ../../papers/deit.md
msgid "1.08"
msgstr ""

#: ../../papers/deit.md
msgid "74.50"
msgstr ""

#: ../../papers/deit.md
msgid "92.24"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-tiny_pt-4xb256_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-tiny_pt-4xb256_in1k_20220218-13b382a0."
"pth)  | [log](https://download.openmmlab.com/mmclassification/v0/deit/deit-"
"tiny_pt-4xb256_in1k_20220218-13b382a0.log.json)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-tiny distilled\\*"
msgstr ""

#: ../../papers/deit.md
msgid "74.51"
msgstr ""

#: ../../papers/deit.md
msgid "91.90"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-tiny-"
"distilled_pt-4xb256_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-tiny-"
"distilled_3rdparty_pt-4xb256_in1k_20211216-c429839a.pth)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-small"
msgstr ""

#: ../../papers/deit.md
msgid "22.05"
msgstr ""

#: ../../papers/deit.md
msgid "4.24"
msgstr ""

#: ../../papers/deit.md
msgid "80.69"
msgstr ""

#: ../../papers/deit.md
msgid "95.06"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-small_pt-4xb256_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb."
"pth)  | [log](https://download.openmmlab.com/mmclassification/v0/deit/deit-"
"small_pt-4xb256_in1k_20220218-9425b9bb.log.json)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-small distilled\\*"
msgstr ""

#: ../../papers/deit.md
msgid "81.17"
msgstr ""

#: ../../papers/deit.md
msgid "95.40"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-small-"
"distilled_pt-4xb256_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-small-"
"distilled_3rdparty_pt-4xb256_in1k_20211216-4de1d725.pth)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-base"
msgstr ""

#: ../../papers/deit.md
msgid "86.57"
msgstr ""

#: ../../papers/deit.md
msgid "16.86"
msgstr ""

#: ../../papers/deit.md ../../papers/swin_transformer_v2.md:76
msgid "81.76"
msgstr ""

#: ../../papers/deit.md
msgid "95.81"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-base_pt-16xb64_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c."
"pth)  | [log](https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-"
"db63c16c.log.json)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-base\\*"
msgstr ""

#: ../../papers/deit.md
msgid "81.79"
msgstr ""

#: ../../papers/deit.md
msgid "95.59"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-"
"base_3rdparty_pt-16xb64_in1k_20211124-6f40c188.pth)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-base distilled\\*"
msgstr ""

#: ../../papers/deit.md
msgid "83.33"
msgstr ""

#: ../../papers/deit.md
msgid "96.49"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-base-"
"distilled_pt-16xb64_in1k.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-base-"
"distilled_3rdparty_pt-16xb64_in1k_20211216-42891296.pth)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-base 384px\\*"
msgstr ""

#: ../../papers/deit.md ../../papers/vision_transformer.md:71
msgid "86.86"
msgstr ""

#: ../../papers/deit.md
msgid "49.37"
msgstr ""

#: ../../papers/deit.md
msgid "83.04"
msgstr ""

#: ../../papers/deit.md
msgid "96.31"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-"
"base_ft-16xb32_in1k-384px.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-"
"base_3rdparty_ft-16xb32_in1k-384px_20211124-822d02f2.pth)"
msgstr ""

#: ../../papers/deit.md
msgid "DeiT-base distilled 384px\\*"
msgstr ""

#: ../../papers/deit.md
msgid "85.55"
msgstr ""

#: ../../papers/deit.md
msgid "97.35"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit/deit-base-"
"distilled_ft-16xb32_in1k-384px.py)"
msgstr ""

#: ../../papers/deit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit/deit-base-"
"distilled_3rdparty_ft-16xb32_in1k-384px_20211216-e48d6000.pth)"
msgstr ""

#: ../../papers/deit.md:36 ../../papers/deit3.md:41
msgid ""
"*Models with * are converted from the [official repo](https://github.com/facebookresearch/deit). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/deit.md:39
msgid ""
"MMClassification doesn't support training the distilled version DeiT. And we provide distilled version "
"checkpoints for inference only."
msgstr ""

#: ../../papers/deit3.md:4
msgid "DeiT III: Revenge of the ViT"
msgstr ""

#: ../../papers/deit3.md:6
msgid "[DeiT III: Revenge of the ViT](https://arxiv.org/pdf/2204.07118.pdf)"
msgstr ""

#: ../../papers/deit3.md:12
msgid ""
"A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. "
"It has limited built-in architectural priors, in contrast to more recent architectures that incorporate "
"priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-"
"supervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the "
"supervised training of ViTs. Our procedure builds upon and simplifies a recipe introduced for training "
"ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations, closer to the "
"practice in self-supervised learning. Our evaluations on Image classification (ImageNet-1k with and without "
"pre-training on ImageNet-21k), transfer learning and semantic segmentation show that our procedure "
"outperforms by a large margin previous fully supervised training recipes for ViT. It also reveals that the "
"performance of our ViT trained with supervision is comparable to that of more recent architectures. Our "
"results could serve as better baselines for recent self-supervised approaches demonstrated on ViT."
msgstr ""

#: ../../papers/deit3.md
msgid "DeiT3-S\\*"
msgstr ""

#: ../../papers/deit3.md
msgid "22.06"
msgstr ""

#: ../../papers/deit3.md
msgid "4.61"
msgstr ""

#: ../../papers/deit3.md
msgid "81.35"
msgstr ""

#: ../../papers/deit3.md
msgid "95.31"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-small-p16_64xb64_in1k."
"py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-small-"
"p16_3rdparty_in1k_20221008-0f7c70cf.pth)"
msgstr ""

#: ../../papers/deit3.md ../../papers/hornet.md ../../papers/replknet.md ../../papers/swin_transformer.md:66
#: ../../papers/swin_transformer_v2.md:76 ../../papers/vision_transformer.md:71
msgid "384x384"
msgstr ""

#: ../../papers/deit3.md
msgid "22.21"
msgstr ""

#: ../../papers/deit3.md
msgid "15.52"
msgstr ""

#: ../../papers/deit3.md
msgid "83.43"
msgstr ""

#: ../../papers/deit3.md
msgid "96.68"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-small-"
"p16_64xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-small-"
"p16_3rdparty_in1k-384px_20221008-a2c1a0c7.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "83.06"
msgstr ""

#: ../../papers/deit3.md ../../papers/hornet.md
msgid "96.77"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-small-p16_in21k-"
"pre_3rdparty_in1k_20221009-dcd90827.pth)"
msgstr ""

#: ../../papers/deit3.md ../../papers/replknet.md
msgid "84.84"
msgstr ""

#: ../../papers/deit3.md
msgid "97.48"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-small-p16_in21k-"
"pre_3rdparty_in1k-384px_20221009-de116dd7.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "DeiT3-M\\*"
msgstr ""

#: ../../papers/deit3.md
msgid "8.00"
msgstr ""

#: ../../papers/deit3.md
msgid "82.99"
msgstr ""

#: ../../papers/deit3.md
msgid "96.22"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-medium-p16_64xb64_in1k."
"py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-medium-"
"p16_3rdparty_in1k_20221008-3b21284d.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "84.56"
msgstr ""

#: ../../papers/deit3.md
msgid "97.19"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-medium-p16_in21k-"
"pre_3rdparty_in1k_20221009-472f11e2.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "DeiT3-B\\*"
msgstr ""

#: ../../papers/deit3.md
msgid "86.59"
msgstr ""

#: ../../papers/deit3.md
msgid "17.58"
msgstr ""

#: ../../papers/deit3.md
msgid "83.80"
msgstr ""

#: ../../papers/deit3.md
msgid "96.55"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-base-p16_64xb64_in1k."
"py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-base-"
"p16_3rdparty_in1k_20221008-60b8c8bf.pth)"
msgstr ""

#: ../../papers/deit3.md ../../papers/swin_transformer.md:66
msgid "86.88"
msgstr ""

#: ../../papers/deit3.md
msgid "55.54"
msgstr ""

#: ../../papers/deit3.md
msgid "85.08"
msgstr ""

#: ../../papers/deit3.md
msgid "97.25"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-base-"
"p16_64xb32_in1k-384px.py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-base-"
"p16_3rdparty_in1k-384px_20221009-e19e36d4.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "85.70"
msgstr ""

#: ../../papers/deit3.md ../../papers/efficientnet.md ../../papers/replknet.md
msgid "97.75"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-base-p16_in21k-"
"pre_3rdparty_in1k_20221009-87983ca1.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "86.73"
msgstr ""

#: ../../papers/deit3.md
msgid "98.11"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-base-p16_in21k-"
"pre_3rdparty_in1k-384px_20221009-5e4e37b9.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "DeiT3-L\\*"
msgstr ""

#: ../../papers/deit3.md
msgid "304.37"
msgstr ""

#: ../../papers/deit3.md
msgid "61.60"
msgstr ""

#: ../../papers/deit3.md
msgid "84.87"
msgstr ""

#: ../../papers/deit3.md
msgid "97.01"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-large-p16_64xb64_in1k."
"py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-large-"
"p16_3rdparty_in1k_20221009-03b427ea.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "304.76"
msgstr ""

#: ../../papers/deit3.md
msgid "191.21"
msgstr ""

#: ../../papers/deit3.md
msgid "85.82"
msgstr ""

#: ../../papers/deit3.md
msgid "97.60"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-large-"
"p16_64xb16_in1k-384px.py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-large-"
"p16_3rdparty_in1k-384px_20221009-4317ce62.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "98.24"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-large-p16_in21k-"
"pre_3rdparty_in1k_20221009-d8d27084.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "87.73"
msgstr ""

#: ../../papers/deit3.md
msgid "98.51"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-large-p16_in21k-"
"pre_3rdparty_in1k-384px_20221009-75fea03f.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "DeiT3-H\\*"
msgstr ""

#: ../../papers/deit3.md
msgid "632.13"
msgstr ""

#: ../../papers/deit3.md
msgid "167.40"
msgstr ""

#: ../../papers/deit3.md
msgid "85.21"
msgstr ""

#: ../../papers/deit3.md
msgid "97.36"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/deit3/deit3-huge-p14_64xb32_in1k."
"py)"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-huge-p14_3rdparty_in1k_20221009-"
"e107bcb7.pth)"
msgstr ""

#: ../../papers/deit3.md
msgid "87.19"
msgstr ""

#: ../../papers/deit3.md
msgid "98.26"
msgstr ""

#: ../../papers/deit3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/deit3/deit3-huge-p14_in21k-"
"pre_3rdparty_in1k_20221009-19b8a535.pth)"
msgstr ""

#: ../../papers/densenet.md:4
msgid "DenseNet"
msgstr ""

#: ../../papers/densenet.md:6
msgid "[Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)"
msgstr ""

#: ../../papers/densenet.md:12
msgid ""
"Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient "
"to train if they contain shorter connections between layers close to the input and those close to the "
"output. In this paper, we embrace this observation and introduce the Dense Convolutional Network "
"(DenseNet), which connects each layer to every layer in a feed-forward fashion. Whereas traditional "
"convolutional networks with L layers have L connections - one between each layer and its subsequent layer - "
"our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are "
"used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have "
"several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature "
"propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our "
"proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, "
"SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, "
"whilst requiring less computation to achieve high performance."
msgstr ""

#: ../../papers/densenet.md
msgid "DenseNet121\\*"
msgstr ""

#: ../../papers/densenet.md
msgid "7.98"
msgstr ""

#: ../../papers/densenet.md
msgid "2.88"
msgstr ""

#: ../../papers/densenet.md
msgid "74.96"
msgstr ""

#: ../../papers/densenet.md
msgid "92.21"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/densenet/densenet121_4xb256_in1k."
"py)"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/densenet/"
"densenet121_4xb256_in1k_20220426-07450f99.pth)"
msgstr ""

#: ../../papers/densenet.md
msgid "DenseNet169\\*"
msgstr ""

#: ../../papers/densenet.md
msgid "14.15"
msgstr ""

#: ../../papers/densenet.md
msgid "3.42"
msgstr ""

#: ../../papers/densenet.md
msgid "76.08"
msgstr ""

#: ../../papers/densenet.md
msgid "93.11"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/densenet/densenet169_4xb256_in1k."
"py)"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/densenet/densenet169_4xb256_in1k_20220426-"
"a2889902.pth)"
msgstr ""

#: ../../papers/densenet.md
msgid "DenseNet201\\*"
msgstr ""

#: ../../papers/densenet.md
msgid "20.01"
msgstr ""

#: ../../papers/densenet.md
msgid "4.37"
msgstr ""

#: ../../papers/densenet.md
msgid "77.32"
msgstr ""

#: ../../papers/densenet.md
msgid "93.64"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/densenet/densenet201_4xb256_in1k."
"py)"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/densenet/"
"densenet201_4xb256_in1k_20220426-05cae4ef.pth)"
msgstr ""

#: ../../papers/densenet.md
msgid "DenseNet161\\*"
msgstr ""

#: ../../papers/densenet.md
msgid "28.68"
msgstr ""

#: ../../papers/densenet.md
msgid "7.82"
msgstr ""

#: ../../papers/densenet.md
msgid "77.61"
msgstr ""

#: ../../papers/densenet.md ../../papers/mobileone.md:86
msgid "93.83"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/densenet/densenet161_4xb256_in1k."
"py)"
msgstr ""

#: ../../papers/densenet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/densenet/densenet161_4xb256_in1k_20220426-"
"ee6a80a9.pth)"
msgstr ""

#: ../../papers/densenet.md:29
msgid ""
"*Models with * are converted from [pytorch](https://pytorch.org/vision/stable/models.html), guided by "
"[original repo](https://github.com/liuzhuang13/DenseNet). The config files of these models are only for "
"inference. We don't ensure these config files' training accuracy and welcome you to contribute your "
"reproduction results.*"
msgstr ""

#: ../../papers/edgenext.md:4
msgid "EdgeNeXt"
msgstr ""

#: ../../papers/edgenext.md:6
msgid ""
"[EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications](https://"
"arxiv.org/abs/2206.10589)"
msgstr ""

#: ../../papers/edgenext.md:14
#, python-format
msgid ""
"In the pursuit of achieving ever-increasing accuracy, large and complex neural networks are usually "
"developed. Such models demand high computational resources and therefore cannot be deployed on edge "
"devices. It is of great interest to build resource-efficient general purpose networks due to their "
"usefulness in several application areas. In this work, we strive to effectively combine the strengths of "
"both CNN and Transformer models and propose a new efficient hybrid architecture EdgeNeXt. Specifically in "
"EdgeNeXt, we introduce split depth-wise transpose attention (SDTA) encoder that splits input tensors into "
"multiple channel groups and utilizes depth-wise convolution along with self-attention across channel "
"dimensions to implicitly increase the receptive field and encode multi-scale features. Our extensive "
"experiments on classification, detection and segmentation tasks, reveal the merits of the proposed "
"approach, outperforming state-of-the-art methods with comparatively lower compute requirements. Our "
"EdgeNeXt model with 1.3M parameters achieves 71.2% top-1 accuracy on ImageNet-1K, outperforming MobileViT "
"with an absolute gain of 2.2% with 28% reduction in FLOPs. Further, our EdgeNeXt model with 5.6M parameters "
"achieves 79.4% top-1 accuracy on ImageNet-1K."
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-Base-usi\\*"
msgstr ""

#: ../../papers/edgenext.md
msgid "18.51"
msgstr ""

#: ../../papers/edgenext.md
msgid "3.84"
msgstr ""

#: ../../papers/edgenext.md
msgid "83.67"
msgstr ""

#: ../../papers/edgenext.md
msgid "96.7"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-base_8xb256-"
"usi_in1k.py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-base_3rdparty-"
"usi_in1k_20220801-909e8939.pth)"
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-Base\\*"
msgstr ""

#: ../../papers/edgenext.md
msgid "82.48"
msgstr ""

#: ../../papers/edgenext.md
msgid "96.2"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-base_8xb256_in1k."
"py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-"
"base_3rdparty_in1k_20220801-9ade408b.pth)"
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-Small-usi\\*"
msgstr ""

#: ../../papers/edgenext.md
msgid "5.59"
msgstr ""

#: ../../papers/edgenext.md
msgid "1.26"
msgstr ""

#: ../../papers/edgenext.md ../../papers/hrnet.md
msgid "81.06"
msgstr ""

#: ../../papers/edgenext.md ../../papers/efficientnet.md ../../papers/resnet.md:76
msgid "95.34"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-small_8xb256-"
"usi_in1k.py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-small_3rdparty-"
"usi_in1k_20220801-ae6d8dd3.pth)"
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-Small\\*"
msgstr ""

#: ../../papers/edgenext.md ../../papers/resnet.md:76
msgid "79.41"
msgstr ""

#: ../../papers/edgenext.md
msgid "94.53"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-"
"small_8xb256_in1k.py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-small_3rdparty_in1k_20220801-"
"d00db5f8.pth)"
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-X-Small\\*"
msgstr ""

#: ../../papers/edgenext.md
msgid "2.34"
msgstr ""

#: ../../papers/edgenext.md
msgid "0.538"
msgstr ""

#: ../../papers/edgenext.md
msgid "74.86"
msgstr ""

#: ../../papers/edgenext.md
msgid "92.31"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-"
"xsmall_8xb256_in1k.py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-"
"xsmall_3rdparty_in1k_20220801-974f9fe7.pth)"
msgstr ""

#: ../../papers/edgenext.md
msgid "EdgeNeXt-XX-Small\\*"
msgstr ""

#: ../../papers/edgenext.md
msgid "1.33"
msgstr ""

#: ../../papers/edgenext.md
msgid "0.261"
msgstr ""

#: ../../papers/edgenext.md
msgid "71.2"
msgstr ""

#: ../../papers/edgenext.md
msgid "89.91"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/edgenext/edgenext-"
"xxsmall_8xb256_in1k.py)"
msgstr ""

#: ../../papers/edgenext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/edgenext/edgenext-"
"xxsmall_3rdparty_in1k_20220801-7ca8a81d.pth)"
msgstr ""

#: ../../papers/edgenext.md:35
msgid ""
"*Models with * are converted from the [official repo](https://github.com/mmaaz60/EdgeNeXt). The config "
"files of these models are only for inference. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/efficientformer.md:4
msgid "EfficientFormer"
msgstr ""

#: ../../papers/efficientformer.md:6
msgid "[EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)"
msgstr ""

#: ../../papers/efficientformer.md:12
msgid ""
"Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results "
"on various benchmarks. However, due to the massive number of parameters and model design, e.g., attention "
"mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, "
"the deployment of ViT for real-time applications is particularly challenging, especially on resource-"
"constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT "
"through network architecture search or hybrid design with MobileNet block, yet the inference speed is still "
"unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while "
"obtaining high performance? To answer this, we first revisit the network architecture and operators used in "
"ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure "
"transformer (without MobileNet blocks) as a design paradigm.  Finally, we perform latency-driven slimming "
"to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of "
"EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves "
"79.2% top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on iPhone 12 (compiled with CoreML), "
"which runs as fast as MobileNetV2×1.4 (1.6 ms, 74.7% top-1), and our largest model, EfficientFormer-L7, "
"obtains 83.3% accuracy with only 7.0 ms latency. Our work proves that properly designed transformers can "
"reach extremely low latency on mobile devices while maintaining high performance."
msgstr ""

#: ../../papers/efficientformer.md
msgid "EfficientFormer-l1\\*"
msgstr ""

#: ../../papers/efficientformer.md
msgid "12.19"
msgstr ""

#: ../../papers/efficientformer.md
msgid "1.30"
msgstr ""

#: ../../papers/efficientformer.md
msgid "80.46"
msgstr ""

#: ../../papers/efficientformer.md
msgid "94.99"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientformer/efficientformer-"
"l1_8xb128_in1k.py)"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientformer/efficientformer-"
"l1_3rdparty_in1k_20220915-cc3e1ac6.pth)"
msgstr ""

#: ../../papers/efficientformer.md
msgid "EfficientFormer-l3\\*"
msgstr ""

#: ../../papers/efficientformer.md
msgid "31.41"
msgstr ""

#: ../../papers/efficientformer.md
msgid "3.93"
msgstr ""

#: ../../papers/efficientformer.md
msgid "82.45"
msgstr ""

#: ../../papers/efficientformer.md ../../papers/t2t_vit.md
msgid "96.18"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientformer/efficientformer-"
"l3_8xb128_in1k.py)"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientformer/efficientformer-"
"l3_3rdparty_in1k_20220915-466793d6.pth)"
msgstr ""

#: ../../papers/efficientformer.md
msgid "EfficientFormer-l7\\*"
msgstr ""

#: ../../papers/efficientformer.md
msgid "82.23"
msgstr ""

#: ../../papers/efficientformer.md
msgid "10.16"
msgstr ""

#: ../../papers/efficientformer.md
msgid "83.40"
msgstr ""

#: ../../papers/efficientformer.md
msgid "96.60"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientformer/efficientformer-"
"l7_8xb128_in1k.py)"
msgstr ""

#: ../../papers/efficientformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientformer/efficientformer-"
"l7_3rdparty_in1k_20220915-185e30af.pth)"
msgstr ""

#: ../../papers/efficientformer.md:28
msgid ""
"*Models with * are converted from the [official repo](https://github.com/snap-research/EfficientFormer). "
"The config files of these models are only for inference. We don't ensure these config files' training "
"accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/efficientnet.md:4
msgid "EfficientNet"
msgstr ""

#: ../../papers/efficientnet.md:6
msgid "[Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946v5)"
msgstr ""

#: ../../papers/efficientnet.md:12
msgid ""
"EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet "
"being an order-of-magnitude smaller and faster than previous models."
msgstr ""

#: ../../papers/efficientnet.md:14
msgid ""
"EfficientNets are based on AutoML and Compound Scaling. In particular, we first use [AutoML MNAS Mobile "
"framework](https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html) to develop a mobile-"
"size baseline network, named as EfficientNet-B0; Then, we use the compound scaling method to scale up this "
"baseline to obtain EfficientNet-B1 to B7."
msgstr ""

#: ../../papers/efficientnet.md:35
msgid ""
"In the result table, AA means trained with AutoAugment pre-processing, more details can be found in the "
"[paper](https://arxiv.org/abs/1805.09501); AdvProp is a method to train with adversarial examples, more "
"details can be found in the [paper](https://arxiv.org/abs/1911.09665); RA means trained with RandAugment "
"pre-processing, more details can be found in the [paper](https://arxiv.org/abs/1909.13719); NoisyStudent "
"means trained with extra JFT-300M unlabeled data, more details can be found in the [paper](https://arxiv."
"org/abs/1911.04252); L2-475 means the same L2 architecture with input image size 475."
msgstr ""

#: ../../papers/efficientnet.md:37
msgid "Note: In MMClassification, we support training with AutoAugment, don't support AdvProp by now."
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B0\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "5.29"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/mobilevit.md:71
msgid "0.42"
msgstr ""

#: ../../papers/efficientnet.md
msgid "76.74"
msgstr ""

#: ../../papers/efficientnet.md
msgid "93.17"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b0_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b0_3rdparty_8xb32_in1k_20220119-a7e2a0b1.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B0 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "77.26"
msgstr ""

#: ../../papers/efficientnet.md
msgid "93.41"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b0_3rdparty_8xb32-"
"aa_in1k_20220119-8d939117.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B0 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "77.53"
msgstr ""

#: ../../papers/efficientnet.md
msgid "93.61"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b0_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b0_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-26434485.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B0 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "77.63"
msgstr ""

#: ../../papers/efficientnet.md
msgid "94.00"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b0_3rdparty-ra-"
"noisystudent_in1k_20221103-75cd08d3.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B1\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "7.79"
msgstr ""

#: ../../papers/efficientnet.md
msgid "0.74"
msgstr ""

#: ../../papers/efficientnet.md
msgid "78.68"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "94.28"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b1_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b1_3rdparty_8xb32_in1k_20220119-002556d9.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B1 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/res2net.md
msgid "79.20"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/repvgg.md
msgid "94.42"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b1_3rdparty_8xb32-"
"aa_in1k_20220119-619d8ae3.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B1 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "79.52"
msgstr ""

#: ../../papers/efficientnet.md
msgid "94.43"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b1_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b1_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-5715267d.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B1 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "81.44"
msgstr ""

#: ../../papers/efficientnet.md
msgid "95.83"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b1_3rdparty-ra-"
"noisystudent_in1k_20221103-756bcbc0.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B2\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "9.11"
msgstr ""

#: ../../papers/efficientnet.md
msgid "1.07"
msgstr ""

#: ../../papers/efficientnet.md
msgid "79.64"
msgstr ""

#: ../../papers/efficientnet.md
msgid "94.80"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b2_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b2_3rdparty_8xb32_in1k_20220119-ea374a30.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B2 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "80.21"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b2_3rdparty_8xb32-"
"aa_in1k_20220119-dd61e80b.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B2 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "80.45"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b2_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b2_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-1655338a.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B2 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "82.47"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/swin_transformer_v2.md:76
msgid "96.23"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B3\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "12.23"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/van.md
msgid "81.01"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b3_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b3_3rdparty_8xb32_in1k_20220119-4b4d7487.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B3 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "81.58"
msgstr ""

#: ../../papers/efficientnet.md
msgid "95.67"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b3_3rdparty_8xb32-"
"aa_in1k_20220119-5b4887a0.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B3 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/repvgg.md
msgid "81.81"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/twins.md
msgid "95.69"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b3_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b3_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-53b41118.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B3 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "84.02"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b3_3rdparty-ra-"
"noisystudent_in1k_20221103-a4ab5fd6.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B4\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "19.34"
msgstr ""

#: ../../papers/efficientnet.md
msgid "1.95"
msgstr ""

#: ../../papers/efficientnet.md
msgid "82.57"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/t2t_vit.md
msgid "96.09"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b4_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b4_3rdparty_8xb32_in1k_20220119-81fd4077.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B4 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "82.95"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/twins.md
msgid "96.26"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b4_3rdparty_8xb32-"
"aa_in1k_20220119-45b8bd2b.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B4 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "83.25"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b4_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b4_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-38c2238c.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B4 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/mvit.md
msgid "85.25"
msgstr ""

#: ../../papers/efficientnet.md
msgid "97.52"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b4_3rdparty-ra-"
"noisystudent_in1k_20221103-16ba8a2d.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B5\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "30.39"
msgstr ""

#: ../../papers/efficientnet.md
msgid "10.1"
msgstr ""

#: ../../papers/efficientnet.md
msgid "83.18"
msgstr ""

#: ../../papers/efficientnet.md
msgid "96.47"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b5_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-"
"b5_3rdparty_8xb32_in1k_20220119-e9814430.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B5 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "96.76"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b5_3rdparty_8xb32-"
"aa_in1k_20220119-2cab8b78.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B5 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "84.21"
msgstr ""

#: ../../papers/efficientnet.md
msgid "96.98"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b5_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b5_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-f57a895a.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B5 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "86.08"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b5_3rdparty-ra-"
"noisystudent_in1k_20221103-111a185f.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B6 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "43.04"
msgstr ""

#: ../../papers/efficientnet.md
msgid "20.0"
msgstr ""

#: ../../papers/efficientnet.md
msgid "84.05"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b6_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b6_3rdparty_8xb32-"
"aa_in1k_20220119-45b03310.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B6 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "84.74"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/mvit.md
msgid "97.14"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b6_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b6_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-bfe3485e.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B6 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "86.47"
msgstr ""

#: ../../papers/efficientnet.md
msgid "97.87"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b6_3rdparty-ra-"
"noisystudent_in1k_20221103-7de7d2cc.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B7 (AA)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "66.35"
msgstr ""

#: ../../papers/efficientnet.md
msgid "39.3"
msgstr ""

#: ../../papers/efficientnet.md
msgid "84.38"
msgstr ""

#: ../../papers/efficientnet.md
msgid "96.88"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b7_8xb32_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b7_3rdparty_8xb32-"
"aa_in1k_20220119-bf03951c.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B7 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "85.14"
msgstr ""

#: ../../papers/efficientnet.md
msgid "97.23"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b7_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b7_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-c6dbff10.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B7 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "65.0"
msgstr ""

#: ../../papers/efficientnet.md
msgid "86.83"
msgstr ""

#: ../../papers/efficientnet.md
msgid "98.08"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b7_3rdparty-ra-"
"noisystudent_in1k_20221103-a82894bc.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-B8 (AA + AdvProp)\\*"
msgstr ""

#: ../../papers/efficientnet.md ../../papers/mobilenet_v3.md
msgid "87.41"
msgstr ""

#: ../../papers/efficientnet.md
msgid "85.38"
msgstr ""

#: ../../papers/efficientnet.md
msgid "97.28"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"b8_8xb32-01norm_in1k.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-b8_3rdparty_8xb32-aa-"
"advprop_in1k_20220119-297ce1b7.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-L2-475 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "480.30"
msgstr ""

#: ../../papers/efficientnet.md
msgid "174.20"
msgstr ""

#: ../../papers/efficientnet.md
msgid "88.18"
msgstr ""

#: ../../papers/efficientnet.md
msgid "98.55"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"l2_8xb32_in1k-475px.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-l2_3rdparty-ra-"
"noisystudent_in1k-475px_20221103-5a0d8058.pth)"
msgstr ""

#: ../../papers/efficientnet.md
msgid "EfficientNet-L2 (RA + NoisyStudent)\\*"
msgstr ""

#: ../../papers/efficientnet.md
msgid "484.98"
msgstr ""

#: ../../papers/efficientnet.md
msgid "88.33"
msgstr ""

#: ../../papers/efficientnet.md
msgid "98.65"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/efficientnet/efficientnet-"
"l2_8xb8_in1k-800px.py)"
msgstr ""

#: ../../papers/efficientnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/efficientnet/efficientnet-l2_3rdparty-ra-"
"noisystudent_in1k_20221103-be73be13.pth)"
msgstr ""

#: ../../papers/efficientnet.md:75
msgid ""
"*Models with * are converted from the [official repo](https://github.com/tensorflow/tpu/tree/master/models/"
"official/efficientnet). The config files of these models are only for inference. We don't ensure these "
"config files' training accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/efficientnet.md:121 ../../papers/mobilevit.md:80 ../../papers/resnet.md:85
#: ../../papers/swin_transformer.md:75 ../../papers/swin_transformer_v2.md:85
#: ../../papers/vision_transformer.md:80
msgid "Test:"
msgstr "测试："

#: ../../papers/efficientnet.md:128
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.EfficientNet.html#mmcls.models.backbones.EfficientNet)."
msgstr ""

#: ../../papers/hornet.md:4
msgid "HorNet"
msgstr ""

#: ../../papers/hornet.md:6
msgid ""
"[HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions](https://arxiv.org/"
"pdf/2207.14284v2.pdf)"
msgstr ""

#: ../../papers/hornet.md:12
msgid ""
"Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial "
"modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients "
"behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can "
"also be efficiently implemented with a convolution-based framework. We present the Recursive Gated "
"Convolution (g nConv) that performs high-order spatial interactions with gated convolutions and recursive "
"designs. The new operation is highly flexible and customizable, which is compatible with various variants "
"of convolution and extends the two-order interactions in self-attention to arbitrary orders without "
"introducing significant extra computation. g nConv can serve as a plug-and-play module to improve various "
"vision Transformers and convolution-based models. Based on the operation, we construct a new family of "
"generic vision backbones named HorNet. Extensive experiments on ImageNet classification, COCO object "
"detection and ADE20K semantic segmentation show HorNet outperform Swin Transformers and ConvNeXt by a "
"significant margin with similar overall architecture and training configurations. HorNet also shows "
"favorable scalability to more training data and a larger model size. Apart from the effectiveness in visual "
"encoders, we also show g nConv can be applied to task-specific decoders and consistently improve dense "
"prediction performance with less computation. Our results demonstrate that g nConv can be a new basic "
"module for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code "
"is available at https://github.com/raoyongming/HorNet."
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-T\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "22.41"
msgstr ""

#: ../../papers/hornet.md
msgid "3.98"
msgstr ""

#: ../../papers/hornet.md
msgid "82.84"
msgstr ""

#: ../../papers/hornet.md
msgid "96.24"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-tiny_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-"
"tiny_3rdparty_in1k_20220915-0e8eedff.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-T-GF\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "22.99"
msgstr ""

#: ../../papers/hornet.md
msgid "3.9"
msgstr ""

#: ../../papers/hornet.md
msgid "82.98"
msgstr ""

#: ../../papers/hornet.md
msgid "96.38"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-tiny-"
"gf_8xb128_in1k.py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-tiny-"
"gf_3rdparty_in1k_20220915-4c35a66b.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-S\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "49.53"
msgstr ""

#: ../../papers/hornet.md
msgid "8.83"
msgstr ""

#: ../../papers/hornet.md
msgid "83.79"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-small_8xb64_in1k."
"py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-"
"small_3rdparty_in1k_20220915-5935f60f.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-S-GF\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "50.4"
msgstr ""

#: ../../papers/hornet.md
msgid "8.71"
msgstr ""

#: ../../papers/hornet.md
msgid "83.98"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-small-"
"gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-small-"
"gf_3rdparty_in1k_20220915-649ca492.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-B\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "87.26"
msgstr ""

#: ../../papers/hornet.md
msgid "15.59"
msgstr ""

#: ../../papers/hornet.md
msgid "84.24"
msgstr ""

#: ../../papers/hornet.md
msgid "96.94"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-base_8xb64_in1k."
"py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-base_3rdparty_in1k_20220915-"
"a06176bb.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-B-GF\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "88.42"
msgstr ""

#: ../../papers/hornet.md
msgid "15.42"
msgstr ""

#: ../../papers/hornet.md
msgid "84.32"
msgstr ""

#: ../../papers/hornet.md ../../papers/swin_transformer.md:66
msgid "96.95"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/master/configs/hornet/hornet-base-"
"gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-base-"
"gf_3rdparty_in1k_20220915-82c06fa7.pth)"
msgstr ""

#: ../../papers/hornet.md:31
msgid ""
"\\*Models with * are converted from [the official repo](https://github.com/raoyongming/HorNet). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results."
msgstr ""

#: ../../papers/hornet.md:35
msgid "The pre-trained models on ImageNet-21k are used to fine-tune on the downstream tasks."
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-L\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "194.54"
msgstr ""

#: ../../papers/hornet.md
msgid "34.83"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-"
"large_3rdparty_in21k_20220909-9ccef421.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-L-GF\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "196.29"
msgstr ""

#: ../../papers/hornet.md
msgid "34.58"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-large-"
"gf_3rdparty_in21k_20220909-3aea3b61.pth)"
msgstr ""

#: ../../papers/hornet.md
msgid "HorNet-L-GF384\\*"
msgstr ""

#: ../../papers/hornet.md
msgid "201.23"
msgstr ""

#: ../../papers/hornet.md
msgid "101.63"
msgstr ""

#: ../../papers/hornet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hornet/hornet-large-"
"gf384_3rdparty_in21k_20220909-80894290.pth)"
msgstr ""

#: ../../papers/hornet.md:43
msgid "\\*Models with * are converted from [the official repo](https://github.com/raoyongming/HorNet)."
msgstr ""

#: ../../papers/hrnet.md:4
msgid "HRNet"
msgstr ""

#: ../../papers/hrnet.md:6
msgid ""
"[Deep High-Resolution Representation Learning for Visual Recognition](https://arxiv.org/abs/1908.07919v2)"
msgstr ""

#: ../../papers/hrnet.md:12
msgid ""
"High-resolution representations are essential for position-sensitive vision problems, such as human pose "
"estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode "
"the input image as a low-resolution representation through a subnetwork that is formed by connecting high-"
"to-low resolution convolutions *in series* (e.g., ResNet, VGGNet), and then recover the high-resolution "
"representation from the encoded low-resolution representation. Instead, our proposed network, named as High-"
"Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are "
"two key characteristics: (i) Connect the high-to-low resolution convolution streams *in parallel*; (ii) "
"Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is "
"semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide "
"range of applications, including human pose estimation, semantic segmentation, and object detection, "
"suggesting that the HRNet is a stronger backbone for computer vision problems."
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W18\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "21.30"
msgstr ""

#: ../../papers/hrnet.md
msgid "4.33"
msgstr ""

#: ../../papers/hrnet.md
msgid "76.75"
msgstr ""

#: ../../papers/hrnet.md
msgid "93.44"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w18_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-"
"w18_3rdparty_8xb32_in1k_20220120-0c10b180.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W30\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "37.71"
msgstr ""

#: ../../papers/hrnet.md
msgid "8.17"
msgstr ""

#: ../../papers/hrnet.md
msgid "78.19"
msgstr ""

#: ../../papers/hrnet.md ../../papers/regnet.md
msgid "94.22"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w30_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-"
"w30_3rdparty_8xb32_in1k_20220120-8aa3832f.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W32\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "41.23"
msgstr ""

#: ../../papers/hrnet.md ../../papers/van.md
msgid "8.99"
msgstr ""

#: ../../papers/hrnet.md
msgid "78.44"
msgstr ""

#: ../../papers/hrnet.md
msgid "94.19"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w32_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-w32_3rdparty_8xb32_in1k_20220120-"
"c394f1ab.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W40\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "57.55"
msgstr ""

#: ../../papers/hrnet.md
msgid "12.77"
msgstr ""

#: ../../papers/hrnet.md
msgid "78.94"
msgstr ""

#: ../../papers/hrnet.md
msgid "94.47"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w40_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-"
"w40_3rdparty_8xb32_in1k_20220120-9a2dbfc5.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W44\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "67.06"
msgstr ""

#: ../../papers/hrnet.md
msgid "14.96"
msgstr ""

#: ../../papers/hrnet.md ../../papers/resnext.md
msgid "78.88"
msgstr ""

#: ../../papers/hrnet.md ../../papers/resnet.md:76
msgid "94.37"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w44_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-"
"w44_3rdparty_8xb32_in1k_20220120-35d07f73.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W48\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "77.47"
msgstr ""

#: ../../papers/hrnet.md
msgid "17.36"
msgstr ""

#: ../../papers/hrnet.md
msgid "79.32"
msgstr ""

#: ../../papers/hrnet.md
msgid "94.52"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w48_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-w48_3rdparty_8xb32_in1k_20220120-"
"e555ef50.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W64\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "128.06"
msgstr ""

#: ../../papers/hrnet.md
msgid "29.00"
msgstr ""

#: ../../papers/hrnet.md
msgid "79.46"
msgstr ""

#: ../../papers/hrnet.md ../../papers/regnet.md
msgid "94.65"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/hrnet/hrnet-w64_4xb32_in1k.py)"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-"
"w64_3rdparty_8xb32_in1k_20220120-19126642.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W18 (ssld)\\*"
msgstr ""

#: ../../papers/hrnet.md
msgid "95.70"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-w18_3rdparty_8xb32-"
"ssld_in1k_20220120-455f69ea.pth)"
msgstr ""

#: ../../papers/hrnet.md
msgid "HRNet-W48 (ssld)\\*"
msgstr ""

#: ../../papers/hrnet.md ../../papers/mvit.md
msgid "83.63"
msgstr ""

#: ../../papers/hrnet.md
msgid "96.79"
msgstr ""

#: ../../papers/hrnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/hrnet/hrnet-w48_3rdparty_8xb32-"
"ssld_in1k_20220120-d0459c38.pth)"
msgstr ""

#: ../../papers/hrnet.md:34
msgid ""
"*Models with * are converted from the [official repo](https://github.com/HRNet/HRNet-Image-Classification). "
"The config files of these models are only for inference. We don't ensure these config files' training "
"accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/inception_v3.md:4
msgid "Inception V3"
msgstr ""

#: ../../papers/inception_v3.md:6
msgid "[Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)"
msgstr ""

#: ../../papers/inception_v3.md:12
#, python-format
msgid ""
"Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide "
"variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding "
"substantial gains in various benchmarks. Although increased model size and computational cost tend to "
"translate to immediate quality gains for most tasks (as long as enough labeled data is provided for "
"training), computational efficiency and low parameter count are still enabling factors for various use "
"cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that "
"aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and "
"aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation "
"set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single "
"frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and "
"with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we "
"report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the "
"validation set."
msgstr ""

#: ../../papers/inception_v3.md
msgid "Inception V3\\*"
msgstr ""

#: ../../papers/inception_v3.md
msgid "23.83"
msgstr ""

#: ../../papers/inception_v3.md
msgid "5.75"
msgstr ""

#: ../../papers/inception_v3.md
msgid "77.57"
msgstr ""

#: ../../papers/inception_v3.md ../../papers/resnet.md:76
msgid "93.58"
msgstr ""

#: ../../papers/inception_v3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/inception_v3/inception-"
"v3_8xb32_in1k.py)"
msgstr ""

#: ../../papers/inception_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/inception-v3/inception-"
"v3_3rdparty_8xb32_in1k_20220615-dcd4d910.pth)"
msgstr ""

#: ../../papers/inception_v3.md:26
msgid ""
"*Models with * are converted from the [official repo](https://github.com/pytorch/vision/blob/main/"
"torchvision/models/inception.py#L28). The config files of these models are only for inference. We don't "
"ensure these config files' training accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/mlp_mixer.md:4
msgid "Mlp-Mixer"
msgstr ""

#: ../../papers/mlp_mixer.md:6
msgid "[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)"
msgstr ""

#: ../../papers/mlp_mixer.md:12
msgid ""
"Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based "
"networks, such as the Vision Transformer, have also become popular. In this paper we show that while "
"convolutions and attention are both sufficient for good performance, neither of them are necessary. We "
"present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains "
"two types of layers: one with MLPs applied independently to image patches (i.e. \"mixing\" the per-location "
"features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on "
"large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image "
"classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We "
"hope that these results spark further research beyond the realms of well established CNNs and Transformers."
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "Mixer-B/16\\*"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "59.88"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "12.61"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "76.68"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "92.25"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mlp_mixer/mlp-mixer-base-"
"p16_64xb64_in1k.py)"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mlp-mixer/mixer-base-"
"p16_3rdparty_64xb64_in1k_20211124-1377e3e0.pth)"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "Mixer-L/16\\*"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "208.2"
msgstr ""

#: ../../papers/mlp_mixer.md ../../papers/resnet.md:76
msgid "44.57"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "72.34"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid "88.02"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mlp_mixer/mlp-mixer-large-"
"p16_64xb64_in1k.py)"
msgstr ""

#: ../../papers/mlp_mixer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mlp-mixer/mixer-large-"
"p16_3rdparty_64xb64_in1k_20211124-5a2519d2.pth)"
msgstr ""

#: ../../papers/mlp_mixer.md:27
msgid ""
"*Models with * are converted from [timm](https://github.com/rwightman/pytorch-image-models/blob/master/timm/"
"models/mlp_mixer.py). The config files of these models are only for validation. We don't ensure these "
"config files' training accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/mobilenet_v2.md ../../papers/mobilenet_v2.md:4
msgid "MobileNet V2"
msgstr ""

#: ../../papers/mobilenet_v2.md:6
msgid "[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)"
msgstr ""

#: ../../papers/mobilenet_v2.md:12
msgid ""
"In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art "
"performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different "
"model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel "
"framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models "
"through a reduced form of DeepLabv3 which we call Mobile DeepLabv3."
msgstr ""

#: ../../papers/mobilenet_v2.md:14
msgid ""
"The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the "
"residual block are thin bottleneck layers opposite to traditional residual models which use expanded "
"representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in "
"the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in "
"the narrow layers in order to maintain representational power. We demonstrate that this improves "
"performance and provide an intuition that led to this design. Finally, our approach allows decoupling of "
"the input/output domains from the expressiveness of the transformation, which provides a convenient "
"framework for further analysis. We measure our performance on Imagenet classification, COCO object "
"detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations "
"measured by multiply-adds (MAdd), as well as the number of parameters"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid "3.5"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid "0.319"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid "71.86"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid "90.42"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilenet_v2/mobilenet-"
"v2_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobilenet_v2.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v2/"
"mobilenet_v2_batch256_imagenet_20200708-3b2dc3af.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/mobilenet_v2/mobilenet_v2_batch256_imagenet_20200708-3b2dc3af.log.json)"
msgstr ""

#: ../../papers/mobilenet_v3.md:4
msgid "MobileNet V3"
msgstr ""

#: ../../papers/mobilenet_v3.md:6
msgid "[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)"
msgstr ""

#: ../../papers/mobilenet_v3.md:12
#, python-format
msgid ""
"We present the next generation of MobileNets based on a combination of complementary search techniques as "
"well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of "
"hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then "
"subsequently improved through novel architecture advances. This paper starts the exploration of how "
"automated search algorithms and network design can work together to harness complementary approaches "
"improving the overall state of the art. Through this process we create two new MobileNet models for "
"release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. "
"These models are then adapted and applied to the tasks of object detection and semantic segmentation. For "
"the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation "
"decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for "
"mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet "
"classification while reducing latency by 15% compared to MobileNetV2. MobileNetV3-Small is 4.6% more "
"accurate while reducing latency by 5% compared to MobileNetV2. MobileNetV3-Large detection is 25% faster at "
"roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30% faster than "
"MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation."
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Small-050\\*\\*"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "1.59"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "0.02"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "57.91"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "80.19"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilenet_v3/mobilenet-v3-"
"small-050_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/mobilenet-v3-"
"small-050_3rdparty_in1k_20221114-e0b86be1.pth)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Small-075\\*\\*"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "2.04"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "0.04"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "65.23"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "85.44"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilenet_v3/mobilenet-v3-"
"small-075_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/mobilenet-v3-"
"small-075_3rdparty_in1k_20221114-2011fa76.pth)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Small"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "2.54"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "0.06"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "66.68"
msgstr ""

#: ../../papers/mobilenet_v3.md ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66
msgid "86.74"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilenet_v3/mobilenet-v3-"
"small_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/mobilenet-v3-"
"small_8xb128_in1k_20221114-bd1bfcde.pth) | [log](https://download.openmmlab.com/mmclassification/v0/"
"mobilenet_v3/mobilenet-v3-small_8xb128_in1k_20221114-bd1bfcde.log.json)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Small\\*"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "67.66"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/convert/mobilenet_v3_small-8427ecf0."
"pth)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Large"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "5.48"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "0.23"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "73.49"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "91.31"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilenet_v3/mobilenet-v3-"
"large_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/mobilenet-v3-"
"large_8xb128_in1k_20221114-0ed9ed9a.pth) | [log](https://download.openmmlab.com/mmclassification/v0/"
"mobilenet_v3/mobilenet-v3-large_8xb128_in1k_20221114-0ed9ed9a.log.json)"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "MobileNetV3-Large\\*"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "74.04"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid "91.34"
msgstr ""

#: ../../papers/mobilenet_v3.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/convert/mobilenet_v3_large-3ea3c186."
"pth)"
msgstr ""

#: ../../papers/mobilenet_v3.md:31
#, python-format
msgid ""
"We cannot reproduce the performances provided by TorchVision with the [training script](https://github.com/"
"pytorch/vision/tree/master/references/classification#mobilenetv3-large--small), and the accuracy results we "
"got are 65.5±0.5% and 73.39% for small and large model respectively. Here we provide checkpoints trained by "
"MMClassification that outperform the aforementioned results, and the original checkpoints provided by "
"TorchVision."
msgstr ""

#: ../../papers/mobilenet_v3.md:33
msgid ""
"*Models with * are converted from [torchvision](https://pytorch.org/vision/stable/_modules/torchvision/"
"models/mobilenetv3.html). Models with \\*\\* are converted from [timm](https://github.com/rwightman/pytorch-"
"image-models/blob/main/timm/models/mobilenetv3.py). The config files of these models are only for "
"validation. We don't ensure these config files' training accuracy and welcome you to contribute your "
"reproduction results.*"
msgstr ""

#: ../../papers/mobileone.md:4
msgid "MobileOne"
msgstr ""

#: ../../papers/mobileone.md:6
msgid "[An Improved One millisecond Mobile Backbone](https://arxiv.org/abs/2206.04040)"
msgstr ""

#: ../../papers/mobileone.md:12
msgid ""
"Mobileone is proposed by apple and based on reparameterization. On the apple chips, the accuracy of the "
"model is close to 0.76 on the ImageNet dataset when the latency is less than 1ms. Its main improvements "
"based on [RepVGG](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/../repvgg) are "
"fllowing:"
msgstr ""

#: ../../papers/mobileone.md:14
msgid ""
"Reparameterization using Depthwise convolution and Pointwise convolution instead of normal convolution."
msgstr ""

#: ../../papers/mobileone.md:15
msgid "Removal of the residual structure which is not friendly to access memory."
msgstr ""

#: ../../papers/mobileone.md:33 ../../papers/replknet.md:33 ../../papers/repmlp.md:29
#: ../../papers/repvgg.md:39
msgid "How to use"
msgstr ""

#: ../../papers/mobileone.md:35
msgid ""
"The checkpoints provided are all `training-time` models. Use the reparameterize tool or `switch_to_deploy` "
"interface to switch them to more efficient `inference-time` architecture, which not only has fewer "
"parameters but also less calculations."
msgstr ""

#: ../../papers/mobileone.md:40
msgid "Use `classifier.backbone.switch_to_deploy()` interface to switch the MobileOne to a inference mode."
msgstr ""

#: ../../papers/mobileone.md:95
msgid "Download Checkpoint:"
msgstr ""

#: ../../papers/mobileone.md:101
msgid "Test use unfused model:"
msgstr ""

#: ../../papers/mobileone.md:107
msgid "Reparameterize checkpoint:"
msgstr ""

#: ../../papers/mobileone.md:113
msgid "Test use fused model:"
msgstr ""

#: ../../papers/mobileone.md:120
msgid "Reparameterize Tool"
msgstr ""

#: ../../papers/mobileone.md:122 ../../papers/replknet.md:39 ../../papers/repmlp.md:35
#: ../../papers/repvgg.md:45
msgid "Use provided tool to reparameterize the given model and save the checkpoint:"
msgstr ""

#: ../../papers/mobileone.md:128
msgid ""
"`${CFG_PATH}` is the config file path, `${SRC_CKPT_PATH}` is the source chenpoint file path, `"
"${TARGET_CKPT_PATH}` is the target deploy weight file path."
msgstr ""

#: ../../papers/mobileone.md:130
msgid "For example:"
msgstr ""

#: ../../papers/mobileone.md:137
msgid ""
"To use reparameterized weights, the config file must switch to [**the deploy config files**](https://github."
"com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/deploy)."
msgstr ""

#: ../../papers/mobileone.md:143
msgid "For example of using the reparameterized weights above:"
msgstr ""

#: ../../papers/mobileone.md:149
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.MobileOne.html#mmcls.models.backbones.MobileOne)."
msgstr ""

#: ../../papers/mobileone.md:86
msgid "MobileOne-s0"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "5.29（train) | 2.08 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "1.09 (train) | 0.28 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "71.34"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "89.87"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/mobileone-"
"s0_8xb32_in1k.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"mobileone/deploy/mobileone-s0_deploy_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-"
"s0_8xb32_in1k_20221110-0bc94952.pth) | [log](https://download.openmmlab.com/mmclassification/v0/mobileone/"
"mobileone-s0_8xb32_in1k_20221110-0bc94952.json)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "MobileOne-s1"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "4.83 (train) | 4.76 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "0.86 (train) | 0.84 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "75.72"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "92.54"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/mobileone-"
"s1_8xb32_in1k.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"mobileone/deploy/mobileone-s1_deploy_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-s1_8xb32_in1k_20221110-"
"ceeef467.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-"
"s1_8xb32_in1k_20221110-ceeef467.json)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "MobileOne-s2"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "7.88 (train) | 7.88 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "1.34 (train)  | 1.31 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "77.37"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "93.34"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/mobileone-"
"s2_8xb32_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"mobileone/deploy/mobileone-s2_deploy_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-"
"s2_8xb32_in1k_20221110-9c7ecb97.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/mobileone/"
"mobileone-s2_8xb32_in1k_20221110-9c7ecb97.json)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "MobileOne-s3"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "10.17 (train) | 10.08 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "1.95 (train)  | 1.91 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "78.06"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/mobileone-"
"s3_8xb32_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"mobileone/deploy/mobileone-s3_deploy_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-s3_8xb32_in1k_20221110-"
"c95eb3bf.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-"
"s3_8xb32_in1k_20221110-c95eb3bf.pth)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "MobileOne-s4"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "14.95 (train) | 14.84 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "3.05 (train) | 3.00 (deploy)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "79.69"
msgstr ""

#: ../../papers/mobileone.md:86
msgid "94.46"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobileone/mobileone-"
"s4_8xb32_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"mobileone/deploy/mobileone-s4_deploy_8xb32_in1k.py)"
msgstr ""

#: ../../papers/mobileone.md:86
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobileone/mobileone-"
"s4_8xb32_in1k_20221110-28d888cb.pth) | [log](https://download.openmmlab.com/mmclassification/v0/mobileone/"
"mobileone-s4_8xb32_in1k_20221110-28d888cb.pth)"
msgstr ""

#: ../../papers/mobilevit.md:4
msgid "MobileVit"
msgstr ""

#: ../../papers/mobilevit.md:6
msgid ""
"[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/"
"abs/2110.02178)"
msgstr ""

#: ../../papers/mobilevit.md:12
msgid ""
"**MobileViT** aims at introducing a light-weight network, which takes the advantages of both ViTs and CNNs, "
"uses the `InvertedResidual` blocks in [MobileNetV2](https://github.com/open-mmlab/mmclassification/blob/1.x/"
"configs/mobilevit/../mobilenet_v2/README.md) and `MobileViTBlock` which refers to [ViT](https://github.com/"
"open-mmlab/mmclassification/blob/1.x/configs/mobilevit/../vision_transformer/README.md) transformer blocks "
"to build a standard 5-stage model structure."
msgstr ""

#: ../../papers/mobilevit.md:14
msgid ""
"The MobileViTBlock reckons transformers as convolutions to perform a global representation, meanwhile "
"conbined with original convolution layers for local representation to build a block with global receptive "
"field. This is different from ViT, which adds an extra class token and position embeddings for learning "
"relative relationship. Without any position embeddings, MobileViT can benfit from multi-scale inputs during "
"training."
msgstr ""

#: ../../papers/mobilevit.md:16
msgid ""
"Also, this paper puts forward a strategy for multi-scale training to dynamically adjust batch size based on "
"the image size to both improve training efficiency and final performance."
msgstr ""

#: ../../papers/mobilevit.md:18
msgid "It is also proven effective in downstream tasks such as object detection and segmentation."
msgstr ""

#: ../../papers/mobilevit.md:32
msgid ""
"Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial "
"inductive biases allow them to learn representations with fewer parameters across different vision tasks. "
"However, these networks are spatially local. To learn global representations, self-attention-based vision "
"trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the "
"following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and "
"low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and "
"general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the "
"global processing of information with transformers, i.e., transformers as convolutions. Our results show "
"that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. "
"On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, "
"which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number "
"of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a "
"similar number of parameters. </br>"
msgstr ""

#: ../../papers/mobilevit.md:87
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.MobileViT.html#mmcls.models.backbones.MobileViT)."
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "MobileViT-XXSmall\\*"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "1.27"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "69.02"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "88.91"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilevit/mobilevit-"
"xxsmall_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilevit/mobilevit-"
"xxsmall_3rdparty_in1k_20221018-77835605.pth)"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "MobileViT-XSmall\\*"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "2.32"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "1.05"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "74.75"
msgstr ""

#: ../../papers/mobilevit.md:71 ../../papers/regnet.md
msgid "92.32"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilevit/mobilevit-"
"xsmall_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilevit/mobilevit-"
"xsmall_3rdparty_in1k_20221018-be39a6e7.pth)"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "MobileViT-Small\\*"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "5.58"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "2.03"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "78.25"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid "94.09"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mobilevit/mobilevit-"
"small_8xb128_in1k.py)"
msgstr ""

#: ../../papers/mobilevit.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mobilevit/mobilevit-small_3rdparty_in1k_20221018-"
"cb4f741c.pth)"
msgstr ""

#: ../../papers/mobilevit.md:99
msgid ""
"*Models with * are converted from [ml-cvnets](https://github.com/apple/ml-cvnets). The config files of "
"these models are only for validation. We don't ensure these config files' training accuracy and welcome you "
"to contribute your reproduction results.*"
msgstr ""

#: ../../papers/mvit.md:4
msgid "MViT V2"
msgstr ""

#: ../../papers/mvit.md:6
msgid ""
"[MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](http://openaccess.thecvf."
"com//content/CVPR2022/papers/"
"Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf)"
msgstr ""

#: ../../papers/mvit.md:12
#, python-format
msgid ""
"In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and "
"video classification, as well as object detection. We present an improved version of MViT that incorporates "
"decomposed relative positional embeddings and residual pooling connections. We instantiate this "
"architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video "
"recognition where it outperforms prior work. We further compare MViTv2s' pooling attention to window "
"attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, "
"MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 58.7 boxAP "
"on COCO object detection as well as 86.1% on Kinetics-400 video classification."
msgstr ""

#: ../../papers/mvit.md
msgid "MViTv2-tiny\\*"
msgstr ""

#: ../../papers/mvit.md
msgid "24.17"
msgstr ""

#: ../../papers/mvit.md
msgid "4.70"
msgstr ""

#: ../../papers/mvit.md
msgid "82.33"
msgstr ""

#: ../../papers/mvit.md ../../papers/vision_transformer.md:71
msgid "96.15"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mvit/mvitv2-tiny_8xb256_in1k.py)"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mvit/mvitv2-tiny_3rdparty_in1k_20220722-db7beeef."
"pth)"
msgstr ""

#: ../../papers/mvit.md
msgid "MViTv2-small\\*"
msgstr ""

#: ../../papers/mvit.md
msgid "34.87"
msgstr ""

#: ../../papers/mvit.md
msgid "7.00"
msgstr ""

#: ../../papers/mvit.md
msgid "96.51"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mvit/mvitv2-small_8xb256_in1k.py)"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mvit/mvitv2-"
"small_3rdparty_in1k_20220722-986bd741.pth)"
msgstr ""

#: ../../papers/mvit.md
msgid "MViTv2-base\\*"
msgstr ""

#: ../../papers/mvit.md
msgid "51.47"
msgstr ""

#: ../../papers/mvit.md
msgid "10.20"
msgstr ""

#: ../../papers/mvit.md
msgid "84.34"
msgstr ""

#: ../../papers/mvit.md ../../papers/swin_transformer_v2.md:76
msgid "96.86"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mvit/mvitv2-base_8xb256_in1k.py)"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mvit/mvitv2-base_3rdparty_in1k_20220722-9c4f0a17."
"pth)"
msgstr ""

#: ../../papers/mvit.md
msgid "MViTv2-large\\*"
msgstr ""

#: ../../papers/mvit.md
msgid "217.99"
msgstr ""

#: ../../papers/mvit.md
msgid "42.10"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/mvit/mvitv2-large_8xb256_in1k.py)"
msgstr ""

#: ../../papers/mvit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/mvit/mvitv2-"
"large_3rdparty_in1k_20220722-2b57b983.pth)"
msgstr ""

#: ../../papers/mvit.md:36
msgid ""
"*Models with * are converted from the [official repo](https://github.com/facebookresearch/mvit). The config "
"files of these models are only for inference. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/poolformer.md:4
msgid "PoolFormer"
msgstr ""

#: ../../papers/poolformer.md:6
msgid "[MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)"
msgstr ""

#: ../../papers/poolformer.md:12
#, python-format
msgid ""
"Transformers have shown great potential in computer vision tasks. A common belief is their attention-based "
"token mixer module contributes most to their competence. However, recent works show the attention-based "
"module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. "
"Based on this observation, we hypothesize that the general architecture of the transformers, instead of the "
"specific token mixer module, is more essential to the model's performance. To verify this, we deliberately "
"replace the attention module in transformers with an embarrassingly simple spatial pooling operator to "
"conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, "
"achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer "
"achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-"
"B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of "
"PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general "
"architecture abstracted from transformers without specifying the token mixer. Based on the extensive "
"experiments, we argue that MetaFormer is the key player in achieving superior results for recent "
"transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to "
"improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer "
"could serve as a starting baseline for future MetaFormer architecture design."
msgstr ""

#: ../../papers/poolformer.md
msgid "PoolFormer-S12\\*"
msgstr ""

#: ../../papers/poolformer.md
msgid "11.92"
msgstr ""

#: ../../papers/poolformer.md ../../papers/shufflenet_v1.md
msgid "1.87"
msgstr ""

#: ../../papers/poolformer.md
msgid "77.24"
msgstr ""

#: ../../papers/poolformer.md ../../papers/regnet.md
msgid "93.51"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/poolformer/poolformer-"
"s12_32xb128_in1k.py)"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/poolformer/poolformer-"
"s12_3rdparty_32xb128_in1k_20220414-f8d83051.pth)"
msgstr ""

#: ../../papers/poolformer.md
msgid "PoolFormer-S24\\*"
msgstr ""

#: ../../papers/poolformer.md
msgid "21.39"
msgstr ""

#: ../../papers/poolformer.md
msgid "3.51"
msgstr ""

#: ../../papers/poolformer.md
msgid "80.33"
msgstr ""

#: ../../papers/poolformer.md
msgid "95.05"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/poolformer/poolformer-"
"s24_32xb128_in1k.py)"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/poolformer/poolformer-"
"s24_3rdparty_32xb128_in1k_20220414-d7055904.pth)"
msgstr ""

#: ../../papers/poolformer.md
msgid "PoolFormer-S36\\*"
msgstr ""

#: ../../papers/poolformer.md
msgid "30.86"
msgstr ""

#: ../../papers/poolformer.md
msgid "5.15"
msgstr ""

#: ../../papers/poolformer.md
msgid "81.43"
msgstr ""

#: ../../papers/poolformer.md
msgid "95.45"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/poolformer/poolformer-"
"s36_32xb128_in1k.py)"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/poolformer/poolformer-"
"s36_3rdparty_32xb128_in1k_20220414-d78ff3e8.pth)"
msgstr ""

#: ../../papers/poolformer.md
msgid "PoolFormer-M36\\*"
msgstr ""

#: ../../papers/poolformer.md
msgid "56.17"
msgstr ""

#: ../../papers/poolformer.md
msgid "8.96"
msgstr ""

#: ../../papers/poolformer.md
msgid "82.14"
msgstr ""

#: ../../papers/poolformer.md
msgid "95.71"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/poolformer/poolformer-"
"m36_32xb128_in1k.py)"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/poolformer/poolformer-"
"m36_3rdparty_32xb128_in1k_20220414-c55e0949.pth)"
msgstr ""

#: ../../papers/poolformer.md
msgid "PoolFormer-M48\\*"
msgstr ""

#: ../../papers/poolformer.md
msgid "73.47"
msgstr ""

#: ../../papers/poolformer.md
msgid "11.80"
msgstr ""

#: ../../papers/poolformer.md
msgid "82.51"
msgstr ""

#: ../../papers/poolformer.md
msgid "95.95"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/poolformer/poolformer-"
"m48_32xb128_in1k.py)"
msgstr ""

#: ../../papers/poolformer.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/poolformer/poolformer-"
"m48_3rdparty_32xb128_in1k_20220414-9378f3eb.pth)"
msgstr ""

#: ../../papers/poolformer.md:30
msgid ""
"*Models with * are converted from the [official repo](https://github.com/sail-sg/poolformer). The config "
"files of these models are only for inference. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/regnet.md:4
msgid "RegNet"
msgstr ""

#: ../../papers/regnet.md:6
msgid "[Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)"
msgstr ""

#: ../../papers/regnet.md:12
msgid ""
"In this work, we present a new network design paradigm. Our goal is to help advance the understanding of "
"network design and discover design principles that generalize across settings. Instead of focusing on "
"designing individual network instances, we design network design spaces that parametrize populations of "
"networks. The overall process is analogous to classic manual design of networks, but elevated to the design "
"space level. Using our methodology we explore the structure aspect of network design and arrive at a low-"
"dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of "
"the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a "
"quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do "
"not match the current practice of network design. The RegNet design space provides simple and fast networks "
"that work well across a wide range of flop regimes. Under comparable training settings and flops, the "
"RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs."
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-400MF"
msgstr ""

#: ../../papers/regnet.md
msgid "5.16"
msgstr ""

#: ../../papers/regnet.md
msgid "0.41"
msgstr ""

#: ../../papers/regnet.md
msgid "72.56"
msgstr ""

#: ../../papers/regnet.md
msgid "90.78"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-400mf_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-400mf_8xb128_in1k_20211213-89bfc226.pth) | [log](https://download.openmmlab.com/mmclassification/v0/"
"regnet/regnetx-400mf_8xb128_in1k_20211208_143316.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-800MF"
msgstr ""

#: ../../papers/regnet.md
msgid "7.26"
msgstr ""

#: ../../papers/regnet.md
msgid "0.81"
msgstr ""

#: ../../papers/regnet.md
msgid "74.76"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-800mf_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-800mf_8xb128_in1k_20211213-222b0f11.pth) | [log](https://download.openmmlab.com/mmclassification/v0/"
"regnet/regnetx-800mf_8xb128_in1k_20211207_143037.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-1.6GF"
msgstr ""

#: ../../papers/regnet.md
msgid "9.19"
msgstr ""

#: ../../papers/regnet.md
msgid "1.63"
msgstr ""

#: ../../papers/regnet.md
msgid "76.84"
msgstr ""

#: ../../papers/regnet.md
msgid "93.31"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-1.6gf_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/regnetx-1.6gf_8xb128_in1k_20211213-"
"d1b89758.pth) | [log](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-1.6gf_8xb128_in1k_20211208_143018.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-3.2GF"
msgstr ""

#: ../../papers/regnet.md
msgid "15.3"
msgstr ""

#: ../../papers/regnet.md
msgid "3.21"
msgstr ""

#: ../../papers/regnet.md
msgid "78.09"
msgstr ""

#: ../../papers/regnet.md ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "94.08"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-3.2gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-3.2gf_8xb64_in1k_20211213-1fdd82ae.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/"
"regnet/regnetx-3.2gf_8xb64_in1k_20211208_142720.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-4.0GF"
msgstr ""

#: ../../papers/regnet.md
msgid "22.12"
msgstr ""

#: ../../papers/regnet.md
msgid "4.0"
msgstr ""

#: ../../papers/regnet.md
msgid "78.60"
msgstr ""

#: ../../papers/regnet.md ../../papers/resnext.md
msgid "94.17"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-4.0gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/regnetx-4.0gf_8xb64_in1k_20211213-"
"efed675c.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-4.0gf_8xb64_in1k_20211207_150431.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-6.4GF"
msgstr ""

#: ../../papers/regnet.md
msgid "26.21"
msgstr ""

#: ../../papers/regnet.md
msgid "6.51"
msgstr ""

#: ../../papers/regnet.md ../../papers/repvgg.md
msgid "79.38"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-6.4gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-6.4gf_8xb64_in1k_20211215-5c6089da.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/"
"regnet/regnetx-6.4gf_8xb64_in1k_20211213_172748.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-8.0GF"
msgstr ""

#: ../../papers/regnet.md
msgid "39.57"
msgstr ""

#: ../../papers/regnet.md ../../papers/resnext.md
msgid "8.03"
msgstr ""

#: ../../papers/regnet.md
msgid "79.12"
msgstr ""

#: ../../papers/regnet.md
msgid "94.51"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-8.0gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-8.0gf_8xb64_in1k_20211213-9a9fcc76.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/"
"regnet/regnetx-8.0gf_8xb64_in1k_20211208_103250.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-12GF"
msgstr ""

#: ../../papers/regnet.md
msgid "46.11"
msgstr ""

#: ../../papers/regnet.md
msgid "12.15"
msgstr ""

#: ../../papers/regnet.md
msgid "79.67"
msgstr ""

#: ../../papers/regnet.md
msgid "95.03"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/regnet/regnetx-12gf_8xb64_in1k.py)"
msgstr ""

#: ../../papers/regnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/regnet/regnetx-12gf_8xb64_in1k_20211213-5df8c2f8."
"pth)  | [log](https://download.openmmlab.com/mmclassification/v0/regnet/"
"regnetx-12gf_8xb64_in1k_20211208_143713.log.json)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-400MF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "72.55"
msgstr ""

#: ../../papers/regnet.md
msgid "90.91"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-400MF-0db9f35c.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-800MF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "75.21"
msgstr ""

#: ../../papers/regnet.md
msgid "92.37"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-800MF-4f9d1e8a.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-1.6GF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "77.04"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-1.6GF-cfb32375.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-3.2GF\\*"
msgstr ""

#: ../../papers/regnet.md ../../papers/seresnet.md
msgid "78.26"
msgstr ""

#: ../../papers/regnet.md
msgid "94.20"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-3.2GF-82c43fd5.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-4.0GF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "78.72"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-4.0GF-ef8bb32c.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-6.4GF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "79.22"
msgstr ""

#: ../../papers/regnet.md
msgid "94.61"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-6.4GF-6888c0ea.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-8.0GF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "79.31"
msgstr ""

#: ../../papers/regnet.md
msgid "94.57"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-8.0GF-cb4c77ec.pth)"
msgstr ""

#: ../../papers/regnet.md
msgid "RegNetX-12GF\\*"
msgstr ""

#: ../../papers/regnet.md
msgid "79.91"
msgstr ""

#: ../../papers/regnet.md ../../papers/resnet.md:76
msgid "94.78"
msgstr ""

#: ../../papers/regnet.md
msgid "[model](https://download.openmmlab.com/mmclassification/v0/regnet/convert/RegNetX-12GF-0574538f.pth)"
msgstr ""

#: ../../papers/regnet.md:41
msgid ""
"*Models with * are converted from [pycls](https://github.com/facebookresearch/pycls/blob/master/MODEL_ZOO."
"md). The config files of these models are only for validation.*"
msgstr ""

#: ../../papers/replknet.md:4
msgid "RepLKNet"
msgstr ""

#: ../../papers/replknet.md:6
msgid ""
"[Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs](https://arxiv.org/abs/2203.06717)"
msgstr ""

#: ../../papers/replknet.md:12
msgid ""
"We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances "
"in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels "
"instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., "
"applying re-parameterized large depth-wise convolutions, to design efficient highperformance large-kernel "
"CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large "
"as 31×31, in contrast to commonly used 3×3. RepLKNet greatly closes the performance gap between CNNs and "
"ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical "
"downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, "
"obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the "
"state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel "
"CNNs, large kernel CNNs have much larger effective receptive fields and higher shape bias rather than "
"texture bias."
msgstr ""

#: ../../papers/replknet.md
msgid "Resolution"
msgstr ""

#: ../../papers/replknet.md
msgid "Pretrained Dataset"
msgstr ""

#: ../../papers/replknet.md
msgid "RepLKNet-31B\\*"
msgstr ""

#: ../../papers/replknet.md
msgid "From Scratch"
msgstr ""

#: ../../papers/replknet.md
msgid "79.9（train) | 79.5 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "15.6 (train) | 15.4 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "83.48"
msgstr ""

#: ../../papers/replknet.md
msgid "96.57"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/replknet/"
"replknet-31B_32xb64_in1k.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/"
"configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k.py)"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-"
"fd08e268.pth)"
msgstr ""

#: ../../papers/replknet.md
msgid "46.0 (train) | 45.3 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "97.34"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/replknet/"
"replknet-31B_32xb64_in1k-384px.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/"
"replknet-31B_3rdparty_in1k-384px_20221118-03a170ce.pth)"
msgstr ""

#: ../../papers/replknet.md
msgid "ImageNet-21K"
msgstr ""

#: ../../papers/replknet.md
msgid "85.20"
msgstr ""

#: ../../papers/replknet.md
msgid "97.56"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_in21k-"
"pre_3rdparty_in1k_20221118-54ed5c46.pth)"
msgstr ""

#: ../../papers/replknet.md
msgid "85.99"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_in21k-"
"pre_3rdparty_in1k-384px_20221118-76c92b24.pth)"
msgstr ""

#: ../../papers/replknet.md
msgid "RepLKNet-31L\\*"
msgstr ""

#: ../../papers/replknet.md
msgid "172.7（train) | 172.0 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "97.2 (train) | 97.0 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "86.63"
msgstr ""

#: ../../papers/replknet.md
msgid "98.00"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/replknet/"
"replknet-31L_32xb64_in1k-384px.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/replknet/deploy/replknet-31L-deploy_32xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31L_in21k-"
"pre_3rdparty_in1k-384px_20221118-dc3fc07c.pth)"
msgstr ""

#: ../../papers/replknet.md
msgid "RepLKNet-XL\\*"
msgstr ""

#: ../../papers/replknet.md
msgid "320x320"
msgstr ""

#: ../../papers/replknet.md
msgid "MegData-73M"
msgstr ""

#: ../../papers/replknet.md
msgid "335.4（train) | 335.0 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "129.6 (train) | 129.0 (deploy)"
msgstr ""

#: ../../papers/replknet.md
msgid "87.57"
msgstr ""

#: ../../papers/replknet.md
msgid "98.39"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/replknet/replknet-"
"XL_32xb64_in1k-320px.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/"
"configs/replknet/deploy/replknet-XL-deploy_32xb64_in1k-320px.py)"
msgstr ""

#: ../../papers/replknet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/replknet/replknet-XL_meg73m-"
"pre_3rdparty_in1k-320px_20221118-88259b1d.pth)"
msgstr ""

#: ../../papers/replknet.md:31 ../../papers/repvgg.md:37
msgid ""
"*Models with * are converted from the [official repo](https://github.com/DingXiaoH/RepVGG). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/replknet.md:35 ../../papers/repmlp.md:31 ../../papers/repvgg.md:41
msgid ""
"The checkpoints provided are all `training-time` models. Use the reparameterize tool to switch them to more "
"efficient `inference-time` architecture, which not only has fewer parameters but also less calculations."
msgstr ""

#: ../../papers/replknet.md:37 ../../papers/repmlp.md:33 ../../papers/repvgg.md:43
msgid "Use tool"
msgstr ""

#: ../../papers/replknet.md:45 ../../papers/repmlp.md:41 ../../papers/repvgg.md:51
msgid ""
"`${CFG_PATH}` is the config file, `${SRC_CKPT_PATH}` is the source chenpoint file, `${TARGET_CKPT_PATH}` is "
"the target deploy weight file path."
msgstr ""

#: ../../papers/replknet.md:47 ../../papers/repmlp.md:43 ../../papers/repvgg.md:53
msgid "To use reparameterized weights, the config file must switch to the deploy config files."
msgstr ""

#: ../../papers/replknet.md:53 ../../papers/repmlp.md:49 ../../papers/repvgg.md:59
msgid "In the code"
msgstr ""

#: ../../papers/replknet.md:55 ../../papers/repmlp.md:51 ../../papers/repvgg.md:61
msgid ""
"Use `backbone.switch_to_deploy()` or `classificer.backbone.switch_to_deploy()` to switch to the deploy "
"mode. For example:"
msgstr ""

#: ../../papers/replknet.md:65 ../../papers/repmlp.md:61 ../../papers/repvgg.md:71
msgid "or"
msgstr ""

#: ../../papers/repmlp.md:4
msgid "RepMLP"
msgstr ""

#: ../../papers/repmlp.md:6
msgid ""
"[RepMLP: Re-parameterizing Convolutions into Fully-connected Layers forImage Recognition](https://arxiv.org/"
"abs/2105.01883)"
msgstr ""

#: ../../papers/repmlp.md:12
#, python-format
msgid ""
"We propose RepMLP, a multi-layer-perceptron-style neural network building block for image recognition, "
"which is composed of a series of fully-connected (FC) layers. Compared to convolutional layers, FC layers "
"are more efficient, better at modeling the long-range dependencies and positional patterns, but worse at "
"capturing the local structures, hence usually less favored for image recognition. We propose a structural "
"re-parameterization technique that adds local prior into an FC to make it powerful for image recognition. "
"Specifically, we construct convolutional layers inside a RepMLP during training and merge them into the FC "
"for inference. On CIFAR, a simple pure-MLP model shows performance very close to CNN. By inserting RepMLP "
"in traditional CNN, we improve ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and 2.3% "
"mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight that combining the global "
"representational capacity and positional perception of FC with the local prior of convolution can improve "
"the performance of neural network with faster speed on both the tasks with translation invariance (e.g., "
"semantic segmentation) and those with aligned images and positional patterns (e.g., face recognition)."
msgstr ""

#: ../../papers/repmlp.md
msgid "RepMLP-B224\\*"
msgstr ""

#: ../../papers/repmlp.md
msgid "68.24"
msgstr ""

#: ../../papers/repmlp.md
msgid "6.71"
msgstr ""

#: ../../papers/repmlp.md
msgid "80.41"
msgstr ""

#: ../../papers/repmlp.md
msgid "95.12"
msgstr ""

#: ../../papers/repmlp.md
msgid ""
"[train_cfg](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repmlp/repmlp-base_8xb64_in1k."
"py) | [deploy_cfg](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repmlp/repmlp-"
"base_delopy_8xb64_in1k.py)"
msgstr ""

#: ../../papers/repmlp.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repmlp/repmlp-"
"base_3rdparty_8xb64_in1k_20220330-1cb1f11b.pth)"
msgstr ""

#: ../../papers/repmlp.md
msgid "RepMLP-B256\\*"
msgstr ""

#: ../../papers/repmlp.md
msgid "96.45"
msgstr ""

#: ../../papers/repmlp.md
msgid "9.69"
msgstr ""

#: ../../papers/repmlp.md
msgid "81.11"
msgstr ""

#: ../../papers/repmlp.md
msgid "95.5"
msgstr ""

#: ../../papers/repmlp.md
msgid ""
"[train_cfg](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repmlp/repmlp-"
"base_8xb64_in1k-256px.py) | [deploy_cfg](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repmlp/repmlp-base_deploy_8xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/repmlp.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repmlp/repmlp-"
"base_3rdparty_8xb64_in1k-256px_20220330-7c5a91ce.pth)"
msgstr ""

#: ../../papers/repmlp.md:27
msgid ""
"*Models with * are converted from [the official repo.](https://github.com/DingXiaoH/RepMLP). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/repvgg.md:4
msgid "RepVGG"
msgstr ""

#: ../../papers/repvgg.md:6
msgid "[Repvgg: Making vgg-style convnets great again](https://arxiv.org/abs/2101.03697)"
msgstr ""

#: ../../papers/repvgg.md:12
#, python-format
msgid ""
"We present a simple but powerful architecture of convolutional neural network, which has a VGG-like "
"inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time "
"model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is "
"realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, "
"RepVGG reaches over 80% top-1 accuracy, which is the first time for a plain model, to the best of our "
"knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 "
"with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models "
"like EfficientNet and RegNet."
msgstr ""

#: ../../papers/repvgg.md
msgid "Epochs"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-A0\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "120"
msgstr ""

#: ../../papers/repvgg.md
msgid "9.11（train) | 8.31 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "1.52 (train) | 1.36 (deploy)"
msgstr ""

#: ../../papers/repvgg.md ../../papers/vgg.md
msgid "72.41"
msgstr ""

#: ../../papers/repvgg.md
msgid "90.50"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-A0_4xb64-"
"coslr-120e_in1k.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-A0_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-A0_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-883ab98c.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-A1\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "14.09 (train) | 12.79 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "2.64 (train) | 2.37 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "74.47"
msgstr ""

#: ../../papers/repvgg.md
msgid "91.85"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-A1_4xb64-"
"coslr-120e_in1k.py) | [config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-A1_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-A1_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-24003a24.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-A2\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "28.21 (train) | 25.5 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "5.7 (train)  | 5.12 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "76.48"
msgstr ""

#: ../../papers/repvgg.md
msgid "93.01"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-A2_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-A2_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-A2_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-97d7695a.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B0\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "15.82 (train) | 14.34 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "3.42 (train) | 3.06 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "75.14"
msgstr ""

#: ../../papers/repvgg.md
msgid "92.42"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B0_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-B0_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B0_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-446375f4.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B1\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "57.42 (train) | 51.83 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "13.16 (train) | 11.82 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "78.37"
msgstr ""

#: ../../papers/repvgg.md
msgid "94.11"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B1_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-B1_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B1_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-750cdf67.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B1g2\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "45.78 (train) | 41.36 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "9.82 (train) | 8.82 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "77.79"
msgstr ""

#: ../../papers/repvgg.md
msgid "93.88"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B1g2_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-B1g2_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B1g2_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-344f6422.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B1g4\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "39.97 (train) | 36.13 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "8.15 (train) | 7.32 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "77.58"
msgstr ""

#: ../../papers/repvgg.md ../../papers/seresnet.md
msgid "93.84"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B1g4_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-B1g4_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B1g4_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-d4c1a642.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B2\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "89.02 (train) | 80.32 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "20.46 (train) | 18.39 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "78.78"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B2_4xb64-"
"coslr-120e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/"
"repvgg/deploy/repvgg-B2_deploy_4xb64-coslr-120e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B2_3rdparty_4xb64-"
"coslr-120e_in1k_20210909-bd6b937c.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B2g4\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "200"
msgstr ""

#: ../../papers/repvgg.md
msgid "61.76 (train) | 55.78 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "12.63 (train) | 11.34 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B2g4_4xb64-"
"autoaug-lbs-mixup-coslr-200e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/repvgg/deploy/repvgg-B2g4_deploy_4xb64-autoaug-lbs-mixup-coslr-200e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B2g4_3rdparty_4xb64-autoaug-lbs-"
"mixup-coslr-200e_in1k_20210909-7b7955f0.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B3\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "123.09 (train) | 110.96 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "29.17 (train) | 26.22 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "80.52"
msgstr ""

#: ../../papers/repvgg.md
msgid "95.26"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B3_4xb64-"
"autoaug-lbs-mixup-coslr-200e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/repvgg/deploy/repvgg-B3_deploy_4xb64-autoaug-lbs-mixup-coslr-200e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B3_3rdparty_4xb64-autoaug-lbs-"
"mixup-coslr-200e_in1k_20210909-dda968bf.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-B3g4\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "83.83 (train) | 75.63 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "17.9 (train) | 16.08 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "80.22"
msgstr ""

#: ../../papers/repvgg.md
msgid "95.10"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-B3g4_4xb64-"
"autoaug-lbs-mixup-coslr-200e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/repvgg/deploy/repvgg-B3g4_deploy_4xb64-autoaug-lbs-mixup-coslr-200e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-B3g4_3rdparty_4xb64-autoaug-lbs-"
"mixup-coslr-200e_in1k_20210909-4e54846a.pth)"
msgstr ""

#: ../../papers/repvgg.md
msgid "RepVGG-D2se\\*"
msgstr ""

#: ../../papers/repvgg.md
msgid "133.33 (train) | 120.39 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "36.56 (train) | 32.85 (deploy)"
msgstr ""

#: ../../papers/repvgg.md
msgid "95.94"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[config (train)](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/repvgg/repvgg-D2se_4xb64-"
"autoaug-lbs-mixup-coslr-200e_in1k.py) |[config (deploy)](https://github.com/open-mmlab/mmclassification/"
"blob/1.x/configs/repvgg/deploy/repvgg-D2se_deploy_4xb64-autoaug-lbs-mixup-coslr-200e_in1k.py)"
msgstr ""

#: ../../papers/repvgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/repvgg/repvgg-D2se_3rdparty_4xb64-autoaug-lbs-"
"mixup-coslr-200e_in1k_20210909-cf3139b7.pth)"
msgstr ""

#: ../../papers/res2net.md:4
msgid "Res2Net"
msgstr ""

#: ../../papers/res2net.md:6
msgid "[Res2Net: A New Multi-scale Backbone Architecture](https://arxiv.org/pdf/1904.01169.pdf)"
msgstr ""

#: ../../papers/res2net.md:12
msgid ""
"Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances "
"in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale "
"representation ability, leading to consistent performance gains on a wide range of applications. However, "
"most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose "
"a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections "
"within one single residual block. The Res2Net represents multi-scale features at a granular level and "
"increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged "
"into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net "
"block on all these models and demonstrate consistent performance gains over baseline models on widely-used "
"datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative "
"computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, "
"further verify the superiority of the Res2Net over the state-of-the-art baseline methods."
msgstr ""

#: ../../papers/res2net.md
msgid "Res2Net-50-14w-8s\\*"
msgstr ""

#: ../../papers/res2net.md
msgid "25.06"
msgstr ""

#: ../../papers/res2net.md
msgid "4.22"
msgstr ""

#: ../../papers/res2net.md
msgid "78.14"
msgstr ""

#: ../../papers/res2net.md
msgid "93.85"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/res2net/res2net50-w14-"
"s8_8xb32_in1k.py)"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/res2net/res2net50-w14-"
"s8_3rdparty_8xb32_in1k_20210927-bc967bf1.pth)"
msgstr ""

#: ../../papers/res2net.md
msgid "Res2Net-50-26w-8s\\*"
msgstr ""

#: ../../papers/res2net.md
msgid "48.40"
msgstr ""

#: ../../papers/res2net.md
msgid "8.39"
msgstr ""

#: ../../papers/res2net.md
msgid "94.36"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/res2net/res2net50-w26-"
"s8_8xb32_in1k.py)"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/res2net/res2net50-w26-"
"s8_3rdparty_8xb32_in1k_20210927-f547a94b.pth)"
msgstr ""

#: ../../papers/res2net.md
msgid "Res2Net-101-26w-4s\\*"
msgstr ""

#: ../../papers/res2net.md
msgid "45.21"
msgstr ""

#: ../../papers/res2net.md
msgid "8.12"
msgstr ""

#: ../../papers/res2net.md
msgid "79.19"
msgstr ""

#: ../../papers/res2net.md
msgid "94.44"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/res2net/res2net101-w26-"
"s4_8xb32_in1k.py)"
msgstr ""

#: ../../papers/res2net.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/res2net/res2net101-w26-"
"s4_3rdparty_8xb32_in1k_20210927-870b6c36.pth)"
msgstr ""

#: ../../papers/res2net.md:28
msgid ""
"*Models with * are converted from the [official repo](https://github.com/Res2Net/Res2Net-PretrainedModels). "
"The config files of these models are only for validation. We don't ensure these config files' training "
"accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/resnet.md:4
msgid "ResNet"
msgstr ""

#: ../../papers/resnet.md:6
msgid ""
"[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/"
"He_Deep_Residual_Learning_CVPR_2016_paper.html)"
msgstr ""

#: ../../papers/resnet.md:12
msgid ""
"**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead "
"of learning unreferenced functions. In the mainstream previous works, like VGG, the neural networks are a "
"stack of layers and every layer attempts to fit a desired underlying mapping. In ResNets, a few stacked "
"layers are grouped as a block, and the layers in a block attempts to learn a residual mapping."
msgstr ""

#: ../../papers/resnet.md:17
msgid ""
"Formally, denoting the desired underlying mapping of a block as $\\mathcal{H}(x)$, split the underlying "
"mapping into the sum of the identity and the residual mapping as $\\mathcal{H}(x) = x + \\mathcal{F}(x)$, "
"and let the stacked non-linear layers fit the residual mapping $\\mathcal{F}(x)$."
msgstr ""

#: ../../papers/resnet.md:21
msgid ""
"Many works proved this method makes deep neural networks easier to optimize, and can gain accuracy from "
"considerably increased depth. Recently, the residual structure is widely used in various models."
msgstr ""

#: ../../papers/resnet.md:37
#, python-format
msgid ""
"The depth of representations is of central importance for many visual recognition tasks. Solely due to our "
"extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. "
"Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won "
"the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO "
"segmentation. </br>"
msgstr ""

#: ../../papers/resnet.md:92
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.ResNet.html#mmcls.models.backbones.ResNet)."
msgstr ""

#: ../../papers/resnet.md:96 ../../papers/swin_transformer.md:88 ../../papers/swin_transformer_v2.md:98
#: ../../papers/vision_transformer.md:99
msgid ""
"The pre-trained models on ImageNet-21k are used to fine-tune, and therefore don't have evaluation results."
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50-mill"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "15.14"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_3rdparty-mill_in21k_20220331-"
"faac000b.pth)"
msgstr ""

#: ../../papers/resnet.md:102
msgid ""
"*The \"mill\" means using the mutil-label pretrain weight from [ImageNet-21K Pretraining for the Masses]"
"(https://github.com/Alibaba-MIIL/ImageNet21K).*"
msgstr ""

#: ../../papers/resnet.md:104
msgid "Cifar10"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-18"
msgstr ""

#: ../../papers/resnet.md:76
msgid "11.17"
msgstr ""

#: ../../papers/resnet.md:76
msgid "0.56"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.82"
msgstr ""

#: ../../papers/resnet.md:76
msgid "99.87"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet18_8xb16_cifar10.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_b16x8_cifar10_20210528-bd6371c8."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_b16x8_cifar10_20210528-"
"bd6371c8.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-34"
msgstr ""

#: ../../papers/resnet.md:76
msgid "21.28"
msgstr ""

#: ../../papers/resnet.md:76
msgid "1.16"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet34_8xb16_cifar10.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet34_b16x8_cifar10_20210528-a8aa36a6."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet34_b16x8_cifar10_20210528-"
"a8aa36a6.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50"
msgstr ""

#: ../../papers/resnet.md:76
msgid "1.31"
msgstr ""

#: ../../papers/resnet.md:76
msgid "95.55"
msgstr ""

#: ../../papers/resnet.md:76
msgid "99.91"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb16_cifar10.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_b16x8_cifar10_20210528-f54bfad9."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_b16x8_cifar10_20210528-"
"f54bfad9.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-101"
msgstr ""

#: ../../papers/resnet.md:76
msgid "42.51"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/van.md
msgid "2.52"
msgstr ""

#: ../../papers/resnet.md:76
msgid "95.58"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet101_8xb16_cifar10.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_b16x8_cifar10_20210528-2d29e936."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet101_b16x8_cifar10_20210528-2d29e936.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-152"
msgstr ""

#: ../../papers/resnet.md:76
msgid "58.16"
msgstr ""

#: ../../papers/resnet.md:76
msgid "3.74"
msgstr ""

#: ../../papers/resnet.md:76
msgid "95.76"
msgstr ""

#: ../../papers/resnet.md:76
msgid "99.89"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet152_8xb16_cifar10.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet152_b16x8_cifar10_20210528-3e8e9178."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet152_b16x8_cifar10_20210528-3e8e9178.log.json)"
msgstr ""

#: ../../papers/resnet.md:114
msgid "Cifar100"
msgstr ""

#: ../../papers/resnet.md:76
msgid "23.71"
msgstr ""

#: ../../papers/resnet.md:76
msgid "79.90"
msgstr ""

#: ../../papers/resnet.md:76
msgid "95.19"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb16_cifar100.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_b16x8_cifar100_20210528-67b58a1b."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet50_b16x8_cifar100_20210528-67b58a1b.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "11.69"
msgstr ""

#: ../../papers/resnet.md:76
msgid "1.82"
msgstr ""

#: ../../papers/resnet.md:76
msgid "69.90"
msgstr ""

#: ../../papers/resnet.md:76
msgid "89.43"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet18_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_8xb32_in1k_20210831-fbbb1da6."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_8xb32_in1k_20210831-"
"fbbb1da6.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "21.8"
msgstr ""

#: ../../papers/resnet.md:76
msgid "3.68"
msgstr ""

#: ../../papers/resnet.md:76
msgid "73.62"
msgstr ""

#: ../../papers/resnet.md:76
msgid "91.59"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet34_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet34_8xb32_in1k_20210831-f257d4e6."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet34_8xb32_in1k_20210831-"
"f257d4e6.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "25.56"
msgstr ""

#: ../../papers/resnet.md:76
msgid "76.55"
msgstr ""

#: ../../papers/resnet.md:76
msgid "93.06"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-"
"ea4938fc.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "44.55"
msgstr ""

#: ../../papers/resnet.md:76
msgid "7.85"
msgstr ""

#: ../../papers/resnet.md:76
msgid "77.97"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.06"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet101_8xb32_in1k_20210831-539c63f8.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "60.19"
msgstr ""

#: ../../papers/resnet.md:76
msgid "11.58"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "78.48"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.13"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet152_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet152_8xb32_in1k_20210901-4d7582fa."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet152_8xb32_in1k_20210901-4d7582fa.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1C-50"
msgstr ""

#: ../../papers/resnet.md:76
msgid "25.58"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/swin_transformer.md:66
msgid "4.36"
msgstr ""

#: ../../papers/resnet.md:76
msgid "77.01"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1c50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnetv1c50_8xb32_in1k_20220214-3343eccd."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnetv1c50_8xb32_in1k_20220214-3343eccd.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1C-101"
msgstr ""

#: ../../papers/resnet.md:76
msgid "8.09"
msgstr ""

#: ../../papers/resnet.md:76
msgid "78.30"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.27"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1c101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnetv1c101_8xb32_in1k_20220214-434fe45f."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnetv1c101_8xb32_in1k_20220214-434fe45f.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1C-152"
msgstr ""

#: ../../papers/resnet.md:76
msgid "60.21"
msgstr ""

#: ../../papers/resnet.md:76
msgid "11.82"
msgstr ""

#: ../../papers/resnet.md:76
msgid "78.76"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.41"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1c152_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnetv1c152_8xb32_in1k_20220214-c013291f."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnetv1c152_8xb32_in1k_20220214-"
"c013291f.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1D-50"
msgstr ""

#: ../../papers/resnet.md:76
msgid "77.54"
msgstr ""

#: ../../papers/resnet.md:76
msgid "93.57"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1d50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnetv1d50_b32x8_imagenet_20210531-"
"db14775a.pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnetv1d50_b32x8_imagenet_20210531-db14775a.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1D-101"
msgstr ""

#: ../../papers/resnet.md:76
msgid "78.93"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.48"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1d101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnetv1d101_b32x8_imagenet_20210531-6e13bcd3.pth) | [log](https://download.openmmlab.com/mmclassification/"
"v0/resnet/resnetv1d101_b32x8_imagenet_20210531-6e13bcd3.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNetV1D-152"
msgstr ""

#: ../../papers/resnet.md:76
msgid "94.70"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnetv1d152_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnetv1d152_b32x8_imagenet_20210531-278cf22a.pth) | [log](https://download.openmmlab.com/mmclassification/"
"v0/resnet/resnetv1d152_b32x8_imagenet_20210531-278cf22a.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50 (fp16)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "76.30"
msgstr ""

#: ../../papers/resnet.md:76
msgid "93.07"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb32-fp16_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/fp16/resnet50_batch256_fp16_imagenet_20210320-"
"b3964210.pth) | [log](https://download.openmmlab.com/mmclassification/v0/fp16/"
"resnet50_batch256_fp16_imagenet_20210320-b3964210.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "Wide-ResNet-50\\*"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "68.88"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "11.44"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/../wrn/wide-"
"resnet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/wide-"
"resnet50_3rdparty_8xb32_in1k_20220304-66678344.pth)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "Wide-ResNet-101\\*"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "126.89"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "22.81"
msgstr ""

#: ../../papers/resnet.md:76 ../../papers/wrn.md
msgid "78.84"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/../wrn/wide-"
"resnet101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/wide-"
"resnet101_3rdparty_8xb32_in1k_20220304-8d5f9d61.pth)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50 (rsb-a1)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "80.12"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb256-rsb-"
"a1-600e_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb256-rsb-"
"a1-600e_in1k_20211228-20e21305.pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet50_8xb256-rsb-a1-600e_in1k_20211228-20e21305.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50 (rsb-a2)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb256-rsb-"
"a2-300e_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb256-rsb-"
"a2-300e_in1k_20211228-0fd8be6e.pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet50_8xb256-rsb-a2-300e_in1k_20211228-0fd8be6e.log.json)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "ResNet-50 (rsb-a3)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "93.80"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb256-rsb-"
"a3-100e_in1k.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb256-rsb-"
"a3-100e_in1k_20211228-3493673c.pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnet/"
"resnet50_8xb256-rsb-a3-100e_in1k_20211228-3493673c.log.json)"
msgstr ""

#: ../../papers/resnet.md:142
msgid ""
"*The \"rsb\" means using the training settings from [ResNet strikes back: An improved training procedure in "
"timm](https://arxiv.org/abs/2110.00476).*"
msgstr ""

#: ../../papers/resnet.md:144
msgid ""
"*Models with * are converted from the [official repo](https://github.com/pytorch/vision). The config files "
"of these models are only for validation. We don't ensure these config files' training accuracy and welcome "
"you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/resnet.md:146 ../../papers/swin_transformer.md:114
msgid "CUB-200-2011"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[ImageNet-21k-mill](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_3rdparty-"
"mill_in21k_20220331-faac000b.pth)"
msgstr ""

#: ../../papers/resnet.md:76
msgid "448x448"
msgstr ""

#: ../../papers/resnet.md:76
msgid "23.92"
msgstr ""

#: ../../papers/resnet.md:76
msgid "16.48"
msgstr ""

#: ../../papers/resnet.md:76
msgid "88.45"
msgstr ""

#: ../../papers/resnet.md:76
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnet/resnet50_8xb8_cub.py)"
msgstr ""

#: ../../papers/resnet.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb8_cub_20220307-57840e60.pth) "
"| [log](https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb8_cub_20220307-57840e60.log."
"json)"
msgstr ""

#: ../../papers/resnext.md:4
msgid "ResNeXt"
msgstr ""

#: ../../papers/resnext.md:6
msgid ""
"[Aggregated Residual Transformations for Deep Neural Networks](https://openaccess.thecvf.com/"
"content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html)"
msgstr ""

#: ../../papers/resnext.md:12
msgid ""
"We present a simple, highly modularized network architecture for image classification. Our network is "
"constructed by repeating a building block that aggregates a set of transformations with the same topology. "
"Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters "
"to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of "
"transformations), as an essential factor in addition to the dimensions of depth and width. On the "
"ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining "
"complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing "
"cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named "
"ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd "
"place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better "
"results than its ResNet counterpart. The code and models are publicly available online."
msgstr ""

#: ../../papers/resnext.md
msgid "ResNeXt-32x4d-50"
msgstr ""

#: ../../papers/resnext.md
msgid "25.03"
msgstr ""

#: ../../papers/resnext.md
msgid "4.27"
msgstr ""

#: ../../papers/resnext.md
msgid "77.90"
msgstr ""

#: ../../papers/resnext.md
msgid "93.66"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnext/resnext50-32x4d_8xb32_in1k."
"py)"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnext/"
"resnext50_32x4d_b32x8_imagenet_20210429-56066e27.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/resnext/resnext50_32x4d_b32x8_imagenet_20210429-56066e27.log.json)"
msgstr ""

#: ../../papers/resnext.md
msgid "ResNeXt-32x4d-101"
msgstr ""

#: ../../papers/resnext.md
msgid "44.18"
msgstr ""

#: ../../papers/resnext.md
msgid "78.61"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnext/"
"resnext101-32x4d_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnext/resnext101_32x4d_b32x8_imagenet_20210506-"
"e0fa3dd5.pth) | [log](https://download.openmmlab.com/mmclassification/v0/resnext/"
"resnext101_32x4d_b32x8_imagenet_20210506-e0fa3dd5.log.json)"
msgstr ""

#: ../../papers/resnext.md
msgid "ResNeXt-32x8d-101"
msgstr ""

#: ../../papers/resnext.md
msgid "88.79"
msgstr ""

#: ../../papers/resnext.md
msgid "16.5"
msgstr ""

#: ../../papers/resnext.md
msgid "79.27"
msgstr ""

#: ../../papers/resnext.md
msgid "94.58"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnext/"
"resnext101-32x8d_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnext/"
"resnext101_32x8d_b32x8_imagenet_20210506-23a247d5.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/resnext/resnext101_32x8d_b32x8_imagenet_20210506-23a247d5.log.json)"
msgstr ""

#: ../../papers/resnext.md
msgid "ResNeXt-32x4d-152"
msgstr ""

#: ../../papers/resnext.md
msgid "59.95"
msgstr ""

#: ../../papers/resnext.md
msgid "11.8"
msgstr ""

#: ../../papers/resnext.md
msgid "94.33"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/resnext/"
"resnext152-32x4d_8xb32_in1k.py)"
msgstr ""

#: ../../papers/resnext.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/resnext/"
"resnext152_32x4d_b32x8_imagenet_20210524-927787be.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/resnext/resnext152_32x4d_b32x8_imagenet_20210524-927787be.log.json)"
msgstr ""

#: ../../papers/seresnet.md:4
msgid "SE-ResNet"
msgstr ""

#: ../../papers/seresnet.md:6
msgid ""
"[Squeeze-and-Excitation Networks](https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-"
"Excitation_Networks_CVPR_2018_paper.html)"
msgstr ""

#: ../../papers/seresnet.md:12
msgid ""
"The central building block of convolutional neural networks (CNNs) is the convolution operator, which "
"enables networks to construct informative features by fusing both spatial and channel-wise information "
"within local receptive fields at each layer. A broad range of prior research has investigated the spatial "
"component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the "
"quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the "
"channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation"
"\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling "
"interdependencies between channels. We show that these blocks can be stacked together to form SENet "
"architectures that generalise extremely effectively across different datasets. We further demonstrate that "
"SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight "
"additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 "
"classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the "
"winning entry of 2016 by a relative improvement of ~25%."
msgstr ""

#: ../../papers/seresnet.md
msgid "SE-ResNet-50"
msgstr ""

#: ../../papers/seresnet.md
msgid "28.09"
msgstr ""

#: ../../papers/seresnet.md
msgid "4.13"
msgstr ""

#: ../../papers/seresnet.md
msgid "77.74"
msgstr ""

#: ../../papers/seresnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/seresnet/seresnet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/seresnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/se-resnet/se-resnet50_batch256_imagenet_20200804-"
"ae206104.pth) | [log](https://download.openmmlab.com/mmclassification/v0/se-resnet/se-"
"resnet50_batch256_imagenet_20200708-657b3c36.log.json)"
msgstr ""

#: ../../papers/seresnet.md
msgid "SE-ResNet-101"
msgstr ""

#: ../../papers/seresnet.md
msgid "49.33"
msgstr ""

#: ../../papers/seresnet.md
msgid "7.86"
msgstr ""

#: ../../papers/seresnet.md
msgid "94.07"
msgstr ""

#: ../../papers/seresnet.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/seresnet/seresnet101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/seresnet.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/se-resnet/se-"
"resnet101_batch256_imagenet_20200804-ba5b51d4.pth) | [log](https://download.openmmlab.com/mmclassification/"
"v0/se-resnet/se-resnet101_batch256_imagenet_20200708-038a4d04.log.json)"
msgstr ""

#: ../../papers/shufflenet_v1.md:4
msgid "ShuffleNet V1"
msgstr ""

#: ../../papers/shufflenet_v1.md:6
msgid ""
"[ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://openaccess."
"thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html)"
msgstr ""

#: ../../papers/shufflenet_v1.md:12
msgid ""
"We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed "
"specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture "
"utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation "
"cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection "
"demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute "
"7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On "
"an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining "
"comparable accuracy."
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid "ShuffleNetV1 1.0x (group=3)"
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid "0.146"
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid "68.13"
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid "87.81"
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/shufflenet_v1/shufflenet-"
"v1-1x_16xb64_in1k.py)"
msgstr ""

#: ../../papers/shufflenet_v1.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/shufflenet_v1/"
"shufflenet_v1_batch1024_imagenet_20200804-5d6cec73.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/shufflenet_v1/shufflenet_v1_batch1024_imagenet_20200804-5d6cec73.log.json)"
msgstr ""

#: ../../papers/shufflenet_v2.md:4
msgid "ShuffleNet V2"
msgstr ""

#: ../../papers/shufflenet_v2.md:6
msgid ""
"[Shufflenet v2: Practical guidelines for efficient cnn architecture design](https://openaccess.thecvf.com/"
"content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf)"
msgstr ""

#: ../../papers/shufflenet_v2.md:12
msgid ""
"Currently, the neural network architecture design is mostly guided by the *indirect* metric of computation "
"complexity, i.e., FLOPs. However, the *direct* metric, e.g., speed, also depends on the other factors such "
"as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on "
"the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work "
"derives several practical *guidelines* for efficient network design. Accordingly, a new architecture is "
"presented, called *ShuffleNet V2*. Comprehensive ablation experiments verify that our model is the state-of-"
"the-art in terms of speed and accuracy tradeoff."
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid "ShuffleNetV2 1.0x"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid "2.28"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid "0.149"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid "69.55"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid "88.92"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/shufflenet_v2/shufflenet-"
"v2-1x_16xb64_in1k.py)"
msgstr ""

#: ../../papers/shufflenet_v2.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/shufflenet_v2/"
"shufflenet_v2_batch1024_imagenet_20200812-5bf4721e.pth) | [log](https://download.openmmlab.com/"
"mmclassification/v0/shufflenet_v2/shufflenet_v2_batch1024_imagenet_20200804-8860eec9.log.json)"
msgstr ""

#: ../../papers/swin_transformer.md:4
msgid "Swin Transformer"
msgstr ""

#: ../../papers/swin_transformer.md:6
msgid ""
"[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030."
"pdf)"
msgstr ""

#: ../../papers/swin_transformer.md:12
msgid ""
"**Swin Transformer** (the name **Swin** stands for Shifted window) is initially described in [the paper]"
"(https://arxiv.org/pdf/2103.14030.pdf), which capably serves as a general-purpose backbone for computer "
"vision. It is basically a hierarchical Transformer whose representation is computed with shifted windows. "
"The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-"
"overlapping local windows while also allowing for cross-window connection."
msgstr ""

#: ../../papers/swin_transformer.md:14
msgid ""
"Swin Transformer achieves strong performance on COCO object detection (58.7 box AP and 51.1 mask AP on test-"
"dev) and ADE20K semantic segmentation (53.5 mIoU on val), surpassing previous models by a large margin."
msgstr ""

#: ../../papers/swin_transformer.md:82
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.SwinTransformer.html#mmcls.models.backbones.SwinTransformer)."
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "Swin-B"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/swin-"
"base_3rdparty_in21k.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "44.49"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/swin-"
"base_3rdparty_in21k-384px.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "Swin-L"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "195.00"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "34.04"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/swin-"
"large_3rdparty_in21k.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "195.20"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "100.04"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "Swin-T"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "28.29"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "81.18"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"tiny_16xb64_in1k.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/"
"swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth)  | [log](https://download.openmmlab.com/"
"mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925.log.json)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "Swin-S"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "49.61"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "8.52"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "83.02"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/twins.md
msgid "96.29"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"small_16xb64_in1k.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/"
"swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth)  | [log](https://download.openmmlab.com/"
"mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219.log.json)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "87.77"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "83.36"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"base_16xb64_in1k.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/"
"swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth)  | [log](https://download.openmmlab.com/"
"mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742.log.json)"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "Swin-S\\*"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "83.21"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "96.25"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_small_patch4_window7_224-cc7a01c9.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "Swin-B\\*"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "83.42"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_base_patch4_window7_224-4670dd19.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "87.90"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "84.49"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"base_16xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_base_patch4_window12_384-02c598a4.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "85.16"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "97.50"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_base_patch4_window7_224_22kto1k-f967f799.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "86.44"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "98.05"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_base_patch4_window12_384_22kto1k-d59b0d1d.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "Swin-L\\*"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "196.53"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "86.24"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "97.88"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"large_16xb64_in1k.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_large_patch4_window7_224_22kto1k-5f0996db.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66 ../../papers/swin_transformer_v2.md:76
msgid "196.74"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "87.25"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "98.25"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"large_16xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/"
"swin_large_patch4_window12_384_22kto1k-0a40944b.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:112 ../../papers/swin_transformer_v2.md:120
msgid ""
"*Models with * are converted from the [official repo](https://github.com/microsoft/Swin-Transformer#main-"
"results-on-imagenet-with-pretrained-models). The config files of these models are only for validation. We "
"don't ensure these config files' training accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[ImageNet-21k](https://download.openmmlab.com/mmclassification/v0/swin-transformer/convert/swin-"
"base_3rdparty_in21k-384px.pth)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "195.51"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid "91.87"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer/swin-"
"large_8xb8_cub_384px.py)"
msgstr ""

#: ../../papers/swin_transformer.md:66
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin-"
"large_8xb8_cub_384px_20220307-1bbaee6a.pth) | [log](https://download.openmmlab.com/mmclassification/v0/swin-"
"transformer/swin-large_8xb8_cub_384px_20220307-1bbaee6a.log.json)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:4
msgid "Swin Transformer V2"
msgstr ""

#: ../../papers/swin_transformer_v2.md:6
msgid "[Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883.pdf)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:12
msgid ""
"**Swin Transformer V2** is a work on the scale up visual model based on [Swin Transformer](https://github."
"com/open-mmlab/mmclassification/tree/1.x/configs/swin_transformer). In the visual field, We can not "
"increase the performance by just simply scaling up the visual model like NLP models. The possible reasons "
"mentioned in the article are:"
msgstr ""

#: ../../papers/swin_transformer_v2.md:14
msgid "Training instability when increasing the vision model"
msgstr ""

#: ../../papers/swin_transformer_v2.md:15
msgid "Migrating the model trained at low resolution to a larger scale resolution task"
msgstr ""

#: ../../papers/swin_transformer_v2.md:16
msgid "Too mush GPU memory"
msgstr ""

#: ../../papers/swin_transformer_v2.md:18
msgid "To solve it, The following method improvements are proposed in the paper:"
msgstr ""

#: ../../papers/swin_transformer_v2.md:20
msgid "post normalization: layer normalization after self-attention layer and MLP block"
msgstr ""

#: ../../papers/swin_transformer_v2.md:21
msgid ""
"scaled cosine attention approach: use cosine similarity to calculate the relationship between token pairs"
msgstr ""

#: ../../papers/swin_transformer_v2.md:22
msgid "log-spaced continuous position bias: redefine relative position encoding"
msgstr ""

#: ../../papers/swin_transformer_v2.md:36
msgid ""
"Large-scale NLP models have been shown to significantly improve the performance on language tasks with no "
"signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This "
"paper aims to explore large-scale models in computer vision. We tackle three major issues in training and "
"application of large vision models, including training instability, resolution gaps between pre-training "
"and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm "
"method combined with cosine attention to improve training stability; 2) A log-spaced continuous position "
"bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with "
"high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast "
"labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin "
"Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training "
"with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision "
"tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and "
"Kinetics-400 video action classification. Also note our training is much more efficient than that in "
"Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training "
"time."
msgstr ""

#: ../../papers/swin_transformer_v2.md:92
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.SwinTransformerV2.html#mmcls.models.backbones.SwinTransformerV2)."
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "192x192"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "87.92"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "8.51"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/pretrain/swinv2-base-"
"w12_3rdparty_in21k-192px_20220803-f7dc9763.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "19.04"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/pretrain/swinv2-large-"
"w12_3rdparty_in21k-192px_20220803-d9073fee.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "window"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "Swin-T\\*"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "256x256"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "8x8"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "28.35"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "4.35"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "95.87"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-tiny-"
"w8_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-tiny-"
"w8_3rdparty_in1k-256px_20220803-e318968f.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "16x16"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "4.4"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "82.81"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-tiny-"
"w16_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-tiny-"
"w16_3rdparty_in1k-256px_20220803-9651cdd7.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "49.73"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "8.45"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "83.74"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "96.6"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-small-"
"w8_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-small-"
"w8_3rdparty_in1k-256px_20220803-b01a4332.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "8.57"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "84.13"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "96.83"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-small-"
"w16_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-small-"
"w16_3rdparty_in1k-256px_20220803-b707d206.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "14.99"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "84.2"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-base-"
"w8_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-base-"
"w8_3rdparty_in1k-256px_20220803-8ff28f2b.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "84.6"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "97.05"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-base-"
"w16_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-base-"
"w16_3rdparty_in1k-256px_20220803-5a1886b7.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "86.17"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-base-"
"w16_in21k-pre_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-base-w16_in21k-"
"pre_3rdparty_in1k-256px_20220803-8d7aa8ad.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "24x24"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "34.07"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "87.14"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "98.23"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-base-"
"w24_in21k-pre_16xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-base-w24_in21k-"
"pre_3rdparty_in1k-384px_20220803-44eb70f8.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "256X256"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "196.75"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "33.86"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "86.93"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "98.06"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-large-"
"w16_in21k-pre_16xb64_in1k-256px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-large-w16_in21k-"
"pre_3rdparty_in1k-256px_20220803-c40cbed7.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "76.2"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "87.59"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid "98.27"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/swin_transformer_v2/swinv2-large-"
"w24_in21k-pre_16xb64_in1k-384px.py)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:76
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/swin-v2/swinv2-large-w24_in21k-"
"pre_3rdparty_in1k-384px_20220803-3b36c165.pth)"
msgstr ""

#: ../../papers/swin_transformer_v2.md:122
msgid ""
"*ImageNet-21k pretrained models with input resolution of 256x256 and 384x384 both fine-tuned from the same "
"pre-training model using a smaller input resolution of 192x192.*"
msgstr ""

#: ../../papers/t2t_vit.md:4
msgid "Tokens-to-Token ViT"
msgstr ""

#: ../../papers/t2t_vit.md:6
msgid ""
"[Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet](https://arxiv.org/"
"abs/2101.11986)"
msgstr ""

#: ../../papers/t2t_vit.md:12
#, python-format
msgid ""
"Transformers, which are popular for language modeling, have been explored for solving vision tasks "
"recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into "
"a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global "
"relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch "
"on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails "
"to model the important local structure such as edges and lines among neighboring pixels, leading to low "
"training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature "
"richness for fixed computation budgets and limited training samples. To overcome such limitations, we "
"propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-"
"Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating "
"neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding "
"tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow "
"structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-"
"ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement "
"when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with "
"MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M "
"parameters) can achieve 83.3% top1 accuracy in image resolution 384×384 on ImageNet."
msgstr ""

#: ../../papers/t2t_vit.md
msgid "T2T-ViT_t-14"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "21.47"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "4.34"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "81.83"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "95.84"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/t2t_vit/t2t-vit-t-14_8xb64_in1k.py)"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/t2t-vit/t2t-vit-t-14_8xb64_in1k_20211220-"
"f7378dd5.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/t2t-vit/t2t-vit-"
"t-14_8xb64_in1k_20211220-f7378dd5.log.json)"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "T2T-ViT_t-19"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "39.08"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "7.80"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "82.63"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/t2t_vit/t2t-vit-t-19_8xb64_in1k.py)"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/t2t-vit/t2t-vit-"
"t-19_8xb64_in1k_20211214-7f5e3aaf.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/t2t-vit/"
"t2t-vit-t-19_8xb64_in1k_20211214-7f5e3aaf.log.json)"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "T2T-ViT_t-24"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "64.00"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "12.69"
msgstr ""

#: ../../papers/t2t_vit.md
msgid "82.71"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/t2t_vit/t2t-vit-t-24_8xb64_in1k.py)"
msgstr ""

#: ../../papers/t2t_vit.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/t2t-vit/t2t-vit-t-24_8xb64_in1k_20211214-"
"b2a68ae3.pth)  | [log](https://download.openmmlab.com/mmclassification/v0/t2t-vit/t2t-vit-"
"t-24_8xb64_in1k_20211214-b2a68ae3.log.json)"
msgstr ""

#: ../../papers/t2t_vit.md:28
msgid ""
"*In consistent with the [official repo](https://github.com/yitu-opensource/T2T-ViT), we adopt the best "
"checkpoints during training.*"
msgstr ""

#: ../../papers/tnt.md:4
msgid "TNT"
msgstr ""

#: ../../papers/tnt.md:6
msgid "[Transformer in Transformer](https://arxiv.org/abs/2103.00112)"
msgstr ""

#: ../../papers/tnt.md:12
#, python-format
msgid ""
"Transformer is a new kind of neural architecture which encodes the input data as powerful features via the "
"attention mechanism. Basically, the visual transformers first divide the input images into several local "
"patches and then calculate both representations and their relationship. Since natural images are of high "
"complexity with abundant detail and color information, the granularity of the patch dividing is not fine "
"enough for excavating features of objects in different scales and locations. In this paper, we point out "
"that the attention inside these local patches are also essential for building visual transformers with high "
"performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we "
"regard the local patches (e.g., 16×16) as \"visual sentences\" and present to further divide them into "
"smaller patches (e.g., 4×4) as \"visual words\". The attention of each word will be calculated with other "
"words in the given visual sentence with negligible computational costs. Features of both words and "
"sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks "
"demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on "
"the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar "
"computational cost."
msgstr ""

#: ../../papers/tnt.md
msgid "TNT-small\\*"
msgstr ""

#: ../../papers/tnt.md
msgid "23.76"
msgstr ""

#: ../../papers/tnt.md
msgid "3.36"
msgstr ""

#: ../../papers/tnt.md
msgid "81.52"
msgstr ""

#: ../../papers/tnt.md
msgid "95.73"
msgstr ""

#: ../../papers/tnt.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/tnt/tnt-s-p16_16xb64_in1k.py)"
msgstr ""

#: ../../papers/tnt.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/tnt/tnt-small-p16_3rdparty_in1k_20210903-"
"c56ee7df.pth)"
msgstr ""

#: ../../papers/tnt.md:26
msgid ""
"*Models with * are converted from [timm](https://github.com/rwightman/pytorch-image-models/). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/twins.md:4
msgid "Twins"
msgstr ""

#: ../../papers/twins.md:6
msgid ""
"[Twins: Revisiting the Design of Spatial Attention in Vision Transformers](http://arxiv-export-lb.library."
"cornell.edu/abs/2104.13840)"
msgstr ""

#: ../../papers/twins.md:12
msgid ""
"Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed "
"and they show that the design of spatial attention is critical to their success in these tasks. In this "
"work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple "
"spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we "
"propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures "
"are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized "
"in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent "
"performance on a wide range of visual tasks, including image level classification as well as dense "
"detection and segmentation. The simplicity and strong performance suggest that our proposed architectures "
"may serve as stronger backbones for many vision tasks. Our code is released at [this https URL](https://"
"github.com/Meituan-AutoML/Twins)."
msgstr ""

#: ../../papers/twins.md
msgid "PCPVT-small\\*"
msgstr ""

#: ../../papers/twins.md
msgid "24.11"
msgstr ""

#: ../../papers/twins.md
msgid "3.67"
msgstr ""

#: ../../papers/twins.md
msgid "81.14"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-pcpvt-"
"small_8xb128_in1k.py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-pcpvt-"
"small_3rdparty_8xb128_in1k_20220126-ef23c132.pth)"
msgstr ""

#: ../../papers/twins.md
msgid "PCPVT-base\\*"
msgstr ""

#: ../../papers/twins.md
msgid "43.83"
msgstr ""

#: ../../papers/twins.md
msgid "6.45"
msgstr ""

#: ../../papers/twins.md
msgid "82.66"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-pcpvt-base_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-pcpvt-"
"base_3rdparty_8xb128_in1k_20220126-f8c4b0d5.pth)"
msgstr ""

#: ../../papers/twins.md
msgid "PCPVT-large\\*"
msgstr ""

#: ../../papers/twins.md
msgid "60.99"
msgstr ""

#: ../../papers/twins.md
msgid "9.51"
msgstr ""

#: ../../papers/twins.md
msgid "83.09"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-pcpvt-"
"large_16xb64_in1k.py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-pcpvt-"
"large_3rdparty_16xb64_in1k_20220126-c1ef8d80.pth)"
msgstr ""

#: ../../papers/twins.md
msgid "SVT-small\\*"
msgstr ""

#: ../../papers/twins.md
msgid "24.06"
msgstr ""

#: ../../papers/twins.md
msgid "2.82"
msgstr ""

#: ../../papers/twins.md
msgid "81.77"
msgstr ""

#: ../../papers/twins.md
msgid "95.57"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-svt-small_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-svt-"
"small_3rdparty_8xb128_in1k_20220126-8fe5205b.pth)"
msgstr ""

#: ../../papers/twins.md
msgid "SVT-base\\*"
msgstr ""

#: ../../papers/twins.md
msgid "56.07"
msgstr ""

#: ../../papers/twins.md
msgid "8.35"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-svt-base_8xb128_in1k."
"py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-svt-"
"base_3rdparty_8xb128_in1k_20220126-e31cc8e9.pth)"
msgstr ""

#: ../../papers/twins.md
msgid "SVT-large\\*"
msgstr ""

#: ../../papers/twins.md
msgid "99.27"
msgstr ""

#: ../../papers/twins.md
msgid "14.82"
msgstr ""

#: ../../papers/twins.md
msgid "83.60"
msgstr ""

#: ../../papers/twins.md
msgid "96.50"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/twins/twins-svt-large_16xb64_in1k."
"py)"
msgstr ""

#: ../../papers/twins.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/twins/twins-svt-"
"large_3rdparty_16xb64_in1k_20220126-4817645f.pth)"
msgstr ""

#: ../../papers/twins.md:31
msgid ""
"*Models with * are converted from [the official repo](https://github.com/Meituan-AutoML/Twins). The config "
"files of these models are only for validation. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results. The validation accuracy is a little different from the "
"official paper because of the PyTorch version. This result is get in PyTorch=1.9 while the official result "
"is get in PyTorch=1.7*"
msgstr ""

#: ../../papers/van.md:4
msgid "Visual Attention Network"
msgstr ""

#: ../../papers/van.md:6
msgid "[Visual Attention Network](https://arxiv.org/pdf/2202.09741v2.pdf)"
msgstr ""

#: ../../papers/van.md:12
msgid ""
"While originally designed for natural language processing (NLP) tasks, the self-attention mechanism has "
"recently taken various computer vision areas by storm. However, the 2D nature of images brings three "
"challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects "
"their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only "
"captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large "
"kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while "
"avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual "
"Attention Network (VAN). While extremely simple and efficient, VAN outperforms the state-of-the-art vision "
"transformers and convolutional neural networks with a large margin in extensive experiments, including "
"image classification, object detection, semantic segmentation, instance segmentation, etc."
msgstr ""

#: ../../papers/van.md
msgid "VAN-T\\*"
msgstr ""

#: ../../papers/van.md
msgid "4.11"
msgstr ""

#: ../../papers/van.md
msgid "0.88"
msgstr ""

#: ../../papers/van.md
msgid "75.41"
msgstr ""

#: ../../papers/van.md
msgid "93.02"
msgstr ""

#: ../../papers/van.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/van/van-tiny_8xb128_in1k.py)"
msgstr ""

#: ../../papers/van.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/van/van-tiny_8xb128_in1k_20220501-385941af.pth)"
msgstr ""

#: ../../papers/van.md
msgid "VAN-S\\*"
msgstr ""

#: ../../papers/van.md
msgid "13.86"
msgstr ""

#: ../../papers/van.md
msgid "95.63"
msgstr ""

#: ../../papers/van.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/van/van-small_8xb128_in1k.py)"
msgstr ""

#: ../../papers/van.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/van/van-small_8xb128_in1k_20220501-17bc91aa.pth)"
msgstr ""

#: ../../papers/van.md
msgid "VAN-B\\*"
msgstr ""

#: ../../papers/van.md
msgid "26.58"
msgstr ""

#: ../../papers/van.md
msgid "5.03"
msgstr ""

#: ../../papers/van.md
msgid "82.80"
msgstr ""

#: ../../papers/van.md
msgid "96.21"
msgstr ""

#: ../../papers/van.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/van/van-base_8xb128_in1k.py)"
msgstr ""

#: ../../papers/van.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/van/van-base_8xb128_in1k_20220501-6a4cc31b.pth)"
msgstr ""

#: ../../papers/van.md
msgid "VAN-L\\*"
msgstr ""

#: ../../papers/van.md
msgid "44.77"
msgstr ""

#: ../../papers/van.md
msgid "83.86"
msgstr ""

#: ../../papers/van.md
msgid "96.73"
msgstr ""

#: ../../papers/van.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/van/van-large_8xb128_in1k.py)"
msgstr ""

#: ../../papers/van.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/van/van-large_8xb128_in1k_20220501-f212ba21.pth)"
msgstr ""

#: ../../papers/van.md:29
msgid ""
"\\*Models with * are converted from [the official repo](https://github.com/Visual-Attention-Network/VAN-"
"Classification). The config files of these models are only for validation. We don't ensure these config "
"files' training accuracy and welcome you to contribute your reproduction results."
msgstr ""

#: ../../papers/vgg.md:4
msgid "VGG"
msgstr ""

#: ../../papers/vgg.md:6
msgid "[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)"
msgstr ""

#: ../../papers/vgg.md:12
msgid ""
"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-"
"scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing "
"depth using an architecture with very small (3x3) convolution filters, which shows that a significant "
"improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. "
"These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first "
"and the second places in the localisation and classification tracks respectively. We also show that our "
"representations generalise well to other datasets, where they achieve state-of-the-art results. We have "
"made our two best-performing ConvNet models publicly available to facilitate further research on the use of "
"deep visual representations in computer vision."
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-11"
msgstr ""

#: ../../papers/vgg.md
msgid "132.86"
msgstr ""

#: ../../papers/vgg.md
msgid "7.63"
msgstr ""

#: ../../papers/vgg.md
msgid "68.75"
msgstr ""

#: ../../papers/vgg.md
msgid "88.87"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg11_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg11_batch256_imagenet_20210208-4271cd6c."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/"
"vgg11_batch256_imagenet_20210208-4271cd6c.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-13"
msgstr ""

#: ../../papers/vgg.md
msgid "133.05"
msgstr ""

#: ../../papers/vgg.md
msgid "11.34"
msgstr ""

#: ../../papers/vgg.md
msgid "70.02"
msgstr ""

#: ../../papers/vgg.md
msgid "89.46"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg13_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg13_batch256_imagenet_20210208-4d1d6080."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/"
"vgg13_batch256_imagenet_20210208-4d1d6080.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-16"
msgstr ""

#: ../../papers/vgg.md
msgid "138.36"
msgstr ""

#: ../../papers/vgg.md
msgid "71.62"
msgstr ""

#: ../../papers/vgg.md
msgid "90.49"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg16_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_batch256_imagenet_20210208-db26f1a5."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_batch256_imagenet_20210208-"
"db26f1a5.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-19"
msgstr ""

#: ../../papers/vgg.md
msgid "143.67"
msgstr ""

#: ../../papers/vgg.md
msgid "19.67"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg19_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_batch256_imagenet_20210208-e6920e4a."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_batch256_imagenet_20210208-"
"e6920e4a.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-11-BN"
msgstr ""

#: ../../papers/vgg.md
msgid "132.87"
msgstr ""

#: ../../papers/vgg.md
msgid "7.64"
msgstr ""

#: ../../papers/vgg.md
msgid "70.67"
msgstr ""

#: ../../papers/vgg.md
msgid "90.16"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg11bn_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg11_bn_batch256_imagenet_20210207-f244902c."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/vgg11_bn_batch256_imagenet_20210207-"
"f244902c.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-13-BN"
msgstr ""

#: ../../papers/vgg.md
msgid "11.36"
msgstr ""

#: ../../papers/vgg.md
msgid "72.12"
msgstr ""

#: ../../papers/vgg.md
msgid "90.66"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg13bn_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg13_bn_batch256_imagenet_20210207-1a8b7864."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/"
"vgg13_bn_batch256_imagenet_20210207-1a8b7864.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-16-BN"
msgstr ""

#: ../../papers/vgg.md
msgid "138.37"
msgstr ""

#: ../../papers/vgg.md
msgid "15.53"
msgstr ""

#: ../../papers/vgg.md
msgid "73.74"
msgstr ""

#: ../../papers/vgg.md
msgid "91.66"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/"
"vgg16_bn_batch256_imagenet_20210208-7e55cd29.log.json)"
msgstr ""

#: ../../papers/vgg.md
msgid "VGG-19-BN"
msgstr ""

#: ../../papers/vgg.md
msgid "143.68"
msgstr ""

#: ../../papers/vgg.md
msgid "19.7"
msgstr ""

#: ../../papers/vgg.md
msgid "74.68"
msgstr ""

#: ../../papers/vgg.md
msgid "92.27"
msgstr ""

#: ../../papers/vgg.md
msgid "[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vgg/vgg19bn_8xb32_in1k.py)"
msgstr ""

#: ../../papers/vgg.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f."
"pth) | [log](https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-"
"da620c4f.log.json)"
msgstr ""

#: ../../papers/vision_transformer.md:4
msgid "Vision Transformer"
msgstr ""

#: ../../papers/vision_transformer.md:6
msgid ""
"[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/"
"pdf/2010.11929.pdf)"
msgstr ""

#: ../../papers/vision_transformer.md:12
msgid ""
"**Vision Transformer**, known as **ViT**, succeeded in using a full transformer to outperform previous "
"works that based on convolutional networks in vision field. ViT splits image into patches to feed the multi-"
"head attentions, concatenates a learnable class token for final prediction and adds a learnable position "
"embeddings for relative positional message between patches. Based on these three techniques with "
"attentions, ViT provides a brand-new pattern to build a basic structure in vision field."
msgstr ""

#: ../../papers/vision_transformer.md:14
msgid ""
"The strategy works even better when coupled with large datasets pre-trainings. Because of its simplicity "
"and effectiveness, some after works in classification field are originated from ViT. And even in recent "
"multi-modality field, ViT-based method still plays a role in it."
msgstr ""

#: ../../papers/vision_transformer.md:28
msgid ""
"While the Transformer architecture has become the de-facto standard for natural language processing tasks, "
"its applications to computer vision remain limited. In vision, attention is either applied in conjunction "
"with convolutional networks, or used to replace certain components of convolutional networks while keeping "
"their overall structure in place. We show that this reliance on CNNs is not necessary and a pure "
"transformer applied directly to sequences of image patches can perform very well on image classification "
"tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image "
"recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent "
"results compared to state-of-the-art convolutional networks while requiring substantially fewer "
"computational resources to train. </br>"
msgstr ""

#: ../../papers/vision_transformer.md:87
msgid ""
"For more configurable parameters, please refer to the [API](https://mmclassification.readthedocs.io/en/1.x/"
"api/generated/mmcls.models.backbones.VisionTransformer.html#mmcls.models.backbones.VisionTransformer)."
msgstr ""

#: ../../papers/vision_transformer.md:91
msgid ""
"The training step of Vision Transformers is divided into two steps. The first step is training the model on "
"a large dataset, like ImageNet-21k, and get the pre-trained model. And the second step is training the "
"model on the target dataset, like ImageNet-1k, and get the fine-tuned model. Here, we provide both pre-"
"trained models and fine-tuned models."
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "ViT-B16\\*"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "33.03"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/pretrain/vit-base-"
"p16_3rdparty_pt-64xb64_in1k-224_20210928-02284250.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "ViT-B32\\*"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "88.30"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "8.56"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/pretrain/vit-base-"
"p32_3rdparty_pt-64xb64_in1k-224_20210928-eee25dd4.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "ViT-L16\\*"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "304.72"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "116.68"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/pretrain/vit-large-"
"p16_3rdparty_pt-64xb64_in1k-224_20210928-0001f9a1.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:107
msgid ""
"*Models with * are converted from the [official repo](https://github.com/google-research/"
"vision_transformer#available-vit-models).*"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "ViT-B16"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "82.37"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vision_transformer/vit-base-"
"p16_pt-32xb128-mae_in1k-224.py)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/vit-base-p16_pt-32xb128-"
"mae_in1k_20220623-4c544545.pth) | [log](https://download.openmmlab.com/mmclassification/v0/vit/vit-base-"
"p16_pt-32xb128-mae_in1k_20220623-4c544545.log)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "85.43"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "97.77"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vision_transformer/vit-base-"
"p16_ft-64xb64_in1k-384.py)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/finetune/vit-base-p16_in21k-"
"pre-3rdparty_ft-64xb64_in1k-384_20210928-98e8652b.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "ViT-B16 (IPU)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "81.22"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "95.56"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vision_transformer/vit-base-"
"p16_ft-4xb544-ipu_in1k.py)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/vit-base-p16_ft-4xb544-ipu_in1k_20220603-"
"c215811a.pth) | [log](https://download.openmmlab.com/mmclassification/v0/vit/vit-base-p16_ft-4xb544-"
"ipu_in1k.log)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "84.01"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "97.08"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vision_transformer/vit-base-"
"p32_ft-64xb64_in1k-384.py)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/finetune/vit-base-p32_in21k-"
"pre-3rdparty_ft-64xb64_in1k-384_20210928-9cea8599.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "85.63"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid "97.63"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/vision_transformer/vit-large-"
"p16_ft-64xb64_in1k-384.py)"
msgstr ""

#: ../../papers/vision_transformer.md:71
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/vit/finetune/vit-large-p16_in21k-"
"pre-3rdparty_ft-64xb64_in1k-384_20210928-b20ba619.pth)"
msgstr ""

#: ../../papers/vision_transformer.md:119
msgid ""
"*Models with * are converted from the [official repo](https://github.com/google-research/"
"vision_transformer#available-vit-models). The config files of these models are only for validation. We "
"don't ensure these config files' training accuracy and welcome you to contribute your reproduction results.*"
msgstr ""

#: ../../papers/wrn.md:4
msgid "Wide-ResNet"
msgstr ""

#: ../../papers/wrn.md:6
msgid "[Wide Residual Networks](https://arxiv.org/abs/1605.07146)"
msgstr ""

#: ../../papers/wrn.md:12
msgid ""
"Deep residual networks were shown to be able to scale up to thousands of layers and still have improving "
"performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of "
"layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes "
"these networks very slow to train. To tackle these problems, in this paper we conduct a detailed "
"experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture "
"where we decrease depth and increase width of residual networks. We call the resulting network structures "
"wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very "
"deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network "
"outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep "
"networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on "
"ImageNet."
msgstr ""

#: ../../papers/wrn.md
msgid "WRN-50\\*"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/wrn/wide-resnet50_8xb32_in1k.py)"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/wrn/wide-"
"resnet50_3rdparty_8xb32_in1k_20220304-66678344.pth)"
msgstr ""

#: ../../papers/wrn.md
msgid "WRN-101\\*"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/wrn/wide-resnet101_8xb32_in1k.py)"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/wrn/wide-"
"resnet101_3rdparty_8xb32_in1k_20220304-8d5f9d61.pth)"
msgstr ""

#: ../../papers/wrn.md
msgid "WRN-50 (timm)\\*"
msgstr ""

#: ../../papers/wrn.md
msgid "81.45"
msgstr ""

#: ../../papers/wrn.md
msgid "95.53"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[config](https://github.com/open-mmlab/mmclassification/blob/1.x/configs/wrn/wide-resnet50_timm_8xb32_in1k."
"py)"
msgstr ""

#: ../../papers/wrn.md
msgid ""
"[model](https://download.openmmlab.com/mmclassification/v0/wrn/wide-resnet50_3rdparty-"
"timm_8xb32_in1k_20220304-83ae4399.pth)"
msgstr ""

#: ../../papers/wrn.md:28
msgid ""
"*Models with * are converted from the [TorchVision](https://github.com/pytorch/vision/blob/main/torchvision/"
"models/resnet.py) and [TIMM](https://github.com/rwightman/pytorch-image-models/blob/master). The config "
"files of these models are only for inference. We don't ensure these config files' training accuracy and "
"welcome you to contribute your reproduction results.*"
msgstr ""
